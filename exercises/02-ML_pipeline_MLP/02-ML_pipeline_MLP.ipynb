{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning pipeline and MNIST\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Loading data\n",
    "2. Preprocessing\n",
    "3. Building different models\n",
    "4. Training models\n",
    "5. Model selection\n",
    "6. Model evaluation\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "1. Put into practice what you have learned with the tutorials\n",
    "1. Get familiar with the MNIST dataset\n",
    "1. Build a classic machine learning pipeline that includes the following steps:\n",
    "    - Loading data\n",
    "    - Preprocessing data\n",
    "    - Building different models\n",
    "    - Training models\n",
    "    - Selecting the best model\n",
    "    - Evaluating the best model\n",
    "    \n",
    "Of course, in reality, there are more intermediate steps and the pipeline looks more like an iterative process than a straight line, but after completing this notebook you should already have a good understanding of the machine learning pipeline and how to implement it in pytorch. :) \n",
    "\n",
    "**Andrew's videos related to this notebook**\n",
    "\n",
    "If you are struggling with some concepts when completing this notebook, you can (re-)watch the following videos: \n",
    "\n",
    "- About train/validation/test datasets:  \n",
    "    - [Train/Dev/Test Sets (C2W1L01)](https://www.youtube.com/watch?v=1waHlpKiNyY&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=2)\n",
    "    - [Train/Dev/Test Set Distributions (C3W1L05)](https://www.youtube.com/watch?v=M3qpIzy4MQk&list=PLkDaE6sCZn6E7jZ9sN_xHwSHOdjUxUW_b&index=6)\n",
    "    - [Sizeof Dev and Test Sets (C3W1L06)](https://www.youtube.com/watch?v=_Fe5kKmFieg&list=PLkDaE6sCZn6E7jZ9sN_xHwSHOdjUxUW_b&index=7)\n",
    "- About the machine learning pipeline and model Performance:\n",
    "    - [Bias/Variance (C2W1L02)](https://www.youtube.com/watch?v=SjQyLhQIXSM&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=3)\n",
    "    - [Basic Recipe for Machine Learning (C2W1L03)](https://www.youtube.com/watch?v=C1N_PDHuJ6Q&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=4)\n",
    "    - [Avoidable Bias (C3W1L09)](https://www.youtube.com/watch?v=CZf3oo0fuh0&list=PLkDaE6sCZn6E7jZ9sN_xHwSHOdjUxUW_b&index=10)\n",
    "    - [Improving Model Performance (C3W1L12)](https://www.youtube.com/watch?v=zg26t-BH7ao&list=PLkDaE6sCZn6E7jZ9sN_xHwSHOdjUxUW_b&index=13)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "from collections import Counter\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data\n",
    "\n",
    "In this notebook, we will use the MNIST dataset, a large dataset of black and white images of handwritten digits.\n",
    "\n",
    "**TODO**\n",
    "\n",
    "Write a ``load_MNIST`` function that:  \n",
    "1. Loads the MNIST dataset (see [torchvision.datasets.MNIST](https://pytorch.org/vision/stable/datasets.html#mnist))\n",
    "2. Splits the dataset into 3: training, validation and test datasets\n",
    "3. Returns the 3 datasets\n",
    "\n",
    "**Hints**\n",
    "\n",
    "You can adapt the ``load_cifar`` function written in the beginning of the 2nd and 3rd tutorials. (and all steps of this function are detailed in the 1st tutorial, \"1.1 Loading the CIFAR dataset in Pytorch\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot one instance of each class   \n",
    "\n",
    "**TODO**\n",
    "\n",
    "Plot once instance of each class.\n",
    "\n",
    "**Hints**\n",
    "\n",
    "- You can adapt the corresponding code from the 1st tutorial, \"1.3 Plot images\".\n",
    "- You can use ``cmap='gray'`` when calling [ax.imshow](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.imshow.html?highlight=imshow#matplotlib.axes.Axes.imshow) in order to get black and white images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the number of elements for each class   \n",
    "\n",
    "**TODO**\n",
    "\n",
    "1. Count the number of elements for each class in each of the 3 datasets. \n",
    "2. Does the dataset seem balanced?\n",
    "\n",
    "**Hints**\n",
    "\n",
    "You can adapt the corresponding code from the 1st tutorial, \"1.4 Count how many samples there are for each class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "The preprocessing step typically comes *after* loading the data, but as we saw in the tutorials, the preprocessor can be passed as a parameter of the loading function in Pytorch. \n",
    "\n",
    "#### TODO\n",
    "\n",
    "1. Compute the mean and the standard deviation of the training dataset. Note that contrary to the cifar dataset, the MNIST dataset contains only black and white images, so there is only one channel left. The mean and standard deviation should then be a scalar and not a tensor of 3 elements.\n",
    "1. Re-load your 3 datasets, now including a preprocessor that:\n",
    "    1. Crop the images from 28x28 to 24x24 (see [transforms.CenterCrop](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.CenterCrop))\n",
    "    2. Convert images to tensors (see [transforms.ToTensorp](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor))\n",
    "    3. Normalize the dataset using the computed mean and standard deviation (see [transforms.Normalize](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Normalize))\n",
    "    \n",
    "**Hints**\n",
    "- You can adapt the corresponding code from the 1st tutorial. \"2. Transforms\"\n",
    "- Use [transforms.Compose](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Compose) to define your preprocessor as a combination of your transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building different models\n",
    "\n",
    "**TODO**\n",
    "\n",
    "Define 2 (or 3) neural networks by writing classes inheriting the ``nn.Module`` class.\n",
    "\n",
    "**Hints**\n",
    "\n",
    "- See the 3rd tutorial, \"2.2 Using the functional API\"\n",
    "- 2-3 layers are enough, otherwise the training will take too long if you don't have a gpu.\n",
    "- Use only linear layers [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear), (and potentially non-trainable layers such as [F.max_pool2d](https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool2d.html?highlight=max_pool#torch.nn.functional.max_pool2d) or [torch.flatten](https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten)) we will study the other types of layers later on this semester.\n",
    "- Remember that we don't need a softmax function in the output layer if we use [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?highlight=crossentro#torch.nn.CrossEntropyLoss) as the loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training models\n",
    "\n",
    "**TODO**  \n",
    "Write a function ``train`` that \n",
    "- Trains the model for ``n`` epochs (complete passes through the training dataset)\n",
    "- Computes and stores the training loss and the validation loss for each epoch\n",
    "- Returns the list of training and validation losses\n",
    "\n",
    "**Hints**\n",
    "- You can find how to train and compute the training loss in the the ``train`` function in the tutorials. However, you need to modify this function in order to return the validation loss as well.\n",
    "- Use [mode.train()](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train) and [model.eval()](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval) correctly (see ``train`` and ``compute_accuracy`` in the tutorials)\n",
    "- Use \"with [torch.no_grad()](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad):\" correctly (see ``compute_accuracy`` in the tutorials)\n",
    "- Don't backpropagate the loss when computing validation loss (see ``compute_accuracy`` in the tutorials)\n",
    "- Don't update weights when computing validation loss (see ``compute_accuracy`` in the tutorials)\n",
    "- Here, we want the validation loss, not the accuracy on the validation dataset.\n",
    "- Both ``train`` and ``compute_accuracy`` are written at the beginning of the 3rd tutorial, with detailed explanation in the 2nd tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**  \n",
    "\n",
    "1. Define a train loader and a validation loader. \n",
    "2. Train 3 different models, i.e.:\n",
    "    1. Instanciate a model using your custom Pytorch modules  \n",
    "    2. Choose an optimizer (and its parameters, e.g the learning rate)  \n",
    "    3. Choose a loss function  (e.g cross-entropy)\n",
    "    4. Train your model and store its training and validation loss\n",
    "\n",
    "**Hints**\n",
    "\n",
    "- Again, you can find some hints in the tutorials\n",
    "- 20 epochs or even a bit less are enough if your computer is a bit slow\n",
    "- If you defined only 2 modules in section 3, you can also play with the learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ploting the evolution of the training loss and the validation loss during the training\n",
    "\n",
    "**TODO**\n",
    "\n",
    "For each of your 3 models, plot the training loss and the validation loss. You can adapt the code in the cell below to plot your curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvalues = np.linspace(0, 20, 1000)\n",
    "yvalues01 = np.cos(xvalues)\n",
    "yvalues02 = np.sin(xvalues)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot cosine and specify its legend label\n",
    "ax.plot(xvalues, yvalues01, label='cosine')\n",
    "# Plot sine and specify its legend label\n",
    "ax.plot(xvalues, yvalues02, label='sine')\n",
    "ax.set_title('Cosine and Sine')\n",
    "ax.set_xlabel('x label')\n",
    "ax.set_ylabel('y label')\n",
    "# Show legend\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model selection\n",
    "\n",
    "1. Write a function ``compute_accuracy`` that computes the accuracy of a given model on a given dataset. You can find all you need in the tutorials.\n",
    "2. Select your best model, that is to say:\n",
    "    1. For each model, compute the accuracy on the validation dataset\n",
    "    2. Choose the model with the highest accuracy\n",
    "3. Print the training and the validation accuracies of your selected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model evaluation\n",
    "\n",
    "**TODO**\n",
    "\n",
    "1. Evaluate your selected model, that is to say, compute and print the accuracy of your selected model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "71c2cb666ff353b4e7b5c350d66179fa0af5c84ce239ad9fa105d94543f3ad59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
