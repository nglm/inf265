{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a LSTM layer\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The objective of this exercise is get an in-depth understanding of the LSTM layer as it is a crucial layer in Recurrent Neural Networks. To achieve this, you will simply implement your own LSTM layer ``MyLSTM`` and make sure that it yields the same results as [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html?highlight=lstm#torch.nn.LSTM).\n",
    "\n",
    "## Contents:\n",
    "\n",
    "1. Utils\n",
    "2. Reading txt files, building a vocabulary and creating a dataset\n",
    "2. Implementing a custom layer in Pytorch: ``MyLSTM``\n",
    "3. Using ``MyLSTM`` inside a neural network model\n",
    "4. Testing ``MyLSTM`` by comparing it to [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html?highlight=lstm#torch.nn.LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import os\n",
    "import re\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "torch.manual_seed(265)\n",
    "# We use torch.double to get the same results as Pytorch\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Utils\n",
    "\n",
    "Nothing to see in the cell below, just the definition functions we'll need later, you don't even need to read them, just know that there are 2 functions:\n",
    "\n",
    "- ``train``: Train a model, save weight values of the LSTM layer for each epoch of the training. \n",
    "- ``relative_error(a, b)``: Compute the relative error of ``b``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "print(f\"Device {device}.\")\n",
    "\n",
    "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    \"\"\"\n",
    "    Train our model and save weight values\n",
    "    \"\"\"\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    weight = {\n",
    "        \"weight_ih_l0\" : [],\n",
    "        \"weight_hh_l0\" : [],\n",
    "        \"bias_ih_l0\" : [],\n",
    "        \"bias_hh_l0\" : [],\n",
    "    }\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        loss_train = 0.0\n",
    "        for contexts, labels in train_loader:\n",
    "\n",
    "            contexts = torch.permute(contexts, (1,0,2))\n",
    "            # We use torch.double to get the same results as Pytorch\n",
    "            contexts = contexts.to(device=device, dtype=torch.double) \n",
    "            \n",
    "            labels = labels.to(device=device, dtype=torch.double)\n",
    "\n",
    "            outputs = model(contexts).squeeze()\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        losses_train.append(loss_train / n_batch)\n",
    "        \n",
    "        # Here we store weight values at each step of the training process\n",
    "        # The \"0\" written after each layer name refers to the fact that this is the first LSTM layer (and only one)\n",
    "        with torch.no_grad():\n",
    "            weight[\"weight_ih_l0\"].append(model.lstm.weight_ih_l0.data.clone().detach())\n",
    "            weight[\"weight_hh_l0\"].append(model.lstm.weight_hh_l0.data.clone().detach())\n",
    "            weight[\"bias_ih_l0\"].append(model.lstm.bias_ih_l0.data.clone().detach())\n",
    "            weight[\"bias_hh_l0\"].append(model.lstm.bias_hh_l0.data.clone().detach())\n",
    "\n",
    "        print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "            datetime.now().time(), epoch, loss_train / n_batch))\n",
    "    return weight\n",
    "\n",
    "\n",
    "def relative_error(a, b):\n",
    "    return (torch.norm(a - b) / torch.norm(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading txt files, building a vocabulary and creating a dataset\n",
    "\n",
    "The objective this week is only to implement a LSTM layer. We will then use a short text and keep only a very short vocabulary and simply try to predict if, given a sequence, the next word will be a known word. (`<unk>` token or not)\n",
    "\n",
    "There is nothing to do in this cell, apart from running it. You might want to read it carefully though, as it is an important step of the pipeline in pytorch when dealing with text data. Note that last week there were also a tutorial on text data (see `06 - Text data`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer will split a long text into a list of english words\n",
    "TOKENIZER_EN = get_tokenizer('basic_english')\n",
    "# Minimum number of occurence of a word in the text to add it to the vocabulary\n",
    "MIN_FREQ = 2\n",
    "\n",
    "def read_files(datapath='./data_train/'):\n",
    "    \"\"\"\n",
    "    Return a list of strings, one for each line in each .txt files in 'datapath'\n",
    "    \"\"\"\n",
    "    # Find all txt files in directory \n",
    "    files = os.listdir(datapath)\n",
    "    files = [datapath + f for f in files if f.endswith(\".txt\")]\n",
    "    \n",
    "    # Stores each line of each book in a list\n",
    "    lines = []\n",
    "    for f_name in files:\n",
    "        with open(f_name) as f:\n",
    "            lines += f.readlines()\n",
    "    return lines\n",
    "\n",
    "def tokenize(lines, tokenizer=TOKENIZER_EN):\n",
    "    \"\"\"\n",
    "    Tokenize the list of lines\n",
    "    \"\"\"\n",
    "    list_text = []\n",
    "    for line in lines:\n",
    "        list_text += tokenizer(line)\n",
    "    return list_text\n",
    "\n",
    "def yield_tokens(lines, tokenizer=TOKENIZER_EN):\n",
    "    \"\"\"\n",
    "    Yield tokens, ignoring names and digits to build vocabulary\n",
    "    \"\"\"\n",
    "    # Match any word containing digit\n",
    "    no_digits = '\\w*[0-9]+\\w*'\n",
    "    # Match word containing a uppercase \n",
    "    no_names = '\\w*[A-Z]+\\w*'\n",
    "    # Match any sequence containing more than one space\n",
    "    no_spaces = '\\s+'\n",
    "    \n",
    "    for line in lines:\n",
    "        line = re.sub(no_digits, ' ', line)\n",
    "        line = re.sub(no_names, ' ', line)\n",
    "        line = re.sub(no_spaces, ' ', line)\n",
    "        yield tokenizer(line)\n",
    "\n",
    "def count_freqs(words, vocab):\n",
    "    \"\"\"\n",
    "    Count occurrences of each word in vocabulary in the data\n",
    "    \n",
    "    Useful to get some insight on the data and to compute loss weights\n",
    "    \"\"\"\n",
    "    freqs = torch.zeros(len(vocab), dtype=torch.int)\n",
    "    for w in words:\n",
    "        freqs[vocab[w]] += 1\n",
    "    return freqs\n",
    "\n",
    "def create_vocabulary(lines, min_freq=MIN_FREQ):\n",
    "    \"\"\"\n",
    "    Create a vocabulary (list of known tokens) from a list of strings\n",
    "    \"\"\"\n",
    "    # vocab contains the vocabulary found in the data, associating an index to each word\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(lines), min_freq=min_freq, specials=[\"<unk>\"])\n",
    "    # Since we removed all words with an uppercase when building the vocabulary, we skipped the word \"I\"\n",
    "    vocab.append_token(\"i\")\n",
    "    # Value of default index. This index will be returned when OOV (Out Of Vocabulary) token is queried.\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the training dataset:      240\n",
      "Number of distinct words in the training dataset:   98\n",
      "Number of distinct words kept (vocabulary size):    32\n",
      "occurences:\n",
      " [(78, '<unk>'), (18, 'a'), (12, 'down'), (12, 'go'), (12, 'medicine'), (19, 'the'), (10, 'of'), (6, 'helps'), (6, 'sugar'), (5, 'to'), (6, 'spoonful'), (4, 'that'), (3, '!'), (6, 'and'), (3, 'delightful'), (3, 'every'), (3, 'find'), (3, 'his'), (3, 'job'), (3, 'most'), (3, 'way'), (2, '('), (2, ')'), (2, 'fun'), (2, 'hence'), (2, 'is'), (2, 'little'), (2, 's'), (2, 'task'), (4, 'they'), (2, 'very'), (0, 'i')]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------- Tokenize texts -------------------------------\n",
    "# Get lists of strings, one for each line in each .txt files in 'datapath' \n",
    "lines_books_train = read_files('./data_train/')\n",
    "\n",
    "# List of words contained in the dataset\n",
    "words_train = tokenize(lines_books_train)\n",
    "\n",
    "# ----------------------- Create vocabulary ----------------------------\n",
    "# Create vocabulary based on the words in the training dataset\n",
    "vocab = create_vocabulary(lines_books_train, min_freq=MIN_FREQ)\n",
    "\n",
    "# ------------------------ Quick analysis ------------------------------\n",
    "VOCAB_SIZE = len(vocab)\n",
    "print(\"Total number of words in the training dataset:     \", len(words_train))\n",
    "print(\"Number of distinct words in the training dataset:  \", len(set(words_train)))\n",
    "print(\"Number of distinct words kept (vocabulary size):   \", VOCAB_SIZE)\n",
    "\n",
    "freqs = count_freqs(words_train, vocab)\n",
    "print(\"occurences:\\n\", [(f.item(), w) for (f, w)  in zip(freqs, vocab.lookup_tokens(range(VOCAB_SIZE)))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a ``(contexts, targets)`` toydataset such that for each ``(c, t)`` context/target pair in the dataset, ``t = 0`` if the next word after the sequence ``c`` is the ``<unk>`` token (i.e. the first element of the vocabulary representing words that are outside the vocabulary), and ``t = 1``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ Define targets ------------------------------\n",
    "def compute_label(w):\n",
    "    \"\"\"\n",
    "    helper function to define MAP_TARGET\n",
    "    \n",
    "    - 0 = 'unknown word'\n",
    "    - 1 = 'is an actual word'\n",
    "    \"\"\"\n",
    "    if w in ['<unk>']:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# true labels for this task:\n",
    "MAP_TARGET = {\n",
    "    vocab[w]:compute_label(w) for w in vocab.lookup_tokens(range(VOCAB_SIZE))\n",
    "}\n",
    "\n",
    "# context size for this task \n",
    "CONTEXT_SIZE = 3\n",
    "\n",
    "\n",
    "# ---------------- Define context / target pairs -----------------------\n",
    "def create_dataset(\n",
    "    text, vocab, \n",
    "    context_size=CONTEXT_SIZE, map_target=MAP_TARGET\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a pytorch dataset of context / target pairs from a text\n",
    "    \"\"\"\n",
    "    \n",
    "    n_text = len(text)\n",
    "    n_vocab = len(vocab)\n",
    "    \n",
    "    # Change labels if only a few target are kept, otherwise, each word is\n",
    "    # associated with its index in the vocabulary\n",
    "    if map_target is None:\n",
    "        map_target = {i:i for i in range(n_vocab)}\n",
    "    \n",
    "    # Transform the text as a list of integers.\n",
    "    txt = [vocab[w] for w in text]\n",
    "\n",
    "    # Start constructing the context / target pairs...\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(n_text - context_size):\n",
    "        \n",
    "        # Word used to define target\n",
    "        t = txt[i + context_size]\n",
    "        \n",
    "        # Context before the target\n",
    "        c = txt[i:i + context_size]\n",
    "        \n",
    "        targets.append(map_target[t])\n",
    "        # Normally we should use word embedding, and not hot encoding, but we \n",
    "        # skip that part for this exercise\n",
    "        contexts.append(F.one_hot(torch.tensor(c), num_classes=n_vocab))\n",
    "            \n",
    "    contexts = torch.stack(contexts)\n",
    "    targets = torch.tensor(targets)\n",
    "    # Create a pytorch dataset out of these context / target pairs\n",
    "    return TensorDataset(contexts, targets)\n",
    "\n",
    "data = create_dataset(words_train, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing a LSTM layer in pytorch\n",
    "\n",
    "### Illustration and notations of LSTM layers in pytorch \n",
    "\n",
    "![LSTM in pytorch](lstm.png) ![LSTM in pytorch](nn.lstm.png)\n",
    "\n",
    "image credits [diagram](https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm/48305882#48305882), [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html?highlight=lstm#torch.nn.LSTM)\n",
    "\n",
    "Reminder about Pytorch notations and dimensions:\n",
    "\n",
    "- ``N``: batch size\n",
    "- ``L``: sequence length (number of words given as input)\n",
    "- $H_{in}$: input size (dimension of a word)\n",
    "- $H_{cell}, H_{out}$: hidden size (dimension of tensor between cells) \n",
    "- $h_t^{(l)}$​ is the hidden state at time $t$ in layer $l$\n",
    "- $c_t^{(l)}$​ is the cell state at time $t$ in layer $l$\n",
    "- $x_t^{(l)}$​ is the input at time $t$ in layer $l$\n",
    "- $h_{t−1}^{(l)}$​ is the hidden state of the layer at time $t-1$ or the initial hidden state at time $0$ in layer $l$\n",
    "\n",
    "Note that for a **many-to-one LSTM** network (i.e network with input sequence of length $L>1$ and output sequence of length $L=1$), we are only interested in $h_n^{(w)}$, i.e the final hidden state of the last layer.\n",
    "\n",
    "\n",
    "### Specific settings for this exercise\n",
    "\n",
    "We will restrict the scope of this exercise by setting the following parameters of the [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html?highlight=lstm#torch.nn.LSTM) layer. (those are the defaults ones)\n",
    "\n",
    "- ``num_layers = 1``\n",
    "- ``bias = True``\n",
    "- ``batch_first = False``\n",
    "- ``dropout = 0``\n",
    "- ``bidirectional = False``\n",
    "- ``proj_size = 0``\n",
    "\n",
    "In these specific settings:\n",
    "\n",
    "- we give an input ``input`` of shape $(L, N, H_{in})$\n",
    "- we \"only\" have to compute: \n",
    "    - ``LSTM.weight_ih_l0[k]``: the learnable input-hidden weights  $(W_{ii}|W_{if}|W_{ig}|W_{io})$ of the $k^{th}$ layer. For $k=0$ (which is in fact our unique case here, since we have only one layer) the shape is ``(4*hidden_size, input_size)``. Otherwise, the shape is ``(4*hidden_size, hidden_size)``\n",
    "    - ``LSTM.weight_hh_l0[0]``: the learnable hidden-hidden weights $(W_{hi}|W_{hf}|W_{hg}|W_{ho})$, all of shape ``(4*hidden_size, hidden_size)``.\n",
    "    - ``LSTM.bias_ih_l0[0]``  : the learnable input-hidden bias of the layer $(b_{ii}|b_{if}|b_{ig}|b_{io})$, all of shape ``(4*hidden_size)``.\n",
    "    - ``LSTM.bias_hh_l0[0]``  : the learnable hidden-hidden bias of the layer $(b_{hi}|b_{hf}|b_{hg}|b_{ho})$, all of shape ``(4*hidden_size)``.\n",
    "- we get as output ``output, (h_n, c_n)`` such that:\n",
    "    - ``output`` has shape ``(L, N, H_out​)``, containing the output features ``h_t`` from the last layer of the LSTM, for each $t$\n",
    "    - ``h_n`` has shape ``(num_layers, N, H_out​)`` (with ``num_layers = 1``) containing the final hidden state for each layer.\n",
    "    - ``c_n`` has shape ``(num_layers, N, H_cell​)`` (with ``num_layers = 1``) containing the final cell state for each layer.\n",
    "    \n",
    "\n",
    "### TODO\n",
    "\n",
    "Define a LSTM module, by implementing the operations described in [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html?highlight=lstm#torch.nn.LSTM). \n",
    "\n",
    "**WARNING** Pytorch doesn't like to compute the backward pass on recurrent operations! When implementing the instructions for each cell, don't hesitate to add '.clone' whenever you use an old value of a given tensor so that Pytorch creates a new component inside the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size:int,\n",
    "        hidden_size:int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_size = int(input_size)\n",
    "        self.hidden_size = int(hidden_size)\n",
    "\n",
    "        self.weight_ih_l0 = nn.Parameter(torch.Tensor( (4*self.hidden_size, self.input_size) ))\n",
    "        self.weight_hh_l0 = nn.Parameter(torch.Tensor( (4*self.hidden_size, self.hidden_size) )) \n",
    "        self.bias_ih_l0 = nn.Parameter(torch.Tensor( (4*self.hidden_size) )) \n",
    "        self.bias_hh_l0 = nn.Parameter(torch.Tensor( (4*self.hidden_size) )) \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        (L, N, H_in) = x.shape\n",
    "\n",
    "        output = torch.zeros((L, N, self.hidden_size))\n",
    "        h = torch.zeros((L+1, N, self.hidden_size))\n",
    "        c = torch.zeros((L+1, N, self.hidden_size))\n",
    "        \n",
    "        s = [self.hidden_size*k for k in range(4)]\n",
    "        e = [self.hidden_size*k for k in range(1,5)]\n",
    "        \n",
    "        # shorthand\n",
    "        mm = torch.matmul\n",
    "        for j in range(N):\n",
    "            for t in range(L):\n",
    "                i = torch.sigmoid( mm(self.weight_ih_l0[s[0]:e[0]], x[t, j]) + self.bias_ih_l0[s[0]:e[0]] + mm(self.weight_hh_l0[s[0]:e[0]], h[t, j].clone()) + self.bias_hh_l0[s[0]:e[0]])\n",
    "                f = torch.sigmoid( mm(self.weight_ih_l0[s[1]:e[1]], x[t, j]) + self.bias_ih_l0[s[1]:e[1]] + mm(self.weight_hh_l0[s[1]:e[1]], h[t, j].clone()) + self.bias_hh_l0[s[1]:e[1]])\n",
    "                g = torch.tanh( mm(self.weight_ih_l0[s[2]:e[2]], x[t, j]) + self.bias_ih_l0[s[2]:e[2]] +    mm(self.weight_hh_l0[s[2]:e[2]], h[t, j].clone()) + self.bias_hh_l0[s[2]:e[2]])\n",
    "                o = torch.sigmoid( mm(self.weight_ih_l0[s[3]:e[3]], x[t, j]) + self.bias_ih_l0[s[3]:e[3]] + mm(self.weight_hh_l0[s[3]:e[3]], h[t, j].clone()) + self.bias_hh_l0[s[3]:e[3]])\n",
    "                c[t+1, j] = f.clone() * c[t, j].clone() + i.clone() * g.clone()\n",
    "                h[t+1, j] = o.clone() * torch.tanh(c[t+1, j].clone())\n",
    "                output[t, j] = h[0, j]\n",
    "\n",
    "        return output, (h, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using MyLSTM inside a neural network model\n",
    "\n",
    "Here is defined a very basic neural network to test ``MyLSTM``. It consists of: \n",
    "\n",
    "- a LSTM layer (``MyLSTM`` or ``nn.LSTM``) (see ``lstm_type`` parameter)\n",
    "- a fully connected layer with 1 output\n",
    "\n",
    "You don't have to do anything here, simply run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple net with only one LSTM layer and one fc layer. \n",
    "    \n",
    "    The LSTM layer can be ``MyLSTM`` or ``nn.LSTM``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size:int,\n",
    "        hidden_size:int,\n",
    "        lstm_type:str = 'custom',\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        # Use MyLSTM\n",
    "        if lstm_type == 'custom':\n",
    "            self.lstm = MyLSTM(input_size, hidden_size)\n",
    "        # or use pytorch's LSTM\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM outputs: (out, (h, c)) with h of shape (num_layer, N, H_out) and we want h[-1,:,:]\n",
    "        output, (h_n, c_n) = self.lstm(x)\n",
    "        out = self.fc1(h_n[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing MyLSTM by comparing it to nn.LSTM\n",
    "\n",
    "MyLSTM and nn.LSTM are compared by storing after each epoch:\n",
    "\n",
    "- Weight and biases values of the LSTM layer\n",
    "- Training loss\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "- Your implementation will probably be much slower than nn.LSTM.\n",
    "- If your implementation is correct, the training loss printed should be identical and the expected relative error should be around 1e-16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n",
      "\n",
      " ========= Training using MyLSTM =========\n",
      "21:53:07.802600  |  Epoch 1  |  Training loss 0.65921\n",
      "21:53:08.687727  |  Epoch 2  |  Training loss 0.64978\n",
      "21:53:09.517562  |  Epoch 3  |  Training loss 0.64260\n",
      "21:53:10.331952  |  Epoch 4  |  Training loss 0.63711\n",
      "\n",
      " ========= Training using nn.LSTM =========\n",
      "21:53:10.346035  |  Epoch 1  |  Training loss 0.65921\n",
      "21:53:10.352859  |  Epoch 2  |  Training loss 0.64978\n",
      "21:53:10.359853  |  Epoch 3  |  Training loss 0.64260\n",
      "21:53:10.366550  |  Epoch 4  |  Training loss 0.63711\n",
      "\n",
      " ======= Relative error:     nn.LSTM    VS    MyLSTM   =========\n",
      "-------------------- weight_ih_l0 --------------------\n",
      "[tensor(0.), tensor(1.3273e-19), tensor(4.2825e-18), tensor(4.3154e-18)]\n",
      "-------------------- weight_hh_l0 --------------------\n",
      "[tensor(4.1897e-18), tensor(8.2476e-18), tensor(8.1194e-18), tensor(1.0694e-17)]\n",
      "-------------------- bias_ih_l0 --------------------\n",
      "[tensor(0.), tensor(1.7132e-17), tensor(2.5687e-17), tensor(2.5676e-17)]\n",
      "-------------------- bias_hh_l0 --------------------\n",
      "[tensor(4.6845e-17), tensor(4.8308e-17), tensor(5.3394e-17), tensor(5.3320e-17)]\n"
     ]
    }
   ],
   "source": [
    "# DONT USE GPU!!! IT WOULD REQUIRE USING register_buffer TO MOVE THE MODEL CORRECTLY TO\n",
    "# THE GPU. (See https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer)\n",
    "# And you probably don't want to spend the entire week learning how to use this properly\n",
    "device = torch.device('cpu')\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "\n",
    "\n",
    "# We set shuffle to False so that we can compare both models more accurately\n",
    "train_loader = DataLoader(data, batch_size=64, shuffle=False)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# These parameters don't matter much in this assignment\n",
    "n_epochs = 4    \n",
    "lr = 0.1\n",
    "hidden_size = 12\n",
    "\n",
    "torch.manual_seed(265)\n",
    "# Using MyLSTM\n",
    "model01 = MyNet(VOCAB_SIZE, hidden_size, lstm_type='custom').to(device=device) \n",
    "# Using nn.LSTM\n",
    "\n",
    "torch.manual_seed(265)\n",
    "model02 = MyNet(VOCAB_SIZE, hidden_size, lstm_type='pytorch').to(device=device) \n",
    "\n",
    "# Make both models start with the same weight values\n",
    "with torch.no_grad():\n",
    "    # The \"0\" written after each layer name refers to the fact that this is the first LSTM layer (and only one)\n",
    "    model01.lstm.weight_ih_l0.data = model02.lstm.weight_ih_l0.data.clone()\n",
    "    model01.lstm.weight_hh_l0.data = model02.lstm.weight_hh_l0.data.clone()\n",
    "    model01.lstm.bias_ih_l0.data = model02.lstm.bias_ih_l0.data.clone()\n",
    "    model01.lstm.bias_hh_l0.data = model02.lstm.bias_hh_l0.data.clone()\n",
    "    model01.fc1.bias.data = model02.fc1.bias.data.clone()\n",
    "    model01.fc1.weight.data = model02.fc1.weight.data.clone()\n",
    "\n",
    "print(\"\\n ========= Training using MyLSTM =========\")\n",
    "optimizer = optim.SGD(model01.parameters(), lr=lr)\n",
    "weight01 = train(\n",
    "    n_epochs = n_epochs,\n",
    "    optimizer = optimizer, \n",
    "    model = model01,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "print(\"\\n ========= Training using nn.LSTM =========\")\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model02.parameters(), lr=lr)\n",
    "weight02= train(\n",
    "    n_epochs = n_epochs,\n",
    "    optimizer = optimizer, \n",
    "    model = model02,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "print(\"\\n ======= Relative error:     nn.LSTM    VS    MyLSTM   =========\")\n",
    "for (k1, v1), (k2, v2) in zip(weight01.items(), weight02.items()):\n",
    "    print('-'*20, k1, '-'*20)\n",
    "    print([relative_error(v1[i],v2[i]) for i in range(len(v1))] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
