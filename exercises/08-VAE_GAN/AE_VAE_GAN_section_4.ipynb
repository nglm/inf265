{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "seed = 265\n",
    "torch.manual_seed(seed)\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this assignment we will go through 3 types of unsupervised neural network: AutoEncoder (AE), Variational AutoEncoder (VAE) and Generative Adversarial Network (GAN). In the first section we will also introduce a new type of layer: the transpose convolution as it is widely used in these unsupervised methods.\n",
    "\n",
    "Unsupervised have many advantages including the fact that they don't need labels but they are also harder to train. It is normal if you don't get good results.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Transpose convolution\n",
    "2. AutoEncoder\n",
    "3. Variational AutoEncoder\n",
    "4. GAN\n",
    "\n",
    "## Related videos from the curriculum\n",
    "\n",
    "- [Lecture 15.1 — From PCA to autoencoders](https://www.youtube.com/watch?v=PSOt7u8u23w&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=69)\n",
    "- [Lecture 15.2 — Deep autoencoders](https://www.youtube.com/watch?v=6jhhIPdgkp0&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=70) \n",
    "- [Lecture 15.3 — Deep autoencoders for document retrieval](https://www.youtube.com/watch?v=ZCNbjpcX0yg&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=71)\n",
    "- [Lecture 15.6 — Shallow autoencoders for pre training](https://www.youtube.com/watch?v=xjlvVfEbhz4&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=74)\n",
    "- [Lecture 13 | Generative Models](https://www.youtube.com/watch?v=5WoItGTWV54)\n",
    "\n",
    "\n",
    "## 4. GAN\n",
    "\n",
    "## Related videos from the curriculum\n",
    "\n",
    "- [Lecture 13 | Generative Models](https://www.youtube.com/watch?v=5WoItGTWV54) (From 50:00 to 1:17:00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, _, _ = load_MNIST(label_list=[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data too complex...\n",
    "imgs_train = [F.max_pool2d(img,2) for img, _ in data_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a discriminator and a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Distinguish between fake (0) and real (1) images.\n",
    "    \n",
    "    It takes as input 10x10 black and white images and return a float\n",
    "    determining (hopefully correctly) if images are real (1) or fake (0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.fc1 = nn.Linear(in_features=10*10, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.fc3 = nn.Linear(in_features=32, out_features=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N = x.shape[0]\n",
    "        out = x.view(N, -1)\n",
    "        out = F.leaky_relu(self.fc1(out), 2)\n",
    "        out = F.leaky_relu(self.fc2(out), 2)\n",
    "        out = torch.sigmoid(self.fc3(out))\n",
    "        return out\n",
    "\n",
    "class MyGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generate fake images that look real from random noise\n",
    "    \n",
    "    It takes as input random normal noise (vectors of dimension z_dim) \n",
    "    and it returns a 10x10 black and white generated images (0) that will \n",
    "    hopefully be classified as real (1) by the discriminator. (So we hope that\n",
    "    the discriminator will misclassify these generated images)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__() \n",
    "        self.fc1 = nn.Linear(z_dim, 5*5)\n",
    "        self.transconv1 = nn.ConvTranspose2d(in_channels=1, out_channels=10,  kernel_size=3, stride=1)\n",
    "        self.transconv2 = nn.ConvTranspose2d(in_channels=10, out_channels=1,  kernel_size=4, stride=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, z_dim = x.shape\n",
    "        out = F.leaky_relu(self.fc1(x))\n",
    "        out = out.view(N, 1, 5, 5)\n",
    "        out = F.leaky_relu(self.transconv1(out))\n",
    "        out = torch.sigmoid(self.transconv2(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions\n",
    "\n",
    "At each iteration of the training loop of a GAN, we need to update both the discriminator and the generator. To do so, we can define 2 auxiliary functions\n",
    "``train_discriminator_aux`` and ``train_generator_aux`` that will be called in the main training loop ``training_gan``.\n",
    "\n",
    "#### Training the discriminator\n",
    "\n",
    "The training step for the discriminator can be divided into 4 steps:\n",
    "1. Compute classification loss on real images\n",
    "2. Generate fake images\n",
    "3. Compute classification loss on generated images\n",
    "4. Update step for the discriminator\n",
    "\n",
    "The discriminator loss is then the sum of 2 terms:\n",
    "- the loss on real images, computed by classifying a batch of real images\n",
    "- the loss on fake images, computed by classifying a batch of fake images, generated by the generator\n",
    "\n",
    "\n",
    "#### Training the generator\n",
    "\n",
    "The training step for the discriminator can be divided into 3 steps:\n",
    "1. Generate fake images\n",
    "2. Compute (mis)classification loss on generated images\n",
    "3. Update step for the generator\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "Complete the 3 functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator_aux(true_images, generator, discriminator, optimizer_d, loss, z_dim, device=None):\n",
    "    \"\"\"\n",
    "    Update step for the discriminator\n",
    "    \n",
    "    The discriminator loss is the sum of 2 terms:\n",
    "    - the loss on real images, computed by classifying a batch of real images\n",
    "    - the loss on fake images, computed by classifying a batch of fake images, \n",
    "    generated by the generator\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # ----------- Compute classification loss on real images -----------\n",
    "    \n",
    "    # Let the discriminator classify the real images\n",
    "    #TODO!\n",
    "    \n",
    "    # The targets are \"ones\" because the images are real\n",
    "    #TODO!\n",
    "\n",
    "    # Compute discriminator loss on true images:\n",
    "    # \"how well the discriminator classifies *real* images as *real*\"\n",
    "    #TODO!\n",
    "\n",
    "    \n",
    "    # ------------------- Generate fake images -------------------------\n",
    "    \n",
    "    # Generate a batch of normal noise, of dimension z_dim\n",
    "    #TODO!\n",
    "\n",
    "    # Use the generator to generate fake images from noise\n",
    "    #TODO!\n",
    "\n",
    "    \n",
    "    # ------- Compute classification loss on generated images ----------\n",
    "    \n",
    "    # Let the discriminator classify the real images\n",
    "    #TODO!\n",
    "\n",
    "    # The targets are \"zeros\" because the images are fake\n",
    "    #TODO!\n",
    "\n",
    "    # Compute discriminator loss on fake images:\n",
    "    # \"how well the discriminator classifies *fake* images as *fake*\"\n",
    "    #TODO!\n",
    "\n",
    "\n",
    "    # --------------- Update step for the discriminator ----------------\n",
    "    #TODO!\n",
    "\n",
    "\n",
    "    return #TODO!\n",
    "\n",
    "def train_generator_aux(generator, discriminator, optimizer_g, loss, z_dim, N=256, device=None): \n",
    "    \"\"\"\n",
    "    Update step for the generator.\n",
    "    \n",
    "    The generator loss function is computed by classifying a batch of fake \n",
    "    images but using 1 (real) as target. The objective is then to minimize\n",
    "    the number of fake images correctly classified as fake by the discriminator.\n",
    "    \"\"\"\n",
    "    generator.train()\n",
    "    # The grad might not be clean because of the train_discriminator_aux function\n",
    "    optimizer_g.zero_grad()\n",
    "    \n",
    "    # ------------------- Generate fake images -------------------------\n",
    "    \n",
    "    # Generate a batch of normal noise, of dimension z_dim\n",
    "    #TODO!\n",
    "    \n",
    "    # Use the generator to generate fake images from noise\n",
    "    #TODO!\n",
    "    \n",
    "    # ----- Compute (mis)classification loss on generated images -------\n",
    "    \n",
    "    # Let the discriminator (mis)classify the fake images\n",
    "    #TODO!\n",
    "    \n",
    "    # The label is 0 (generated) but we want the discriminator to say 1\n",
    "    #TODO!\n",
    "    \n",
    "    # Compute generator loss on fake images:\n",
    "    # \"how well the generator fools the discriminator\"\n",
    "    # in other words \"how well the generator makes the discriminator\n",
    "    # misclassify fake images as true images\"\n",
    "    #TODO!\n",
    "    \n",
    "    # ------------------ Update step for the generator -----------------\n",
    "    #TODO!\n",
    "    return #TODO!\n",
    "\n",
    "def training_gan(z_dim, n_epochs, optimizer_g, optimizer_d, generator, discriminator, loss, train_loader, device=None):\n",
    "    \"\"\"\n",
    "    Training loop that updates the discriminator and the generator at each iteration\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "\n",
    "        for true_images in train_loader:\n",
    "\n",
    "            true_images = true_images.to(device=device)\n",
    "\n",
    "            # Train the discriminator \n",
    "            #TODO!\n",
    "            \n",
    "            # Train the generator\n",
    "            #TODO!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your GAN\n",
    "\n",
    "Run the cell below to train your GAN.\n",
    "Keep in mind that training unsupervised model is not easy and that it is okay in this assignment if you don't get good results at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "z_dim = 15\n",
    "\n",
    "generator = MyGenerator(z_dim=z_dim)\n",
    "generator.to(device=device)\n",
    "\n",
    "discriminator = MyDiscriminator()\n",
    "discriminator.to(device=device)\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "train_loader_imgs = DataLoader(imgs_train, batch_size=512, shuffle=True)\n",
    "\n",
    "lr_g = 0.001\n",
    "lr_d = 0.0001\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=lr_g)\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=lr_d)\n",
    "\n",
    "training_gan(\n",
    "    z_dim = z_dim,\n",
    "    n_epochs=400,\n",
    "    optimizer_g=optimizer_g, \n",
    "    optimizer_d=optimizer_d, \n",
    "    generator=generator,\n",
    "    discriminator=discriminator, \n",
    "    loss=loss_fn,\n",
    "    train_loader=train_loader_imgs, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "generator.cpu()\n",
    "N_img = 6*10\n",
    "\n",
    "fig, axs = plt.subplots(nrows=6, ncols=10, figsize=(13,9), sharex=True, sharey=True)\n",
    "fig.suptitle(\"Generated images\")\n",
    "for i in range(N_img):\n",
    "    with torch.no_grad():\n",
    "        # Generate random noise\n",
    "        a_z = torch.randn(1, z_dim)\n",
    "        # Create fake image from this noise\n",
    "        a_img = generator(a_z)\n",
    "        axs.flat[i].imshow(a_img[0].permute(1, 2, 0), cmap='Greys')\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=10, ncols=10, figsize=(13,13), sharex=True, sharey=True)\n",
    "fig.suptitle(\"True images\")\n",
    "for i in range(100):\n",
    "    axs.flat[i].imshow(imgs_train[i].permute(1, 2, 0), cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example generation\n",
    "![Example of GAN generation (see GAN_generation image)](GAN_generation.png)\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "Analyse the results in the image above.\n",
    "\n",
    "1. Comment the behavior of the GAN when generating different digits. Do they seem to have different characteristics (not always the exact same shape)? Do they look like the original images? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
