{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder, Variational AutoEncoder and GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "seed = 265\n",
    "torch.manual_seed(seed)\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment we will go through 3 types of unsupervised neural network: AutoEncoder (AE), Variational AutoEncoder (VAE) and Generative Adversarial Network (GAN). In the first section we will also introduce a new type of layer: the transpose convolution as it is widely used in these unsupervised methods.\n",
    "\n",
    "Unsupervised have many advantages including the fact that they don't need labels but they are also harder to train. It is normal if you don't get good results.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Transpose convolution\n",
    "2. AutoEncoder\n",
    "3. Variational AutoEncoder\n",
    "4. GAN\n",
    "\n",
    "## Related videos from the curriculum\n",
    "\n",
    "- [Lecture 15.1 — From PCA to autoencoders](https://www.youtube.com/watch?v=PSOt7u8u23w&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=69)\n",
    "- [Lecture 15.2 — Deep autoencoders](https://www.youtube.com/watch?v=6jhhIPdgkp0&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=70) \n",
    "- [Lecture 15.3 — Deep autoencoders for document retrieval](https://www.youtube.com/watch?v=ZCNbjpcX0yg&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=71)\n",
    "- [Lecture 15.6 — Shallow autoencoders for pre training](https://www.youtube.com/watch?v=xjlvVfEbhz4&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=74)\n",
    "- [Lecture 13 | Generative Models](https://www.youtube.com/watch?v=5WoItGTWV54)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Transpose convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The next sections do not depend on this section, if you are stuck here you can move on to the section 2.\n",
    "\n",
    "--------------------\n",
    "\n",
    "In this assignment we will use a new type of layer: Transpose Convolution. This layer is typically used when we want to use a neural network to generate images (which is the case for AE, VAE and GAN). To make sure that you understand what it does we ask you to implement a simplified version of the transpose convolution operation. \n",
    "\n",
    "TransposeConvolution is NOT the inverse operation of convolution! In mathematics, deconvolution is the operation inverse to convolution. But in machine learning it is often said that TransposeConvolution layers are the symmetric or the inverse of the convolution layers (which is not mathematically true).  The name comes from the fact that the transposed convolution transposes the weight matrix and the input in the actual calculation process compared to the direct convolution.\n",
    "\n",
    "Surprisingly, Andrew did not make any video about this layer. For some ressource you can check this [blog post](https://medium.com/apache-mxnet/transposed-convolutions-explained-with-ms-excel-52d13030c7e8) or this [video](https://www.youtube.com/watch?v=QmCxqsbn5B0). \n",
    "\n",
    "Specifically, for an input tensor ``x`` of shape ``(N, C_in, H_in, W_in)`` and a weight tensor ``weights`` of shape ``(C_in, C_out, kernel_size[0], kernel_size[1])`` a transpose convolution layer returns  a tensor ``out`` of shape ``(N, C_out, H_out, W_out)`` (See the *shape* section of [torch.nn.ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html?highlight=convtranspose2d#torch.nn.ConvTranspose2d) for the formula that gives ``(H_out, W_out)``)), such that:\n",
    "\n",
    "$$out[n, \\; c_{out}, \\; h_{start}:h_{end}, \\; w_{start}:w_{end}] = \\sum_{c_{in} = 0}^{C_{in}-1} x[n, \\; c_{in}, \\; h, \\; w] \\times weights[c_{in}, \\; c_{out}, \\; :h_{end}-h_{start}, \\; :w_{end}-w_{start}]$$\n",
    "\n",
    "For:\n",
    "\n",
    "- $n = 0 ... N - 1$\n",
    "- $c = 0 ... C_{out} - 1$\n",
    "- $h = 0 ... H_{in} - 1$\n",
    "- $w = 0 ... W_{in} - 1$\n",
    "\n",
    "\n",
    "With:\n",
    "- $h_{start} = h*stride[0]$\n",
    "- $w_{start} = w*stride[1]$\n",
    "- $h_{end} = min(H_{out}, h_{start} + kernel_size[0])$\n",
    "- $w_{end} = min(W_{out}, w_{start} + kernel_size[1])$\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "1. Write a ``get_output_size`` function that takes as parameter an input tensor ``x`` of shape ``(N, C_in, H_in, W_in)``, a tuple of int ``kernel_size`` and a tuple of int ``stride`` and that returns the expected output spatial shape ``(H_out, W_out)`` of the transpose convolution operation. See the *shape* section of [torch.nn.ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html?highlight=convtranspose2d#torch.nn.ConvTranspose2d) for the formula that gives this output shape (We ignore the padding here, so ``padding=0``)\n",
    "\n",
    "2. Write a ``apply_transpose_conv`` function that takes as parameter an input tensor ``x`` of shape ``(N, C_in, H_in, W_in)``, a weight tensor ``weights`` of shape ``(C_in, C_out, kernel_size[0], kernel_size[1])`` and a tuple of int ``stride`` and that returns ``out``, a tensor with the right shape and containing the result of the transpose convolution operation between ``x`` and ``weights``. You are of course not allowed to use [torch.nn.ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html?highlight=convtranspose2d#torch.nn.ConvTranspose2d) not its functional counterpart. \n",
    "\n",
    "**NOTE:** We will not really use this function in practice so it's okay to use for loops for the sake of clarity and simplicity in this section\n",
    "\n",
    "**NOTE:** You will be able to test your functions in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_size(x, kernel_size, stride):\n",
    "    #TODO!\n",
    "    return (H_out, W_out)\n",
    "\n",
    "\n",
    "def apply_transpose_conv(x, weights, stride):\n",
    "    # make sure stride is a pair of int\n",
    "    stride = int_to_pair(stride)\n",
    "\n",
    "    # weights\n",
    "    (_ , C_out, k_w, k_h) = weights.shape\n",
    "    kernel_size = [k_w, k_h]\n",
    "\n",
    "    # get output shape\n",
    "    (N, C_in, H_in, W_in) = x.shape\n",
    "    #TODO! \n",
    "    H_out, W_out = \n",
    "\n",
    "    # Initialize output tensor with the right shape \n",
    "    out = torch.zeros((N, C_out, H_out, W_out))\n",
    "\n",
    "    # We will not really use this function in practice so it's okay to use \n",
    "    # for loops for the sake of clarity \n",
    "    ... #TODO! \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test tranpose convolution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transpose_conv(apply_transpose_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. AutoEncoder\n",
    "\n",
    "*related videos from the curriculum*\n",
    "\n",
    "- [Lecture 15.1 — From PCA to autoencoders](https://www.youtube.com/watch?v=PSOt7u8u23w&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=69)\n",
    "- [Lecture 15.2 — Deep autoencoders](https://www.youtube.com/watch?v=6jhhIPdgkp0&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=70) \n",
    "- [Lecture 15.3 — Deep autoencoders for document retrieval](https://www.youtube.com/watch?v=ZCNbjpcX0yg&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=71)\n",
    "- [Lecture 15.6 — Shallow autoencoders for pre training](https://www.youtube.com/watch?v=xjlvVfEbhz4&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=74)\n",
    "- [Lecture 13 | Generative Models](https://www.youtube.com/watch?v=5WoItGTWV54&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=17) (from 20:40 to 27:05)\n",
    "\n",
    "An AutoEncoder (AE) is a neural network that is composed of 2 sub-networks: an Encoder and a Decoder. AE can have many purposes but commonly the main objective is to efficiently represent the data that lies on a non-linear manifold. From this point of view, the AutoEncoder can be seen as a generalization of PCA for data that lies on non-linear manifolds as explained in this [video](https://www.youtube.com/watch?v=PSOt7u8u23w&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=69) of the curriculum.\n",
    "\n",
    "If this is the objective then the Decoder part can be thrown away once the training is done and we can simply use the encoder part to project the data into a latent space of lower dimension ``z_dim`` (in this section, ``z_dim`` will typically be 15 or 30 while the images are 20x20 (=400)). The Decoder is just here to make sure that the encoding is well done and ensure a faithful representation of the data by comparing the reconstructed (i.e decoded) instances of the compressed (i.e encoded) data with the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules \n",
    "\n",
    "In the cell below are defined the following modules that we will need in this section\n",
    "\n",
    "1. **MyEncoder**\n",
    "    - input: image\n",
    "    - output: tensor `z` in latent space (lower dimension than input space)\n",
    "1. **MyDecoder**\n",
    "    - input: tensor `z` in latent space (lower dimension than image space)\n",
    "    - output: reconstructed image\n",
    "1. **MyAE**\n",
    "    - input: image\n",
    "    - output reconstructed image after reduction to latent space\n",
    "    - attributes:\n",
    "      - self.encoder = MyEncoder(z_dim)\n",
    "      - self.decoder = MyDecoder(z_dim)\n",
    "1. **MyClassifier**\n",
    "    Classifier on compressed images:\n",
    "    - input: tensor `z` in latent space (lower dimension than image space)\n",
    "    - output: label predicted\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "1. Take a look at the ``MyEncoder``, ``MyDecoder`` and ``MyAE`` modules. Do the Encoder and Decoder seem really different from any other neural networks so far?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder module: \n",
    "    - input: image\n",
    "    - output: tensor `z` in latent space (lower dimension than input space)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__() \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=6, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=5, kernel_size=5, stride=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=5, out_channels=4, kernel_size=4, stride=1)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=z_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N = x.shape[0]\n",
    "        out = torch.relu(self.conv1(x))\n",
    "        out = torch.relu(self.conv2(out))\n",
    "        out = torch.relu(self.conv3(out))\n",
    "        out = out.view(N, -1)\n",
    "        out = torch.relu(self.fc3(out))\n",
    "        return out\n",
    "\n",
    "class MyDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder module: \n",
    "    - input: tensor `z` in latent space (lower dimension than image space)\n",
    "    - output: reconstructed image\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__() \n",
    "        c1 = 3\n",
    "        self.fc1 = nn.Linear(z_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 18*18)\n",
    "        self.transconv3 = nn.ConvTranspose2d(in_channels=1, out_channels=1,  kernel_size=3, stride=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, z_dim = x.shape\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = out.view(N, 1, 18, 18)\n",
    "        out = torch.sigmoid(self.transconv3(out))\n",
    "        return out\n",
    "\n",
    "class MyAE(nn.Module):\n",
    "    \"\"\"\n",
    "    AutoEncoder\n",
    "    - input: image\n",
    "    - output reconstructed image after reduction to latent space\n",
    "\n",
    "    attributes\n",
    "    - self.encoder = MyEncoder(z_dim)\n",
    "    - self.decoder = MyDecoder(z_dim)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__() \n",
    "        self.encoder = MyEncoder(z_dim)\n",
    "        self.decoder = MyDecoder(z_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.z = self.encoder(x)\n",
    "        out = self.decoder(self.z)\n",
    "        return out\n",
    "\n",
    "class MyClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Classifier on compressed images:\n",
    "    - input: tensor `z` in latent space (lower dimension than image space)\n",
    "    - output: label predicted\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, z_dim, n_labels=10):\n",
    "        super().__init__() \n",
    "        c1 = 12\n",
    "        self.fc1 = nn.Linear(z_dim, 64)\n",
    "        self.transconv1 = nn.ConvTranspose2d(in_channels=1,  out_channels=c1, kernel_size=3, stride=1)\n",
    "        self.transconv2 = nn.ConvTranspose2d(in_channels=c1, out_channels=1,  kernel_size=3, stride=1)\n",
    "        self.fc2 = nn.Linear(144, n_labels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, z_dim = x.shape\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = out.view(N, 1, 8, 8)\n",
    "        out = F.relu(self.transconv1(out)) \n",
    "        out = F.relu(self.transconv2(out))\n",
    "        out = out.view(N, -1)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_true_VS_reconstructed(ae, imgs):\n",
    "    \"\"\"\n",
    "    Plot side by side original images with their reconstructed counterpart using a trained AE\n",
    "    \"\"\"\n",
    "    ae.eval()\n",
    "    N_img = 25\n",
    "    fig, axs = plt.subplots(nrows=5, ncols=10, figsize=(10,6), sharex=True, sharey=True)\n",
    "    for i, img in enumerate(imgs[:N_img]):\n",
    "        with torch.no_grad():\n",
    "            out = ae(img.unsqueeze(0))\n",
    "            # True image\n",
    "            axs.flat[2*i].imshow(img.permute(1, 2, 0), cmap='Greys')\n",
    "            # Reconstruction\n",
    "            axs.flat[2*i + 1].imshow(out.squeeze(0).permute(1, 2, 0), cmap='Greys') \n",
    "            # Set ax title for the first row\n",
    "            if i<5:\n",
    "                axs.flat[2*i].set_title(\"True\\nimage\")\n",
    "                axs.flat[2*i + 1].set_title(\"AE recon-\\nstruction\")\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_train, data_val, data_test = load_MNIST(data_path='../data/')\n",
    "imgs_train = [img for img, _ in data_train]\n",
    "label_train = [label for _, label in data_train]\n",
    "imgs_val = [img for img, _ in data_val]\n",
    "label_val = [label for _, label in data_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop of an AutoEncoder\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "1. Write a function ``training_ae`` that trains an auto-encoder. The objective is that the encoder part of the AE gets good at summarizing the data in the latent space and that the decoder gets good at reconstructing the images from the lower dimensional vectors in the latent space.\n",
    "\n",
    "Note that:\n",
    "- There is no label in the dataset (so no label in ``train_loader`` neither)\n",
    "- The loss function is computed by comparing the outputs with the original images. We will typically call this function with loss_fn = `` nn.MSELoss()`` so that each reconstructed pixel is compared to its original couterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_ae(n_epochs, optimizer, model, loss_fn, train_loader, device=None):\n",
    "    \"\"\"\n",
    "    Train an AE. No labels required\n",
    "    \"\"\"\n",
    "    #TODO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your AutoEncoder\n",
    "\n",
    "Train your AE.\n",
    "Keep in mind that training unsupervised model is not easy and that it is okay in this assignment if you don't get good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "z_dim = 15\n",
    "\n",
    "ae = MyAE(z_dim=z_dim)\n",
    "ae.to(device=device)\n",
    "\n",
    "train_loader_imgs = DataLoader(imgs_train, batch_size=512, shuffle=True)\n",
    "val_loader_imgs = DataLoader(imgs_val, batch_size=512, shuffle=True)\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "optimizer = optim.Adam(ae.parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "_ = training_ae(\n",
    "    n_epochs = 30,\n",
    "    optimizer = optimizer,\n",
    "    model = ae,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader_imgs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot original images VS reconstruction\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "Analyse your results (regardless of how good/bad they might be)\n",
    "\n",
    "1. Are you satisfied by the reconstructions? If not, what seems to be the problem? Mode collapse? Overfitting? Underfitting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ae.to(device=torch.device('cpu')) \n",
    "fig, axs = plot_true_VS_reconstructed(ae, imgs_train)\n",
    "fig.suptitle(\"Training dataset\", fontsize=15)\n",
    "plt.show()\n",
    "fig, axs = plot_true_VS_reconstructed(ae, imgs_val)\n",
    "fig.suptitle(\"Validation dataset\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Example of AE results on the training dataset\n",
    "\n",
    "![Example of AE results on the training dataset (see AE_train_reconstruction image)](AE_val_reconstruction.png)\n",
    "\n",
    "### Example of AE results on the validation dataset\n",
    "\n",
    "![Example of AE results on the validation dataset (see AE_train_reconstruction image)](AE_val_reconstruction.png)\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "Analyse the results in the images above.\n",
    "\n",
    "1. Would you say that the reconstructed images look good?\n",
    "1. Now take a closer look at the bottom right pair in the training dataset example. What can you say? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress images using the Encoder part of our trained AE\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "\n",
    "1. Use the ``transform_images`` function defined in the cell below to compress ``train_loader_imgs`` and ``val_loader_imgs`` and store them in ``compressed_imgs_train`` and ``compressed_imgs_val``. To do so call the function with the encoder part of our AE (accessible using ``ae.encoder``)\n",
    "2. We have divided by more than 10 the size of the original data, cite some obvious advantages of this compression. \n",
    "3. Recalling this [video](https://www.youtube.com/watch?v=ZCNbjpcX0yg&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=71), this other [video](https://www.youtube.com/watch?v=xjlvVfEbhz4&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=74) and this [lecture](https://www.youtube.com/watch?v=5WoItGTWV54&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=16) (from 20:40 to 27:05 ) from the curriculum, what could be another application of these compressed data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_images(model, dataloader, device=None):\n",
    "    \"\"\"\n",
    "    Apply ``model`` to ``dataloader`` and returns the output \n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "    model.eval()\n",
    "    transformed_imgs = []\n",
    "    for imgs in dataloader:\n",
    "        with torch.no_grad():\n",
    "            imgs = imgs.to(device=device) \n",
    "            outputs = model(imgs)\n",
    "            transformed_imgs.append(outputs.clone().detach())\n",
    "    transformed_imgs = torch.cat(transformed_imgs)\n",
    "    return transformed_imgs\n",
    "\n",
    "\n",
    "ae.to(device=device)\n",
    "\n",
    "# Keep shuffle to False here otherwise imgs won't correspond to labels anymore\n",
    "train_loader_imgs = DataLoader(imgs_train, batch_size=256, shuffle=False) \n",
    "val_loader_imgs = DataLoader(imgs_val, batch_size=256, shuffle=False)\n",
    "\n",
    "# Use the encoder part of our ae to compress our images\n",
    "#TODO!\n",
    "compressed_imgs_train =\n",
    "compressed_imgs_val = \n",
    "\n",
    "# Re-associate compressed images with their corresponding labels\n",
    "data_compressed_train = list(zip(compressed_imgs_train, label_train))\n",
    "data_compressed_val = list(zip(compressed_imgs_val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification on compressed images\n",
    "\n",
    "In the cell below we train ``MyClassifier`` (defined in the \"Modules\" subsection at the beginning of this section), a classifier that takes as input a tensor `z` in the latent space (lower dimension than the image space) and find the label corresponding to the original image.\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "1. Recalling that we started from 20x20 (=400) images that were compressed into a 15 dimensional space, are you satisfied by the classification performance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 30\n",
    "lr=0.001\n",
    "\n",
    "n_labels = 10\n",
    "\n",
    "train_loader_compressed = DataLoader(data_compressed_train, batch_size=512, shuffle=True)\n",
    "val_loader_compressed = DataLoader(data_compressed_val, batch_size=512, shuffle=True)\n",
    "\n",
    "classifier_compressed = MyClassifier(z_dim=z_dim, n_labels=n_labels)\n",
    "classifier_compressed.to(device=device) \n",
    "optimizer = optim.Adam(classifier_compressed.parameters(), lr=lr)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\" ============== Training Classifier on compressed images ============== \")\n",
    "_ = train(\n",
    "    n_epochs = epoch,\n",
    "    optimizer = optimizer,\n",
    "    model = classifier_compressed,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader_compressed,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" ============== Classification on compressed images ============== \")\n",
    "_ = compute_accuracy(classifier_compressed,train_loader_compressed)\n",
    "print(\"Training\")\n",
    "_ = compute_accuracy(classifier_compressed, train_loader_compressed)\n",
    "print(\"Validation\")\n",
    "_ = compute_accuracy(classifier_compressed, val_loader_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct images using the Decoder part of our trained AE\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "\n",
    "1. Use the ``transform_images`` function defined earlier to reconstruct ``train_loader_compressed_imgs`` and ``val_loader_compressed_imgs`` and store them in ``reconstructed_imgs_train`` and ``reconstructed_imgs_val``. To do so call the function with the decoder part of our AE (accessible using ``ae.decoder``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep shuffle to False here otherwise imgs won't correspond to labels anymore\n",
    "train_loader_compressed_imgs = DataLoader(compressed_imgs_train, batch_size=256, shuffle=False)\n",
    "val_loader_compressed_imgs = DataLoader(compressed_imgs_val, batch_size=256, shuffle=False)    \n",
    "\n",
    "# Use the decoder part of our ae to reconstruct our compressed images\n",
    " #TODO!\n",
    "reconstructed_imgs_train = \n",
    "reconstructed_imgs_val =\n",
    "\n",
    "# Re-associate reconstructed images with their corresponding labels\n",
    "data_reconstructed_train = list(zip(reconstructed_imgs_train, label_train))\n",
    "data_reconstructed_val = list(zip(reconstructed_imgs_val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification on reconstructed images VS true images\n",
    "\n",
    "In the cell below we train 2 instances of ``LeNet5`` (defined in ``utils.py``), a regular classifier that takes an image and find its corresponding label. The first instance is trained on the reconstructed images and the second one is trained on the original images\n",
    "\n",
    "In the next cell we evaluate the performance of our classifiers.\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "1. Compare the performance of the 2 models. Are you satisfied with the classification on the reconstructed images?\n",
    "2. If so, can we throw away our original images (and keep only our trained AE and compressed images)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 30\n",
    "lr=0.001\n",
    "\n",
    "print(\" ============== Training Classifier on reconstructed images ============== \")\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "train_loader_reconstructed = DataLoader(data_reconstructed_train, batch_size=512, shuffle=True)\n",
    "val_loader_reconstructed = DataLoader(data_reconstructed_val, batch_size=512, shuffle=True)\n",
    "\n",
    "classifier_reconstructed = LeNet5(n_labels=n_labels)\n",
    "classifier_reconstructed.to(device=device) \n",
    "optimizer = optim.Adam(classifier_reconstructed.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train(\n",
    "    n_epochs = epoch,\n",
    "    optimizer = optimizer,\n",
    "    model = classifier_reconstructed,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader_reconstructed,\n",
    ")\n",
    "\n",
    "print(\" ============== Training Classifier on true images ============== \")\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "train_loader_true = DataLoader(data_train, batch_size=512, shuffle=True)\n",
    "val_loader_true = DataLoader(data_val, batch_size=512, shuffle=True)\n",
    "\n",
    "classifier_true = LeNet5(n_labels=n_labels)\n",
    "classifier_true.to(device=device) \n",
    "optimizer = optim.Adam(classifier_true.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "_ = train(\n",
    "    n_epochs = epoch,\n",
    "    optimizer = optimizer,\n",
    "    model = classifier_true,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader_true,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" ============== Classification on reconstructed images ============== \")\n",
    "print(\"Training\")\n",
    "_ = compute_accuracy(classifier_reconstructed, train_loader_reconstructed)\n",
    "print(\"Validation\")\n",
    "_ = compute_accuracy(classifier_reconstructed, val_loader_reconstructed)\n",
    "\n",
    "print(\" ============== Classification on true images ============== \")\n",
    "print(\"Training\")\n",
    "_ = compute_accuracy(classifier_true, train_loader_true)\n",
    "print(\"Validation\")\n",
    "_ = compute_accuracy(classifier_true, val_loader_true)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "71c2cb666ff353b4e7b5c350d66179fa0af5c84ce239ad9fa105d94543f3ad59"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
