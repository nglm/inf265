{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder, Variational AutoEncoder and GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "seed = 265\n",
    "torch.manual_seed(seed)\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General instructions\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this assignment we will go through 3 types of unsupervised neural network: AutoEncoder (AE), Variational AutoEncoder (VAE) and Generative Adversarial Network (GAN). In the first section we will also introduce a new type of layer: the transpose convolution as it is widely used in these unsupervised methods.\n",
    "\n",
    "Unsupervised have many advantages including the fact that they don't need labels but they are also harder to train. It is normal if you don't get good results.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Transpose convolution\n",
    "2. AutoEncoder\n",
    "3. Variational AutoEncoder\n",
    "4. GAN\n",
    "\n",
    "## Related videos from the curriculum\n",
    "\n",
    "- [Lecture 15.1 — From PCA to autoencoders](https://www.youtube.com/watch?v=PSOt7u8u23w&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=69)\n",
    "- [Lecture 15.2 — Deep autoencoders](https://www.youtube.com/watch?v=6jhhIPdgkp0&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=70) \n",
    "- [Lecture 15.3 — Deep autoencoders for document retrieval](https://www.youtube.com/watch?v=ZCNbjpcX0yg&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=71)\n",
    "- [Lecture 15.6 — Shallow autoencoders for pre training](https://www.youtube.com/watch?v=xjlvVfEbhz4&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=74)\n",
    "- [Lecture 13 | Generative Models](https://www.youtube.com/watch?v=5WoItGTWV54)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. (Reminders) Encoder and Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules \n",
    "\n",
    "In the cell below are defined the following modules that we will need in this section\n",
    "\n",
    "1. **MyEncoder**\n",
    "    - input: image\n",
    "    - output: tensor `z` in latent space (lower dimension than input space)\n",
    "1. **MyDecoder**\n",
    "    - input: tensor `z` in latent space (lower dimension than image space)\n",
    "    - output: reconstructed image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder module: \n",
    "    - input: image\n",
    "    - output: tensor `z` in latent space (lower dimension than input space)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__() \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=6, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=5, kernel_size=5, stride=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=5, out_channels=4, kernel_size=4, stride=1)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=z_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N = x.shape[0]\n",
    "        out = torch.relu(self.conv1(x))\n",
    "        out = torch.relu(self.conv2(out))\n",
    "        out = torch.relu(self.conv3(out))\n",
    "        out = out.view(N, -1)\n",
    "        out = torch.relu(self.fc3(out))\n",
    "        return out\n",
    "\n",
    "class MyDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder module: \n",
    "    - input: tensor `z` in latent space (lower dimension than image space)\n",
    "    - output: reconstructed image\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__() \n",
    "        c1 = 3\n",
    "        self.fc1 = nn.Linear(z_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 18*18)\n",
    "        self.transconv3 = nn.ConvTranspose2d(in_channels=1, out_channels=1,  kernel_size=3, stride=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, z_dim = x.shape\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = out.view(N, 1, 18, 18)\n",
    "        out = torch.sigmoid(self.transconv3(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils \n",
    "\n",
    "Some useful functions:\n",
    "- **plot_generated_images**: Plot images generated by a VAE\n",
    "- **training_vae**: Training loop for a VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_images(vae):\n",
    "    \"\"\"\n",
    "    Plot images generated by a VAE\n",
    "    \"\"\"\n",
    "    vae.eval()\n",
    "    N_img = 100\n",
    "    fig, axs = plt.subplots(nrows=10, ncols=10, figsize=(13,13), sharex=True, sharey=True, tight_layout=True)\n",
    "    fig.suptitle(\"Image generation\", fontsize=15)\n",
    "    for i in range(N_img):\n",
    "        with torch.no_grad():\n",
    "            a_z = torch.randn(1,vae.z_dim)\n",
    "            a_img = vae.decoder(a_z)\n",
    "            axs.flat[i].imshow(a_img[0].permute(1, 2, 0), cmap='Greys')\n",
    "    return fig, axs\n",
    "\n",
    "def training_vae(n_epochs, optimizer, model, loss_fn, train_loader, kld_weight=True, device=None, mse_threshold=0.08,\n",
    "                w_min=10e-9, w_max=10e-3):\n",
    "    \"\"\"\n",
    "    Training loop for a VAE\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "    model.train()\n",
    "    \n",
    "    # Weighted version of the loss\n",
    "    if kld_weight:\n",
    "        w = w_min\n",
    "    else:\n",
    "        w = 1\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        loss_train_mse = 0.\n",
    "        loss_train_kld = 0.\n",
    "\n",
    "        for imgs in train_loader:\n",
    "\n",
    "            imgs = imgs.to(device=device) \n",
    "\n",
    "            outputs = model(imgs)\n",
    "            # Final loss is the sum of the 2 terms (with potentially a weight term)\n",
    "            mse_loss, kld_loss = loss_fn(outputs, imgs, model.mu, model.logvar)\n",
    "            \n",
    "            # Update weight on KLD loss\n",
    "            with torch.no_grad():\n",
    "                if kld_weight:\n",
    "                    if mse_loss < mse_threshold:\n",
    "                        w = min(w*2, w_max)\n",
    "                    else:\n",
    "                        w = max(w/2, w_min)\n",
    "                \n",
    "            loss = mse_loss + w*kld_loss\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "            loss_train_mse += mse_loss.item()\n",
    "            loss_train_kld += kld_loss.item()\n",
    "\n",
    "        \n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}  |  MSE loss {:.3f}  |  KLD loss {:.5}  | KLD weight {:.8f}'.format(\n",
    "                datetime.now().strftime(\"%H:%M:%S\"), \n",
    "                epoch,\n",
    "                loss_train / len(train_loader),\n",
    "                loss_train_mse / len(train_loader),\n",
    "                loss_train_kld / len(train_loader),\n",
    "                w,\n",
    "            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "# It's already hard enough to train VAE, a subset of MNIST will be more than enough.\n",
    "labels_kept = [0,1,2,3]\n",
    "n_labels = len(labels_kept)\n",
    "data_train, data_val, data_test = load_MNIST(label_list=labels_kept)\n",
    "imgs_train = [img for img, _ in data_train]\n",
    "imgs_val = [img for img, _ in data_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Variational AutoEncoder\n",
    "\n",
    "*related videos from the curriculum*\n",
    "\n",
    "- [Lecture 13 | Generative Models](https://www.youtube.com/watch?v=5WoItGTWV54&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=17) \n",
    "  - from 27:05 to 31:05: Introducting VAE \n",
    "  - *Let's forget about tractability :) *\n",
    "  - from 40:55 to 44:00: VAE loss and VAE training\n",
    "  - from 44:00 to 49:00: Generating data using VAE and summary \n",
    "\n",
    "**Introduction to VAE**\n",
    "\n",
    "A Variational AutoEncoder (VAE) is a neural network that is similar to a AE in its structure as it is composed of 2 sub-networks: an Encoder and a Decoder. However, our main objective is no longer to efficiently represent some data lying on a non-linear manifold. Instead we want to generate some new data that would look like the training data but that is just a simple copy of a training input! We want **new** data.\n",
    "\n",
    "To do so, we want our model to first get a good representation of what real data look like and then to be able to generate new plausible instances. The *get a good representation* part seems to be similar to what an Encoder can do and the *generate plausible instances* part to what a Decoder can do. However we can not really *generate new* data with a Decoder but just *reconstruct*. The *generate* part is actually what makes VAE different from AE and it is achieved in 2 steps: \n",
    "\n",
    "- a reparameterization step in the forward pass between the Encoder and Decoder\n",
    "- a KL-divergence term added to the loss function \n",
    "\n",
    "These 2 steps aim at forcing the elements of latent space to look like normally distributed samples. Once the training is finished, this forcing will allow us to generate new data by simply giving a random normally distributed sample to the Decoder (and the Encoder can be thrown away, so this is the opposite of AE where we could throw away the Decoder and keep the Encoder).\n",
    "\n",
    "**Reparameterization**\n",
    "\n",
    "The reparameterization step consists in defining the latent vector ``z`` not as the output of the encoder but as a random sample from $\\mathcal{N}$(``mu``, ``std``$)$ where ``mu`` and ``std`` are the actual outputs of the Encoder (for computational reasons the Encoder actually returns ``mu`` and ``logvar (=log(std**2))``)\n",
    "\n",
    "**KL-divergence**\n",
    "\n",
    "The second difference between a VAE and an AE is in the loss function. In addition to the reconstruction term we want to force the Encoder to learn the parameters of a normal distribution. There exists a measure for that, the Kullback–Leibler divergence or simply KL-divergence ([Wikipedia](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), [PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html?highlight=kld#torch.nn.KLDivLoss)). The KL-divergence measures how one probability distribution $P$ is different from a second $Q$ by computing the following:\n",
    "\n",
    "$$D_{KL}(P||Q) = \\int_{-\\infty}^{+\\infty} p(x)log\\Big(\\frac{p(x)}{q(x)}\\Big) \\,dx \\$$\n",
    "\n",
    "Where $p$ and $q$ are the probability densities of the probability distributions $P$ and $Q$. This measure can be used for different purposes and have as many interpretations (see [Interpretations](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Interpretations) section from the Wikipedia page) but in the context of Variational AutoEncoder we will interpret $Q$ as our prior and $P$ as our *true* distribution and $D_{KL}(P||Q)$ can then be interpreted as the information lost when our prior $Q$ is used to approximate $P$. In our very specific case where $Q \\sim \\mathcal{N}(0, 1)$ and $P \\sim \\mathcal{N}$(``mu``, ``std``$)$, $D_{KL}(P||Q)$ can be formulated as follows (see [Multivariate normal distributions](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions) section from the Wikipedia page):\n",
    "\n",
    "$$D_{KL}(P||Q) = \\frac{1}{2} \\sum_{i=0}^{z_{dim}-1} \\Big( \\sigma_i^2 + \\mu_i^2 -1 -log(\\sigma_i^2) \\Big) \\qquad \\text{with std} = \\begin{bmatrix}\\sigma_0 \\cdots \\sigma_{z_{dim}-1} \\end{bmatrix} \\quad \\text{and mu} = \\begin{bmatrix}\\mu_0 \\cdots \\mu_{z_{dim}-1} \\end{bmatrix}  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "## TODO\n",
    "\n",
    "### MyVAE class: A variational AutoEncoder\n",
    "\n",
    "Complete the ``MyVAE`` class below (that is the variational counterpart of the ``MyAE`` class implemented in section 2). You don't have to start from scratch, you can re-use the ``MyEncoder`` and the ``MyDecoder`` classes from section 2. However a few details must be adapted:\n",
    "\n",
    "1. Encoder must return 2 tensors of shape ``(N, z_dim)``: one for ``mu`` and one for ``logvar (=log(var)=log(std**2))`` **or equivalently** use ``MyEncoder`` with ``z_dim = 2*z_dim`` and then define ``mu`` and ``logvar`` in the forward method as follows: \n",
    "\n",
    "  ```\n",
    "  self.mu_logvar = self.encoder(x)               # Output of the encoder, shape=(N, 2*z_dim)\n",
    "  self.mu = self.mu_logvar[:,:self.z_dim]        # mu                   , shape=(N, z_dim)   (1st half of the encoded vector)\n",
    "  self.logvar = self.mu_logvar[:,self.z_dim:]    # logvar               , shape=(N, z_dim)   (2nd half of the encoded vector)\n",
    "  ```\n",
    "\n",
    "2. On top of returning the reconstructed image, the forward pass of your VAE must store ``mu`` and ``log`` (as suggested in the lines above) because we'll need them when computing the KL-divergence term of the loss function.\n",
    "\n",
    "3. A ``reparameterization`` method must be added that draws a sample $z$ (of shape ``(N, z_dim)``) from $\\mathcal{N}(0,1)$ and return $z \\times std + mu$ so that it corresponds to a sample from $\\mathcal{N}$(``mu``, ``std``$)$ (Reminder: ``logvar = log(std**2))`` so ``std = exp(logvar/2)``). **Hint** you can use [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html?highlight=randn#torch.randn) or [torch.randn_like](https://pytorch.org/docs/stable/generated/torch.randn_like.html?highlight=randn#torch.randn_like) or [torch.nn.init.normal_](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.normal_). This method is to be called between the Encoder and the Decoder both during the training and can also be thrown away once the training is complete.\n",
    "\n",
    "4. Decoder is exactly same as for an AutoEncoder\n",
    "\n",
    "5. Write a ``generate_images`` method that takes as parameter an integer ``N_imgs`` defining the number of images to generate and returns ``imgs_generated`` (shape ``(N_imgs, C_in, H_in, W_in)``) the images generated by the VAE. **Hint** you can use [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html?highlight=randn#torch.randn)\n",
    "\n",
    "### loss_VAE: A loss adapted to VAE\n",
    "\n",
    "Complete the ``loss_VAE`` function below where \n",
    "- ``inputs`` is the original images (shape ``(N, C_in, H_in, W_in)``)\n",
    "- ``outputs`` is the reconstructed images (shape ``(N, C_in, H_in, W_in)``), \n",
    "- ``mu`` and ``logvar (=log(std**2)`` are the outputs of the Encoder representing the parameters of our normal distribution $P$  (both of shape ``(N, z_dim)``)). \n",
    "\n",
    "It returns the 2 terms of the VAE loss function:\n",
    "- ``mse_loss``, the reconstruction term. **Hint** you can use [F.mse_loss](https://pytorch.org/docs/stable/nn.functional.html?highlight=mse_loss#torch.nn.functional.mse_loss) with ``reduction=\"mean\"``\n",
    "- ``kld_loss``, the KL-divergence term. which is defined in the cell above. To adapt formula to batch computations, we need to re-write it as follows:\n",
    "\n",
    "$$D_{KL}(P||Q) =\\text{mean}\\Big( \\frac{1}{2} \\sum_{i=0}^{z_{dim}-1} \\Big( \\sigma_{:,i}^2 + \\mu_{:,i}^2 -1 -log(\\sigma_{:,i}^2) \\Big) \\Big) \\qquad \\text{with std} = \\begin{bmatrix}\\sigma_{:,0} \\cdots \\sigma_{:, z_{dim}-1} \\end{bmatrix} \\quad \\text{and mu} = \\begin{bmatrix}\\mu_{:,0} \\cdots \\mu_{:, z_{dim}-1} \\end{bmatrix}  $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__() \n",
    "        # Latent space dimension\n",
    "        self.z_dim = z_dim\n",
    "        # Encoder similar to what we used for the AE but used to encode both mu and logvar \n",
    "        #TODO\n",
    "        self.encoder = \n",
    "        # There is no difference between a VAE and AE decoder\n",
    "        #TODO\n",
    "        self.decoder = \n",
    "\n",
    "    def reparameterize(self):\n",
    "        \"\"\"\n",
    "        Reparameterization: draw a sample z of shape (N, z_dim) (z~N(mu, logvar))\n",
    "\n",
    "        mu and logvar should be accessible via self.mu and self.logvar\n",
    "        \"\"\"\n",
    "        # Initialize a vector z with the right shape whose elements are drawn from a normal distribution N(0, 1)\n",
    "        #TODO\n",
    "        # Shift z so that it is equivalent to a sample drawn from N(mu, std)\n",
    "        #TODO\n",
    "        return z\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encode the data and output the estimated parameters mu and logvar\n",
    "        #TODO\n",
    "        # mu can be defined as the first half of the encoder output\n",
    "        #TODO\n",
    "        self.mu =\n",
    "        # logvar can be defined as the second half of the encoder output\n",
    "        #TODO\n",
    "        self.logvar =\n",
    "\n",
    "        # Reparameterization: draw a sample z of shape (N, z_dim) (z~N(mu, logvar))\n",
    "        # by calling the reparameterize method\n",
    "        #TODO\n",
    "\n",
    "        # Generate (decode) an image from the sample z\n",
    "        #TODO\n",
    "        return out\n",
    "\n",
    "    def generate(self, N_imgs):\n",
    "        \"\"\"\n",
    "        Generate new images by giving sampled latent vectors from N(0,1) to the decoder\n",
    "        \"\"\"\n",
    "        #TODO\n",
    "        return imgs_generated\n",
    "\n",
    "def loss_VAE(inputs, outputs, mu, logvar):\n",
    "    \"\"\"\n",
    "    Loss for a VAE: a reconstruction term (mse loss) and a distribution term (kl divergence)\n",
    "    \"\"\"\n",
    "    # Regular reconstruction term using the mse loss, same as what we used for the AutoEncoder\n",
    "    #TODO\n",
    "    mse_loss = \n",
    "    # Distribution term: force the latent space to behave like a normal distribution\n",
    "    # Special case of the KL divergence when the prior is Q~N(0,1) and P~N(mu, std)\n",
    "    #TODO\n",
    "    kld_loss = \n",
    "    return mse_loss, kld_loss\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your Variational AutoEncoder\n",
    "\n",
    "Run the cell below to train your VAE.\n",
    "Keep in mind that training unsupervised model is not easy and that it is okay in this assignment if you don't get good results at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "z_dim = 15\n",
    "\n",
    "vae = MyVAE(z_dim=z_dim)\n",
    "vae.to(device=device)\n",
    "\n",
    "train_loader_imgs = DataLoader(imgs_train, batch_size=512, shuffle=True)\n",
    "val_loader_imgs = DataLoader(imgs_val, batch_size=512, shuffle=True)\n",
    "\n",
    "lr = 0.0001\n",
    "\n",
    "optimizer = optim.Adam(vae.parameters(), lr=lr)\n",
    "loss_fn = loss_VAE\n",
    "\n",
    "training_vae(\n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = vae,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader_imgs,\n",
    "    kld_weight = False,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.to(device=torch.device('cpu')) \n",
    "fig, axs = plot_true_VS_reconstructed(vae, imgs_train)\n",
    "fig.suptitle(\"Training dataset\", fontsize=15)\n",
    "plt.show()\n",
    "fig, axs = plot_true_VS_reconstructed(vae, imgs_val)\n",
    "fig.suptitle(\"Validation dataset\", fontsize=15)\n",
    "plt.show()\n",
    "fig, axs = plot_generated_images(vae)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of VAE results on the training dataset\n",
    "![Example of VAE reconstruction results on the training dataset with the KLD weight term (see VAE_train_reconstruction image)](VAE_train_reconstruction.png)\n",
    "\n",
    "### Example of VAE results on the validation dataset\n",
    "\n",
    "![Example of VAE reconstruction results on the validation dataset with the KLD weight term (see VAE_val_reconstruction image)](VAE_val_reconstruction.png)\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "Analyse the results in the image above.\n",
    "\n",
    "1. Comment the behavior of the VAE when reconstructing different digits.\n",
    "2. Mode collapse is a very common problem when training VAE (and other unsupervised method), can you explain what this problem is?\n",
    "\n",
    "### Example of VAE generation\n",
    "![Example of VAE generation results with the KLD weight term (see VAE_generation image)](VAE_generation.png)\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "Analyse the results in the image above.\n",
    "\n",
    "1. Do the generated images look similar to the outputs of the reconstructed images?\n",
    "1. Would you say that the VAE learnt well the parameters of a normal distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your Variational AutoEncoder: with a weighted KLD loss\n",
    "\n",
    "In order to fix the 'mode collapse' problem and to make sure our model learns to both reconstruct images and organize the latent space we will add a weight on the KL divergence term. We will first define this weight as extremely low so that the model learns first how to reconstruct and once the reconstruction is considered good enough we will increasingly amplify this weight so that the model re-organize the latent space properly.\n",
    "\n",
    "Run the cell below to train your VAE.\n",
    "Keep in mind that training unsupervised model is not easy and that it is okay in this assignment if you don't get good results at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = MyVAE(z_dim=z_dim)\n",
    "vae.to(device=device)\n",
    "epochs = 300\n",
    "lr = 0.0001\n",
    "optimizer = optim.Adam(vae.parameters(), lr=lr)\n",
    "kld_weight = True\n",
    "\n",
    "training_vae(\n",
    "    n_epochs = epochs,\n",
    "    optimizer = optimizer,\n",
    "    model = vae,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader_imgs,\n",
    "    kld_weight = kld_weight,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.to(device=torch.device('cpu')) \n",
    "fig, axs = plot_true_VS_reconstructed(vae, imgs_train)\n",
    "fig.suptitle(\"Training dataset\", fontsize=15)\n",
    "plt.show()\n",
    "fig, axs = plot_true_VS_reconstructed(vae, imgs_val)\n",
    "fig.suptitle(\"Validation dataset\", fontsize=15)\n",
    "plt.show()\n",
    "fig, axs = plot_generated_images(vae)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example reconstruction\n",
    "![Example of VAE reconstruction results on the training dataset with the KLD weight term (see VAE_weighted_train_reconstruction image)](VAE_weighted_train_reconstruction.png)\n",
    "\n",
    "![Example of VAE reconstruction results on the validation dataset with the KLD weight term (see VAE_weighted_val_reconstruction image)](VAE_weighted_val_reconstruction.png)\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "Analyse the results in the image above.\n",
    "\n",
    "1. Comment the behavior of the VAE when reconstructing different digits.\n",
    "\n",
    "### Example generation\n",
    "![Example of VAE generation results with the KLD weight term (see VAE_weighted_generation image)](VAE_weighted_generation.png)\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "Analyse the results in the image above.\n",
    "\n",
    "1. Do the generated images look similar to the outputs of the reconstructed images?\n",
    "1. Would you say that the VAE learnt well the parameters of a normal distribution?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "71c2cb666ff353b4e7b5c350d66179fa0af5c84ce239ad9fa105d94543f3ad59"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
