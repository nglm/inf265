{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To generate the trained embedding, just use the functions from the project 3, so if those functions got improved in the meantime, just copy paste again from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Copy paste from project 3\n",
    "from cbow import CBoW, create_dataset\n",
    "from train_embedding_utils import similarity_matrix, find_N_closest, model_selection, model_evaluation, set_device\n",
    "\n",
    "seed = 265\n",
    "torch.manual_seed(seed)\n",
    "device = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the dataset:    347870\n",
      "Total number of words in the dataset:    49526\n",
      "Number of distinct words kept:           324\n"
     ]
    }
   ],
   "source": [
    "# List of words contained in the dataset\n",
    "generated_path = '../generated/'\n",
    "list_words_train = torch.load(generated_path + 'words_train.pt')\n",
    "list_words_val = torch.load(  generated_path + 'words_val.pt')\n",
    "list_words_test = torch.load( generated_path + 'words_test.pt')\n",
    "\n",
    "# vocab contains the vocabulary found in the data, associating an index to each word\n",
    "vocab = torch.load( generated_path + 'vocabulary.pt')\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Total number of words in the dataset:   \", len(list_words_train))\n",
    "print(\"Total number of words in the dataset:   \", len(list_words_val))\n",
    "print(\"Number of distinct words kept:          \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "    context_size, embedding_dim, occ_max=np.inf, use_weight=True, use_unk_limit=True,\n",
    "    black_list=[\"<unk>\", \",\", \".\", \"!\", \"?\", '\"'],\n",
    "    generated_path='../generated/'\n",
    "):\n",
    "    \"\"\"\n",
    "    Warning: this function relies heavily on global variables and default parameters\n",
    "    \"\"\"\n",
    "    device = set_device()\n",
    "    \n",
    "    print(\"=\"*59)\n",
    "    print(\n",
    "        \"Context size  %d  |  Embedding dim  %d  |  occ_max  %s  |  weights %s\"\n",
    "        %(context_size, embedding_dim, str(occ_max), str(use_weight) )\n",
    "    )\n",
    "    print(\n",
    "        \"use_unk_limit %s \" %(str(use_unk_limit))\n",
    "    )\n",
    "    print(\"Black_list: %s\" %\" | \".join(black_list))\n",
    "\n",
    "    # -------------- Datasets -------------\n",
    "    data_train_ngram = create_dataset(list_words_train, vocab, context_size, black_list=black_list, occ_max=occ_max, use_unk_limit=use_unk_limit)\n",
    "    data_val_ngram = create_dataset(list_words_val,     vocab, context_size, black_list=black_list, occ_max=occ_max, use_unk_limit=use_unk_limit)\n",
    "    data_test_ngram = create_dataset(list_words_test,   vocab, context_size, black_list=black_list, occ_max=occ_max, use_unk_limit=use_unk_limit)\n",
    "\n",
    "    print(len(data_train_ngram))\n",
    "    print(len(data_val_ngram))\n",
    "    print(len(data_test_ngram))\n",
    "\n",
    "    batch_size = 512\n",
    "    n_epochs = 30\n",
    "    train_loader = DataLoader(data_train_ngram, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(data_val_ngram, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(data_test_ngram, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # ------- Loss function parameters -------\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # ---------- Optimizer parameters --------\n",
    "    list_lr = [0.001]\n",
    "    optimizers = [optim.Adam for _ in range(len(list_lr))]\n",
    "    optim_params = [{\n",
    "            \"lr\" : list_lr[i],\n",
    "        } for i in range(len(list_lr))]\n",
    "\n",
    "    # -------- Model class parameters --------\n",
    "    model_class = CBoW\n",
    "    model_params = (vocab_size, embedding_dim, context_size)\n",
    "    \n",
    "    # ----------- Model name -----------------\n",
    "    model_name = generated_path +'CBoW_'\n",
    "    hyperparams = {\n",
    "        \"context\": context_size,\n",
    "        \"emb_dim\": embedding_dim,\n",
    "        \"weights\": use_weight,\n",
    "        \"unk_limit\": use_unk_limit,\n",
    "        \"occ_max\": occ_max, \n",
    "    }\n",
    "    model_name += \"_\".join(['%s=%s' %(k, v) for (k, v) in hyperparams.items()]) + '.pt'\n",
    "\n",
    "    # ----------- Model selection -----------\n",
    "    model_cbow, i_best_model = model_selection(\n",
    "        model_class, model_params, optimizers, optim_params,\n",
    "        n_epochs, loss_fn,\n",
    "        train_loader, val_loader,\n",
    "        seed=265, model_name=model_name, device=device\n",
    "    )\n",
    "\n",
    "    # ----------- Model evaluation -----------\n",
    "    test_acc = model_evaluation(model_cbow, train_loader, val_loader, test_loader, device=device)\n",
    "\n",
    "    # ----------- Embedding analysis -----------\n",
    "    sim, embs = similarity_matrix(vocab, model_cbow)\n",
    "    words = [\n",
    "        'the', 'table', \"man\", 'little', 'big', 'always', 'mind', 'black', 'white', 'child', 'children', \n",
    "        'yes', 'out', \"me\", \"have\", \"be\"\n",
    "    ]\n",
    "    for w in words:\n",
    "        print('-'*59)\n",
    "        find_N_closest(sim, w, vocab)\n",
    "        \n",
    "    return model_cbow, embs, sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device cpu.\n",
      "===========================================================\n",
      "Context size  2  |  Embedding dim  4  |  occ_max  inf  |  weights False\n",
      "use_unk_limit True \n",
      "Black_list: <unk>\n",
      "247766\n",
      "30660\n",
      "67934\n",
      "   Current parameters: \n",
      "lr = 0.001\n",
      "\n",
      "On device cpu.\n",
      "08:54:55.914320  |  Epoch 1  |  Training loss 4.91033\n",
      "08:55:06.726443  |  Epoch 5  |  Training loss 3.96708\n",
      "08:55:17.629282  |  Epoch 10  |  Training loss 3.81031\n",
      "08:55:30.194513  |  Epoch 15  |  Training loss 3.76172\n",
      "08:55:42.116142  |  Epoch 20  |  Training loss 3.73408\n",
      "08:55:55.544088  |  Epoch 25  |  Training loss 3.71714\n",
      "08:56:07.389205  |  Epoch 30  |  Training loss 3.70545\n",
      "Training Accuracy:     0.2336\n",
      "Validation Accuracy:   0.1862\n",
      "Training Accuracy:     0.2336\n",
      "Validation Accuracy:   0.1862\n",
      "Test Accuracy:         0.2188\n",
      "On device cpu.\n",
      "-----------------------------------------------------------\n",
      "the\n",
      "0  |   similitude: 1.000000   |   the \n",
      "1  |   similitude: 0.982028   |   any \n",
      "2  |   similitude: 0.930834   |   my \n",
      "3  |   similitude: 0.922812   |   every \n",
      "4  |   similitude: 0.917259   |   an \n",
      "5  |   similitude: 0.916509   |   your \n",
      "6  |   similitude: 0.904993   |   each \n",
      "7  |   similitude: 0.876142   |   both \n",
      "8  |   similitude: 0.858059   |   make \n",
      "9  |   similitude: 0.856504   |   his \n",
      "-----------------------------------------------------------\n",
      "table\n",
      "0  |   similitude: 1.000000   |   <unk> \n",
      "1  |   similitude: 0.984557   |   related \n",
      "2  |   similitude: 0.946461   |   friendship \n",
      "3  |   similitude: 0.935684   |   saw \n",
      "4  |   similitude: 0.934879   |   day \n",
      "5  |   similitude: 0.934824   |   our \n",
      "6  |   similitude: 0.933741   |   brother \n",
      "7  |   similitude: 0.933527   |   money \n",
      "8  |   similitude: 0.928217   |   killed \n",
      "9  |   similitude: 0.917466   |   bondes \n",
      "-----------------------------------------------------------\n",
      "man\n",
      "0  |   similitude: 1.000000   |   man \n",
      "1  |   similitude: 0.991734   |   should \n",
      "2  |   similitude: 0.973250   |   castle \n",
      "3  |   similitude: 0.970836   |   see \n",
      "4  |   similitude: 0.954045   |   board \n",
      "5  |   similitude: 0.951258   |   army \n",
      "6  |   similitude: 0.936689   |   came \n",
      "7  |   similitude: 0.936674   |   carried \n",
      "8  |   similitude: 0.933318   |   sons \n",
      "9  |   similitude: 0.928566   |   those \n",
      "-----------------------------------------------------------\n",
      "little\n",
      "0  |   similitude: 1.000000   |   little \n",
      "1  |   similitude: 0.984565   |   skald \n",
      "2  |   similitude: 0.951180   |   news \n",
      "3  |   similitude: 0.946018   |   s \n",
      "4  |   similitude: 0.942015   |   house \n",
      "5  |   similitude: 0.914349   |   others \n",
      "6  |   similitude: 0.907615   |   killed \n",
      "7  |   similitude: 0.883607   |   ill \n",
      "8  |   similitude: 0.883305   |   him \n",
      "9  |   similitude: 0.860247   |   their \n",
      "-----------------------------------------------------------\n",
      "big\n",
      "0  |   similitude: 1.000000   |   <unk> \n",
      "1  |   similitude: 0.984557   |   related \n",
      "2  |   similitude: 0.946461   |   friendship \n",
      "3  |   similitude: 0.935684   |   saw \n",
      "4  |   similitude: 0.934879   |   day \n",
      "5  |   similitude: 0.934824   |   our \n",
      "6  |   similitude: 0.933741   |   brother \n",
      "7  |   similitude: 0.933527   |   money \n",
      "8  |   similitude: 0.928217   |   killed \n",
      "9  |   similitude: 0.917466   |   bondes \n",
      "-----------------------------------------------------------\n",
      "always\n",
      "0  |   similitude: 1.000000   |   <unk> \n",
      "1  |   similitude: 0.984557   |   related \n",
      "2  |   similitude: 0.946461   |   friendship \n",
      "3  |   similitude: 0.935684   |   saw \n",
      "4  |   similitude: 0.934879   |   day \n",
      "5  |   similitude: 0.934824   |   our \n",
      "6  |   similitude: 0.933741   |   brother \n",
      "7  |   similitude: 0.933527   |   money \n",
      "8  |   similitude: 0.928217   |   killed \n",
      "9  |   similitude: 0.917466   |   bondes \n",
      "-----------------------------------------------------------\n",
      "mind\n",
      "0  |   similitude: 1.000000   |   <unk> \n",
      "1  |   similitude: 0.984557   |   related \n",
      "2  |   similitude: 0.946461   |   friendship \n",
      "3  |   similitude: 0.935684   |   saw \n",
      "4  |   similitude: 0.934879   |   day \n",
      "5  |   similitude: 0.934824   |   our \n",
      "6  |   similitude: 0.933741   |   brother \n",
      "7  |   similitude: 0.933527   |   money \n",
      "8  |   similitude: 0.928217   |   killed \n",
      "9  |   similitude: 0.917466   |   bondes \n",
      "-----------------------------------------------------------\n",
      "black\n",
      "0  |   similitude: 1.000000   |   <unk> \n",
      "1  |   similitude: 0.984557   |   related \n",
      "2  |   similitude: 0.946461   |   friendship \n",
      "3  |   similitude: 0.935684   |   saw \n",
      "4  |   similitude: 0.934879   |   day \n",
      "5  |   similitude: 0.934824   |   our \n",
      "6  |   similitude: 0.933741   |   brother \n",
      "7  |   similitude: 0.933527   |   money \n",
      "8  |   similitude: 0.928217   |   killed \n",
      "9  |   similitude: 0.917466   |   bondes \n",
      "-----------------------------------------------------------\n",
      "white\n",
      "0  |   similitude: 1.000000   |   <unk> \n",
      "1  |   similitude: 0.984557   |   related \n",
      "2  |   similitude: 0.946461   |   friendship \n",
      "3  |   similitude: 0.935684   |   saw \n",
      "4  |   similitude: 0.934879   |   day \n",
      "5  |   similitude: 0.934824   |   our \n",
      "6  |   similitude: 0.933741   |   brother \n",
      "7  |   similitude: 0.933527   |   money \n",
      "8  |   similitude: 0.928217   |   killed \n",
      "9  |   similitude: 0.917466   |   bondes \n",
      "-----------------------------------------------------------\n",
      "child\n",
      "0  |   similitude: 1.000000   |   <unk> \n",
      "1  |   similitude: 0.984557   |   related \n",
      "2  |   similitude: 0.946461   |   friendship \n",
      "3  |   similitude: 0.935684   |   saw \n",
      "4  |   similitude: 0.934879   |   day \n",
      "5  |   similitude: 0.934824   |   our \n",
      "6  |   similitude: 0.933741   |   brother \n",
      "7  |   similitude: 0.933527   |   money \n",
      "8  |   similitude: 0.928217   |   killed \n",
      "9  |   similitude: 0.917466   |   bondes \n",
      "-----------------------------------------------------------\n",
      "children\n",
      "0  |   similitude: 1.000000   |   <unk> \n",
      "1  |   similitude: 0.984557   |   related \n",
      "2  |   similitude: 0.946461   |   friendship \n",
      "3  |   similitude: 0.935684   |   saw \n",
      "4  |   similitude: 0.934879   |   day \n",
      "5  |   similitude: 0.934824   |   our \n",
      "6  |   similitude: 0.933741   |   brother \n",
      "7  |   similitude: 0.933527   |   money \n",
      "8  |   similitude: 0.928217   |   killed \n",
      "9  |   similitude: 0.917466   |   bondes \n",
      "-----------------------------------------------------------\n",
      "yes\n",
      "0  |   similitude: 1.000000   |   <unk> \n",
      "1  |   similitude: 0.984557   |   related \n",
      "2  |   similitude: 0.946461   |   friendship \n",
      "3  |   similitude: 0.935684   |   saw \n",
      "4  |   similitude: 0.934879   |   day \n",
      "5  |   similitude: 0.934824   |   our \n",
      "6  |   similitude: 0.933741   |   brother \n",
      "7  |   similitude: 0.933527   |   money \n",
      "8  |   similitude: 0.928217   |   killed \n",
      "9  |   similitude: 0.917466   |   bondes \n",
      "-----------------------------------------------------------\n",
      "out\n",
      "0  |   similitude: 1.000000   |   out \n",
      "1  |   similitude: 0.994044   |   while \n",
      "2  |   similitude: 0.977023   |   who \n",
      "3  |   similitude: 0.976299   |   property \n",
      "4  |   similitude: 0.959142   |   force \n",
      "5  |   similitude: 0.945560   |   vessels \n",
      "6  |   similitude: 0.940599   |   round \n",
      "7  |   similitude: 0.926937   |   could \n",
      "8  |   similitude: 0.924179   |   themselves \n",
      "9  |   similitude: 0.908025   |   town \n",
      "-----------------------------------------------------------\n",
      "me\n",
      "0  |   similitude: 1.000000   |   me \n",
      "1  |   similitude: 0.940569   |   both \n",
      "2  |   similitude: 0.884541   |   large \n",
      "3  |   similitude: 0.879545   |   war \n",
      "4  |   similitude: 0.853878   |   make \n",
      "5  |   similitude: 0.822929   |   until \n",
      "6  |   similitude: 0.821162   |   what \n",
      "7  |   similitude: 0.794772   |   ? \n",
      "8  |   similitude: 0.792951   |   thy \n",
      "9  |   similitude: 0.771397   |   or \n",
      "-----------------------------------------------------------\n",
      "have\n",
      "0  |   similitude: 1.000000   |   have \n",
      "1  |   similitude: 0.986444   |   be \n",
      "2  |   similitude: 0.979319   |   would \n",
      "3  |   similitude: 0.961215   |   think \n",
      "4  |   similitude: 0.958466   |   will \n",
      "5  |   similitude: 0.956912   |   did \n",
      "6  |   similitude: 0.956455   |   sailed \n",
      "7  |   similitude: 0.956107   |   proceeded \n",
      "8  |   similitude: 0.929547   |   said \n",
      "9  |   similitude: 0.835863   |   might \n",
      "-----------------------------------------------------------\n",
      "be\n",
      "0  |   similitude: 1.000000   |   be \n",
      "1  |   similitude: 0.986444   |   have \n",
      "2  |   similitude: 0.984727   |   will \n",
      "3  |   similitude: 0.978218   |   sailed \n",
      "4  |   similitude: 0.959148   |   think \n",
      "5  |   similitude: 0.957946   |   said \n",
      "6  |   similitude: 0.953858   |   would \n",
      "7  |   similitude: 0.947250   |   proceeded \n",
      "8  |   similitude: 0.913960   |   did \n",
      "9  |   similitude: 0.905058   |   carried \n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "batch_size = 2048\n",
    "\n",
    "# These hyperparameters were decided after analysing the output of bigger experiments\n",
    "# ------------------------------------------------------------------------\n",
    "context_size = 2\n",
    "embedding_dim = 4\n",
    "occ_max = np.inf\n",
    "use_weight = False\n",
    "use_unk_limit=True\n",
    "black_list = [\"<unk>\"]\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "model_cbow, embs, sim = pipeline(\n",
    "    context_size, embedding_dim, \n",
    "    occ_max=occ_max, use_weight=use_weight, use_unk_limit=use_unk_limit,\n",
    "    black_list=black_list, generated_path=generated_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_np = embs.cpu().numpy()\n",
    "embs_df = pd.DataFrame(embs_np)\n",
    "embs_df.to_csv(generated_path + 'embeddings.tsv', sep=\"\\t\", header=False, index=False)\n",
    "words_np = np.array(vocab.lookup_tokens(range(vocab_size)))\n",
    "words_df = pd.DataFrame(words_np)\n",
    "words_df.to_csv(generated_path + 'vocabulary.tsv', sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = model_cbow.embeddings.cpu()\n",
    "torch.save(model_cbow.embeddings, \"embedding.pt\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "71c2cb666ff353b4e7b5c350d66179fa0af5c84ce239ad9fa105d94543f3ad59"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
