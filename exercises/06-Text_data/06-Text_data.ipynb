{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Text data with pytorch\n",
    "\n",
    "The objective of this notebook is get an introduction to text data and using pretrained word embeddings while getting familiar with saving / loading pytorch objects.\n",
    "\n",
    "## Contents:\n",
    "\n",
    "1. Reading `txt` files, tokenizing texts and building a vocabulary\n",
    "2. Creating a \"context/target\" dataset\n",
    "3. Using a pretrained embedding in a model\n",
    "4. Predict next word's class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f88b1ff5210>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import os\n",
    "import re\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from utils import train, set_device, compute_accuracy\n",
    "\n",
    "seed = 265\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading `txt` files, tokenizing texts and building a vocabulary\n",
    "\n",
    "There is nothing to do in this cell, apart from running it. **You might want to read it carefully though, as it is an important step of the pipeline in pytorch when dealing with text data. It is quite unlikely that you need to drastically change this part of the code in your future tasks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer will split a long text into a list of english words\n",
    "TOKENIZER_EN = get_tokenizer('basic_english')\n",
    "# Where we will store / load all our models, datasets, vocabulary, etc.\n",
    "PATH_GENERATED = './generated/'\n",
    "# Minimum number of occurence of a word in the text to add it to the vocabulary\n",
    "MIN_FREQ = 100\n",
    "\n",
    "def read_files(datapath='./data_train/'):\n",
    "    \"\"\"\n",
    "    Return a list of strings, one for each line in each .txt files in 'datapath'\n",
    "    \"\"\"\n",
    "    # Find all txt files in directory \n",
    "    files = os.listdir(datapath)\n",
    "    files = [datapath + f for f in files if f.endswith(\".txt\")]\n",
    "    \n",
    "    # Stores each line of each book in a list\n",
    "    lines = []\n",
    "    for f_name in files:\n",
    "        with open(f_name) as f:\n",
    "            lines += f.readlines()\n",
    "    return lines\n",
    "\n",
    "def tokenize(lines, tokenizer=TOKENIZER_EN):\n",
    "    \"\"\"\n",
    "    Tokenize the list of lines\n",
    "    \"\"\"\n",
    "    list_text = []\n",
    "    for line in lines:\n",
    "        list_text += tokenizer(line)\n",
    "    return list_text\n",
    "\n",
    "def yield_tokens(lines, tokenizer=TOKENIZER_EN):\n",
    "    \"\"\"\n",
    "    Yield tokens, ignoring names and digits to build vocabulary\n",
    "    \"\"\"\n",
    "    # Match any word containing digit\n",
    "    no_digits = '\\w*[0-9]+\\w*'\n",
    "    # Match word containing a uppercase \n",
    "    no_names = '\\w*[A-Z]+\\w*'\n",
    "    # Match any sequence containing more than one space\n",
    "    no_spaces = '\\s+'\n",
    "    \n",
    "    for line in lines:\n",
    "        line = re.sub(no_digits, ' ', line)\n",
    "        line = re.sub(no_names, ' ', line)\n",
    "        line = re.sub(no_spaces, ' ', line)\n",
    "        yield tokenizer(line)\n",
    "\n",
    "def count_freqs(words, vocab):\n",
    "    \"\"\"\n",
    "    Count occurrences of each word in vocabulary in the data\n",
    "    \n",
    "    Useful to get some insight on the data and to compute loss weights\n",
    "    \"\"\"\n",
    "    freqs = torch.zeros(len(vocab), dtype=torch.int)\n",
    "    for w in words:\n",
    "        freqs[vocab[w]] += 1\n",
    "    return freqs\n",
    "\n",
    "def create_vocabulary(lines, min_freq=MIN_FREQ):\n",
    "    \"\"\"\n",
    "    Create a vocabulary (list of known tokens) from a list of strings\n",
    "    \"\"\"\n",
    "    # vocab contains the vocabulary found in the data, associating an index to each word\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(lines), min_freq=min_freq, specials=[\"<unk>\"])\n",
    "    # Since we removed all words with an uppercase when building the vocabulary, we skipped the word \"I\"\n",
    "    vocab.append_token(\"i\")\n",
    "    # Value of default index. This index will be returned when OOV (Out Of Vocabulary) token is queried.\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the functions are defined we can use them.\n",
    "\n",
    "In real life, the size of the datasets can be huge and just building the vocabulary can take some time. To avoid having to recompute it each time you need your vocabulary, it is wiser to save it. You can even save the tokenized version of the texts.\n",
    "\n",
    "We also encourage you to take some time analysing your vocabulary, the frequences of each word, the size of the vocabulary compared to the number of words seen in the dataset, and much much more than what we can see in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the training dataset:      347870\n",
      "Total number of words in the validation dataset:    49526\n",
      "Total number of words in the test dataset:          124152\n",
      "Number of distinct words in the training dataset:   11161\n",
      "Number of distinct words kept (vocabulary size):    324\n",
      "occurences:\n",
      " [(88495, '<unk>'), (23451, ','), (20640, 'the'), (14589, 'and'), (13686, '.'), (9482, 'to'), (7226, 'of'), (5560, 'in'), (4799, 'was'), (5196, 'a'), (5274, 'he'), (4490, 'his'), (3729, 'that'), (6413, 'king'), (3262, 'with'), (2941, 'had'), (3110, 's'), (2661, 'him'), (2705, 'it'), (2619, 'they'), (1994, 'for'), (1969, 'all'), (1937, 'as'), (1763, 'men'), (1786, 'on'), (1620, 'were'), (1925, 'but'), (1569, 'who'), (1583, 'from'), (1433, 'be'), (1501, 'at'), (1323, 'not'), (1519, 'this'), (1313, 'them'), (1334, 'people'), (1318, 'their'), (1253, 'which'), (1225, 'came'), (1483, 'there'), (1204, 'is'), (1412, 'so'), (1116, 'went'), (1087, 'great'), (1583, 'when'), (931, 'out'), (918, 'said'), (965, 'by'), (984, 'many'), (855, 'have'), (856, 'man'), (831, 'up'), (738, 'other'), (736, 'would'), (730, 'country'), (719, 'will'), (1457, 'then'), (658, 'into'), (677, 'before'), (640, '('), (640, ')'), (637, 'made'), (632, 'or'), (1310, 'earl'), (668, 'if'), (594, 'should'), (581, 'ships'), (576, 'also'), (574, 'took'), (581, 'upon'), (563, 'called'), (631, 'one'), (570, 'over'), (613, 'where'), (550, 'land'), (598, 'some'), (631, 'thou'), (521, 'been'), (518, 'could'), (517, 'son'), (928, 'now'), (553, 'my'), (596, 'we'), (549, 'no'), (482, 'himself'), (476, 'about'), (503, 'what'), (451, 'ship'), (453, 'come'), (499, 'battle'), (442, 'time'), (554, 'after'), (445, 'an'), (447, 'her'), (449, 'against'), (444, 'are'), (439, 'both'), (411, 'told'), (407, 'gave'), (402, 'down'), (399, 'very'), (398, 'heard'), (385, 'me'), (383, 'bondes'), (393, 'you'), (377, 'much'), (367, 'says'), (365, 'well'), (378, 'go'), (384, 'long'), (349, 'way'), (359, 'than'), (350, 'each'), (392, 'our'), (340, 'army'), (324, 'day'), (322, 'thee'), (349, 'good'), (321, 'did'), (315, 'got'), (357, 'thy'), (313, 'sent'), (349, 'these'), (307, 'most'), (305, 'sailed'), (291, '?'), (294, 'set'), (285, 'us'), (301, 'has'), (292, 'take'), (300, 'two'), (275, 'winter'), (274, 'together'), (288, 'kings'), (283, 'first'), (348, 'more'), (285, 'between'), (270, 'lay'), (270, 'house'), (261, 'saw'), (340, 'she'), (274, 'under'), (257, 'same'), (263, 'fell'), (255, 'away'), (275, 'do'), (301, 'north'), (274, 'sons'), (313, 'here'), (273, 'such'), (253, 'brought'), (254, 'sea'), (249, 'shall'), (250, 'thought'), (249, 'whole'), (247, 'father'), (243, 'any'), (254, 'must'), (239, 'ordered'), (241, 'town'), (238, 'sword'), (237, 'brother'), (244, 'give'), (235, 'themselves'), (234, 'daughter'), (235, 'laid'), (235, 'stood'), (251, 'through'), (231, 'night'), (227, 'summer'), (258, 'soon'), (223, 'again'), (221, 'ready'), (228, 'among'), (217, 'asked'), (217, 'part'), (264, 'let'), (218, 'home'), (215, 'kingdom'), (211, 'own'), (212, 'side'), (211, 'hand'), (208, '!'), (207, 'replies'), (205, 'back'), (205, 'place'), (215, 'may'), (209, 'peace'), (204, 'off'), (200, 'killed'), (226, 'thus'), (194, 'proceeded'), (194, 'received'), (194, 'years'), (202, 'every'), (192, 'force'), (196, 'little'), (196, 'never'), (191, 'others'), (187, 'along'), (186, 'know'), (202, 'old'), (187, 'can'), (185, 'held'), (189, 'far'), (210, 'south'), (183, 'fleet'), (180, 'friends'), (183, 'make'), (179, 'remained'), (185, 'those'), (230, 'death'), (177, 'get'), (175, 'replied'), (177, 'see'), (174, 'power'), (175, 'left'), (173, 'life'), (173, 'head'), (177, 'taken'), (171, 'your'), (172, 'message'), (170, 'put'), (173, 'without'), (169, 'fled'), (161, 'found'), (162, 'met'), (163, 'until'), (160, 'only'), (158, 'returned'), (157, 'large'), (187, 'how'), (183, 'although'), (153, 'name'), (155, 'whom'), (153, 'fight'), (150, 'hands'), (194, 'while'), (148, 'er'), (148, 'spring'), (154, 'immediately'), (145, 'last'), (146, 'sat'), (147, 'say'), (143, 'chiefs'), (171, 'afterwards'), (143, 'brothers'), (153, 'therefore'), (146, 'high'), (167, 'things'), (140, 'turned'), (137, 'best'), (140, 'river'), (157, 'towards'), (139, 'hold'), (138, 'might'), (135, 'spoke'), (138, 'news'), (164, 'ye'), (152, 'brave'), (136, 'still'), (133, 'meet'), (139, 'church'), (130, 'began'), (179, 'east'), (131, 'think'), (155, 'gold'), (126, 'mother'), (130, 'feast'), (125, 'given'), (124, 'happened'), (134, 'skald'), (123, 'became'), (124, 'board'), (129, 'right'), (134, 'like'), (134, 'tell'), (121, 'friendship'), (122, 'followed'), (120, 'words'), (122, 'often'), (117, 'done'), (122, 'shield'), (115, 'related'), (118, 'round'), (116, 'victory'), (121, 'three'), (114, 'vessel'), (114, 'vessels'), (114, 'body'), (116, 'district'), (115, 'fire'), (117, 'around'), (144, 'fall'), (112, 'help'), (118, 'few'), (121, 'speech'), (112, 'lendermen'), (110, 'near'), (109, 'account'), (106, 'according'), (106, 'am'), (108, 'knew'), (106, 'married'), (106, 'matter'), (107, 'money'), (111, 'nothing'), (106, 'chief'), (105, 'friend'), (113, 'meeting'), (105, 'ran'), (104, 'word'), (108, 'castle'), (106, 'ill'), (130, 'journey'), (103, 'shore'), (114, 'war'), (103, 'being'), (103, 'carried'), (110, 'art'), (126, 'expedition'), (102, 'bring'), (105, 'just'), (101, 'property'), (1441, 'i')]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------- Tokenize texts -------------------------------\n",
    "# Load tokenized versions of texts if you have already generated it\n",
    "# Otherwise, create it and save it\n",
    "if os.path.isfile(PATH_GENERATED + \"words_train.pt\"):\n",
    "    words_train = torch.load(PATH_GENERATED + \"words_train.pt\")\n",
    "    words_val = torch.load(PATH_GENERATED + \"words_val.pt\")\n",
    "    words_test = torch.load(PATH_GENERATED + \"words_test.pt\")\n",
    "else:\n",
    "    # Get lists of strings, one for each line in each .txt files in 'datapath' \n",
    "    lines_books_train = read_files('./data_train/')\n",
    "    lines_books_val = read_files('./data_val/')\n",
    "    lines_books_test = read_files('./data_test/')\n",
    "\n",
    "    # List of words contained in the dataset\n",
    "    words_train = tokenize(lines_books_train)\n",
    "    words_val = tokenize(lines_books_val)\n",
    "    words_test = tokenize(lines_books_test)\n",
    "    \n",
    "    torch.save(words_train , PATH_GENERATED + \"words_train.pt\")\n",
    "    torch.save(words_val , PATH_GENERATED + \"words_val.pt\")\n",
    "    torch.save(words_test , PATH_GENERATED + \"words_test.pt\")\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------- Create vocabulary ----------------------------\n",
    "VOCAB_FNAME = \"vocabulary.pt\"\n",
    "# Load vocabulary if you have already generated it\n",
    "# Otherwise, create it and save it\n",
    "if os.path.isfile(PATH_GENERATED + VOCAB_FNAME):\n",
    "    vocab = torch.load(PATH_GENERATED + VOCAB_FNAME)\n",
    "else:\n",
    "    # Create vocabulary based on the words in the training dataset\n",
    "    vocab = create_vocabulary(lines_books_train, min_freq=MIN_FREQ)\n",
    "    torch.save(vocab, PATH_GENERATED + VOCAB_FNAME)\n",
    "    \n",
    "\n",
    "\n",
    "# ------------------------ Quick analysis ------------------------------\n",
    "VOCAB_SIZE = len(vocab)\n",
    "print(\"Total number of words in the training dataset:     \", len(words_train))\n",
    "print(\"Total number of words in the validation dataset:   \", len(words_val))\n",
    "print(\"Total number of words in the test dataset:         \", len(words_test))\n",
    "print(\"Number of distinct words in the training dataset:  \", len(set(words_train)))\n",
    "print(\"Number of distinct words kept (vocabulary size):   \", VOCAB_SIZE)\n",
    "\n",
    "freqs = count_freqs(words_train, vocab)\n",
    "print(\"occurences:\\n\", [(f.item(), w) for (f, w)  in zip(freqs, vocab.lookup_tokens(range(VOCAB_SIZE)))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating a \"context/target\" dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a context / pair dataset is a key part of any machine learning task involving text, and it has to be adapted to each task. \n",
    "\n",
    "In this notebook, we define a ``(contexts, targets)`` toydataset such that for each ``(c, t)`` context/target pair in the dataset, \n",
    "- ``t = 0`` if the next word after the sequence ``c`` is the ``<unk>`` token\n",
    "- ``t = 1`` if the next word after the sequence ``c`` is a punction symbol ``[',', '.', '(', ')', '?', '!']``\n",
    "- ``t = 2`` if the next word after the sequence ``c`` is an actual word and present in our vocabulary\n",
    "\n",
    "For more realistic tasks, the function that creates the dataset can be much more complex. A non exaustive list of adaptation:\n",
    "\n",
    "- having the context *around* the target and not just before\n",
    "- ignore some context-target pairs when then target is not interesting\n",
    "- ignore some context-target pairs when there are too many `<unk>` token in the context\n",
    "- ignore some context-target pairs when then target is already too present in the dataset\n",
    "- etc.\n",
    "\n",
    "**Make sure you understand this function as you will have to implement a variant of it whenever you have task with text in pytorch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ Define targets ------------------------------\n",
    "def compute_label(w):\n",
    "    \"\"\"\n",
    "    helper function to define MAP_TARGET\n",
    "    \n",
    "    - 0 = 'unknown word'\n",
    "    - 1 = 'punctuation' (i.e. the '<unk>' token)\n",
    "    - 2 = 'is an actual word'\n",
    "    \"\"\"\n",
    "    if w in ['<unk>']:\n",
    "        return 0\n",
    "    elif w in [',', '.', '(', ')', '?', '!']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# true labels for this task:\n",
    "MAP_TARGET = {\n",
    "    vocab[w]:compute_label(w) for w in vocab.lookup_tokens(range(VOCAB_SIZE))\n",
    "}\n",
    "\n",
    "# context size for this task \n",
    "CONTEXT_SIZE = 3\n",
    "\n",
    "\n",
    "# ---------------- Define context / target pairs -----------------------\n",
    "def create_dataset(\n",
    "    text, vocab, \n",
    "    context_size=CONTEXT_SIZE, map_target=MAP_TARGET\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a pytorch dataset of context / target pairs from a text\n",
    "    \"\"\"\n",
    "    \n",
    "    n_text = len(text)\n",
    "    n_vocab = len(vocab)\n",
    "    \n",
    "    # Change labels if only a few target are kept, otherwise, each word is\n",
    "    # associated with its index in the vocabulary\n",
    "    if map_target is None:\n",
    "        map_target = {i:i for i in range(n_vocab)}\n",
    "    \n",
    "    # Transform the text as a list of integers.\n",
    "    txt = [vocab[w] for w in text]\n",
    "\n",
    "    # Start constructing the context / target pairs...\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(n_text - context_size):\n",
    "        \n",
    "        # Word used to define target\n",
    "        t = txt[i + context_size]\n",
    "        \n",
    "        # Context before the target\n",
    "        c = txt[i:i + context_size]\n",
    "        \n",
    "        targets.append(map_target[t])\n",
    "        contexts.append(torch.tensor(c))\n",
    "            \n",
    "    # contexts of shape (N_dataset, context_size)\n",
    "    # targets of shape  (N_dataset)\n",
    "    contexts = torch.stack(contexts)\n",
    "    targets = torch.tensor(targets)\n",
    "    # Create a pytorch dataset out of these context / target pairs\n",
    "    return TensorDataset(contexts, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, now that the functions are defined we can use them.\n",
    "\n",
    "And here again, it is wiser to save your datasets as they can also be quite costly to create. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(words, vocab, fname):\n",
    "    \"\"\"\n",
    "    Load dataset if already generated, otherwise, create it and save it\n",
    "    \"\"\"\n",
    "    # If already generated\n",
    "    if os.path.isfile(PATH_GENERATED + fname):\n",
    "        dataset = torch.load(PATH_GENERATED + fname)\n",
    "    else:\n",
    "        # Create context / target dataset based on the list of strings\n",
    "        dataset = create_dataset(words, vocab)\n",
    "        torch.save(dataset, PATH_GENERATED + fname)\n",
    "    return dataset\n",
    "\n",
    "data_train = load_dataset(words_train, vocab, \"data_train.pt\")\n",
    "data_val = load_dataset(words_val, vocab, \"data_val.pt\")\n",
    "data_test = load_dataset(words_test, vocab, \"data_test.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using a pretrained embedding in a model\n",
    "\n",
    "You don't have to understand what a word embedding is for now, only to understand how to use a pretrained word embedding inside a neural network, and how to \"freeze\" it so that it doesn't get updated while the rest of the network is being trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding=None, context_size=CONTEXT_SIZE):\n",
    "        super().__init__()\n",
    "        \n",
    "        (vocab_size, embedding_dim) = embedding.weight.shape\n",
    "        # Instantiate an embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Load the pretrained weights\n",
    "        self.embedding.load_state_dict(embedding.state_dict())\n",
    "        # Freeze the layer\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "        # Regular MLP\n",
    "        self.fc1 = nn.Linear(embedding_dim*context_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape (N, context_size) but contains integers which can\n",
    "        # be seen as equivalent to (N, context_size, vocab_size) since one hot\n",
    "        # encoding is used under the hood\n",
    "        out = self.embedding(x)\n",
    "        # out is now of shape (N, context_size, embedding_dim)\n",
    "        \n",
    "        out = F.relu(self.fc1(torch.flatten(out, 1)))\n",
    "        # out is now of shape (N, context_size*embedding_dim)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict next word's class\n",
    "\n",
    "We now train our dummy model for our dummy task and we don't forget to save it! For more information on saving pytorch objects, see the [documentation](https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html) \n",
    "\n",
    "**Note that this is a dummy task, with many arbitrary choices such as the performance measure and without any proper model selection / evaluation nor analysis of the results. In a normal situation, much more would be expected there.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device cpu.\n",
      "On device cpu.\n",
      "09:02:21.082854  |  Epoch 1  |  Training loss 0.81438\n",
      "09:02:32.414066  |  Epoch 5  |  Training loss 0.77512\n",
      "09:02:46.950664  |  Epoch 10  |  Training loss 0.76392\n",
      "09:03:03.048266  |  Epoch 15  |  Training loss 0.75964\n",
      "09:03:19.843292  |  Epoch 20  |  Training loss 0.75685\n",
      "09:03:36.346039  |  Epoch 25  |  Training loss 0.75430\n",
      "09:03:52.637025  |  Epoch 30  |  Training loss 0.75276\n",
      "On device cpu.\n",
      "On device cpu.\n",
      "Training Accuracy:     0.6534\n",
      "Validation Accuracy:   0.6160\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "device = set_device()\n",
    "\n",
    "# Load the pretrained embedding \n",
    "if os.path.isfile(\"embedding.pt\"):\n",
    "    embedding = torch.load(\"embedding.pt\").to(device=device)\n",
    "else:\n",
    "    raise ValueError(\"Embedding not found at the given location\")\n",
    "\n",
    "MODEL_FNAME = \"model.pt\"\n",
    "\n",
    "batch_size=512\n",
    "train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(data_val, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(data_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = MyMLP(embedding)\n",
    "\n",
    "if os.path.isfile(PATH_GENERATED + MODEL_FNAME):\n",
    "    # Load the trained model\n",
    "    model = torch.load(PATH_GENERATED + MODEL_FNAME)\n",
    "    model.to(device)\n",
    "else:\n",
    "    # Or train the model...\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    n_epochs=30\n",
    "\n",
    "    train(n_epochs, optimizer, model, loss_fn, train_loader)\n",
    "    # ... and save it\n",
    "    torch.save(model.to(device=\"cpu\"), PATH_GENERATED + MODEL_FNAME)\n",
    "\n",
    "acc_train = compute_accuracy(model, train_loader)\n",
    "acc_val = compute_accuracy(model, val_loader)\n",
    "print(\"Training Accuracy:     %.4f\" %acc_train)\n",
    "print(\"Validation Accuracy:   %.4f\" %acc_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd17699b0f8c8ba78222bc090e4903241cbfccfe2c3d9b347e7b5dfb6ba74475"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
