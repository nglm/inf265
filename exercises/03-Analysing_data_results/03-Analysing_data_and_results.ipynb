{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing the data and the results\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Analyse data\n",
    "   1. Load dataset\n",
    "   2. Inspect type of dataset\n",
    "   3. Analyse dataset\n",
    "   4. Preprocess dataset\n",
    "2. Machine learning pipeline\n",
    "   1. Define different model architectures\n",
    "   2. Utils: training loop and computing performance\n",
    "   3. Define different model instances and train them all\n",
    "   4. Analysis of the evolution of the training and validation losses for each model\n",
    "   5. Analysis of the confusion matrix of each model on the validation dataset\n",
    "   6. Model Selection\n",
    "   7. Model evaluation\n",
    "3. Analyse Results\n",
    "   1. Plot confusion matrix\n",
    "   2. Plot examples of misclassified inputs\n",
    "   3. Final comments\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "1. Practice even more what you have learned with the tutorials and the previous assigments.\n",
    "2. Encourage critical thinking when analysing data and results\n",
    "\n",
    "Of course, in reality, more sanity checks could be implemented and the analysis could go much deeper. Don't hesitate to also check the `project_checklist` document. What you should always keep in mind though is that **you should never just trust the numbers! Nor blindly use tools that you don't understand**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "SEED = 265\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Analyse data\n",
    "\n",
    "### 1.1 Load dataset\n",
    "\n",
    "The dataset is an image classification dataset.\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "1. Download the datasets from the link given in the MittUiB assignment and load it using torch.load().\n",
    "2. Split the `train_val` dataset into a training and a validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_val = torch.load(\"data_train_val.pt\")\n",
    "data_test = torch.load(\"data_test.pt\")\n",
    "\n",
    "# TODO..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Inspect type of dataset\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "Inspect the dataset. Do everything that seems relevant to you to have a better idea of the type of data you will play with. You can for example try to answer the following questions first:\n",
    "\n",
    "- What is the size of each dataset?\n",
    "- What is the shape of the input?\n",
    "- What is the type of the input tensors?\n",
    "- What is the shape of the targets?\n",
    "- What is the type of the target tensors?\n",
    "- How many classes are there?\n",
    "- What do instances of each class look like?\n",
    "\n",
    "Remember that you can acces any element of a dataset without using any dataloader using `data[i]`. \n",
    "\n",
    "See `02 - Machine learning pipeline and MNIST` for more information about plotting instances for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Analyse dataset\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "Inspect the dataset. Do everything that seems relevant to you to have a better idea of the type of data you will play with. You can for example try to answer the following questions first:\n",
    "\n",
    "- What is the range of values of the input? Its mean? And standard deviation? \n",
    "- How many instances are there in each class?\n",
    "\n",
    "See `02 - Machine learning pipeline and MNIST` for more information about counting instances of each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Preprocess dataset\n",
    "\n",
    "See `02 - Machine learning pipeline and MNIST` for more information about the preprocessing in general\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "1. Instantiate a pytorch transforms to preprocess the dataset according to the analysis you just made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Machine learning pipeline\n",
    "\n",
    "See practical exercise 2 `Machine learning pipeline and MNIST` for more information about the machine pipeline in general\n",
    "\n",
    "### 2.1 Define different model architectures\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "1. Define 3 different model architectures that are suitable to classify the images of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Utils: training loop and computing performance\n",
    "\n",
    "See `02 - Machine learning pipeline and MNIST` for more information about the training loop in general\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "1. Write a function ``train`` that \n",
    "   1. Trains the model for ``n`` epochs (complete passes through the training dataset)\n",
    "   2. Computes and stores the training loss and the validation loss for each epoch\n",
    "   3. Returns the list of training and validation losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define different model instances and train them all\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "1. For each model architecture that you defined:\n",
    "   1. create multiple instances and train them with different hyperparameters.\n",
    "   2. store the training and validation loss for each model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Analysis of the evolution of the training and validation losses for each model\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "1. For each of your trained models, plot the training loss and the validation loss. See `02 - Machine learning pipeline and MNIST` for more information about how to plot the training loss and the validation loss. \n",
    "2. Analyse your plots. For example, you can try to answer the following questions:\n",
    "   1. Are you results so unexpected that you think you should look for a bug in your code? For example, if the validation loss is much better that the training loss, or the losses are constants or if the training loss is increasing, etc. \n",
    "   2. Are results so disappointing that you think you should go back to `1. Analyse data` because of a misunderstanding concerning the content of the dataset or an inappropriate preprocessing?\n",
    "   3. Are some models overfitting/underfitting? \n",
    "   4. Do some architectures perform systematically better than others? Do you think you should go back to `2.1. Define different model architectures` and define new models?\n",
    "   5. Do some sets of hyperparameters systematically yield a better/worse performance than other? Do you think you should go back to `3. Define different model instances and train them all`?\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Analysis of the confusion matrix of each model on the validation dataset\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "1. For each trained model, plot the confusion matrix of the validation dataset. To do so:\n",
    "   1. Compute the confusion matrix. You can use `sklearn.metrics.confusion_matrix` (remember to use `.cpu()` on Pytorch tensors right before using a library outside Pytorch)\n",
    "   2. Plot the obtained confusion matrix. You can use the `plot_confusion_matrix` function.\n",
    "2. Analyse the confusion matrix. For example, you can try to answer the following questions\n",
    "   1. Are the different classes equally well (mis-)classified? \n",
    "   2. For each class, does the model tend to misclassify the input with some other specific classes? Do you know why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(matrix, ax=None):\n",
    "    \"\"\"\n",
    "    Plot the given confusion matrix\n",
    "    \"\"\"\n",
    "    ax = sns.heatmap(\n",
    "        data=matrix.round(2),\n",
    "        cmap=sns.color_palette(\"RdBu_r\", 1000), ax=ax\n",
    "    )\n",
    "    return ax\n",
    "\n",
    "#TODO..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Model Selection\n",
    "\n",
    "See `02 - Machine learning pipeline and MNIST` for more information about model selection.\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "1. Choose a performance measure. Justify your choice.\n",
    "2. Select the best model based on the validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Model evaluation\n",
    "\n",
    "See `02 - Machine learning pipeline and MNIST` for more information about model evaluation.\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "1. Evaluate the performance of the selected model on the test dataset.\n",
    "2. Analyse your result. For example, you can try to answer the following questions :\n",
    "\n",
    "   1. Are your results surprising? In other words, relative to your general knowledge in machine learning, your understanding of the data and the problem, and relative to the conÔ¨Ådence you put in your design choices and implementation, did you expect such performance?\n",
    "   2. Are your results satisfying? In other words, is your model better than random outputs? Does your model outperform any baseline model? Do you think you could use your model in real life? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyse Results\n",
    "\n",
    "We would like to analyse the performance of our model a bit further, in order to be able to communicate on its qualities and limits.\n",
    "\n",
    "### 3.1 Plot confusion matrix\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "1. Plot the confusion matrix of the best model on the test dataset.\n",
    "2. Analyse the confusion matrix. For example, you can try to answer the following questions\n",
    "   1. Are the different classes equally well (mis-)classified? \n",
    "   2. For each class, does the model tend to misclassify the input with some other specific classes? Do you know why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Plot examples of misclassified inputs\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "1. For each class, plot examples of misclassified inputs together with their predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Final comments\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "1. In which cases does your selected model seem to struggle? In which cases does your model seem to yield good results? How would you explain that?\n",
    "1. If you were given unlimited time, what would you try to improve your performance?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7334498cbea74be2f983349dd0c062cc89e10cb2d32c736100e0abee6e40bc2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
