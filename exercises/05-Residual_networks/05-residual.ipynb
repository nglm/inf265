{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual neural networks\n",
    "\n",
    "## Objectives:\n",
    "- implementing a residual layer \n",
    "- using batchnormalization layers \n",
    "- observing the effects on the gradients at initialization of residual layers, batchnormalization layers and the depth of neural networks.\n",
    "- observing the effects on overfitting of residual layers, batchnormalization layers and the depth of neural networks. \n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Implementing a Residual Network  \n",
    "2. Observing vanishing, shattering and exploding gradients\n",
    "3. Effects of residual layers, batchnormalization and depth on overfitting   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implementing a Residual Network\n",
    "\n",
    "#### TODO\n",
    "\n",
    "1. Define a class ``MyResidualLayer`` implementing a residual layer, taking as parameter ``n_hid`` and ``use_skip`` such that:\n",
    "- ``out = x + relu(fc(x))`` if ``use_skip = True``\n",
    "- ``out = relu(fc(x))`` if ``use_skip = False``\n",
    "\n",
    "where ``fc`` is a fully connected layer with as many inputs as outputs ``n_hid``.\n",
    "\n",
    "2. Define a class ``MyNet``, implementing a neural network with ``L`` (blocks of) trainable layers such that:\n",
    "- The first layer is a fully connected layer with ``n_in=1`` inputs and ``n_hid`` outputs\n",
    "- The next ``L-2`` layers are residual layers (defined using ``MyResidualLayer``) with ``n_hid`` hidden units or fully connected depending on the value of a boolean parameter ``use_skip``. You can set ``n_hid`` to ``128`` for example.\n",
    "- The last layer is a fully connected layer with ``n_hid`` inputs and ``n_out=1`` outputs. \n",
    "- Every ``batchnorm_frequency`` layers, a batchnorm layer is inserted. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Observing vanishing, shattering and exploding gradients\n",
    "\n",
    "### Computing gradients with respect to the input \n",
    "\n",
    "\n",
    "Rather than computing the gradients with respect to the network's parameters as you are now used to, we will instead compute the gradients with respect to the input data. This is meaningful since by the chain rule, the derivatives with respect to inputs are connected to the derivatives with respect to parameters. Since we are interested in observing how network depth impacts the gradients, we will use a simple grid of uniformly spaced data points ranging from -3 to 3 as input for our network.\n",
    "\n",
    "#### TODO\n",
    "\n",
    "1. Write a function that computes the mean gradient values with respect to an input ``X`` consisting of 256 points uniformly spaced between ``[-3,3]`` for ``n_iter = 30`` random model initializations of your ``MyNet`` model. To do so:\n",
    "1. Seeds pytorch, \n",
    "1. Repeat:\n",
    "    1. Instanciante a ``MyNet`` model. (You might need to add parameters to your function so that you can instanciate your model with different values of ``batchnorm_frequency``, ``use_skip``, etc.)\n",
    "    1. Emulate the beginning of a training phase by calling ``train_one_epoch``\n",
    "    1. Disable autograd for all the network's parameters. Enable autograd for an uniformly spaced input ``X``.\n",
    "    1. Perform a forward pass with ``X`` as input.\n",
    "    1. Run a backward pass on the output. (You might need to sum all the components of the output before calling ``backward()``)\n",
    "    1. Store the gradient with respect to the input ``X``. You might need to use .clone() and/or .detach() methods.\n",
    "1. For each element of ``X`` compute the mean gradient value obtained with the different model initialization.\n",
    "1. Return the mean gradient values.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model):\n",
    "    \"\"\"\n",
    "    Performs one small training iteration to emulate an early training situation\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    X = torch.linspace(-3, 3, 10).unsqueeze(1)  \n",
    "\n",
    "    outputs = model(X)\n",
    "    loss = loss_fn(outputs, torch.cos(X))\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "\n",
    "#### TODO\n",
    "\n",
    "1. Plot mean gradient values with respect to the input ``X`` for different values of depth, ``batchnorm_frequency`` and with or without residual skip (``use_skip`` set to ``True`` or ``False``). You can keep the values suggested in the cell below.\n",
    "1. Comment your results. \n",
    "1. Do you observe shattering gradients? If so, in which conditions?  \n",
    "1. Do you observe vanishing gradients? If so, in which conditions? \n",
    "1. Do you observe exploding gradients? If so, in which conditions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 256\n",
    "X = torch.linspace(-3, 3, N).unsqueeze(1) \n",
    "list_depth = [2, 5, 10, 25, 50, 75]\n",
    "list_frequency = [None, 10, 5, 3, 1]\n",
    "list_use_skip = [False, True]\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Effects of residual layers, batchnormalization and depth on overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "\n",
    "1. Load and preprocess the CIFAR-10 dataset. Split it into 3 datasets: training, validation and test. Take a subset of these datasets by keeping only 2 labels: bird and plane.\n",
    "1. Modify your residual network so that its input and output layers match the dataset (Now ``n_in=32*32*3``, ``n_out=2``. You can also set ``n_hid`` to ``64`` for example, to reduce computations) \n",
    "1. Plot the training loss and the validation loss for different values of depth, ``batchnorm_frequency`` and with or without residual skip. (you can keep the values suggested in the cell below)\n",
    "1. Comment your results.\n",
    "1. Select and evaluate the best model among the different values of depth, ``batchnorm_frequency`` and ``use_skip`` used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 512\n",
    "\n",
    "list_depth = [3, 5, 10, 20]\n",
    "list_frequency = [None, 9, 7, 3, 1]\n",
    "list_use_skip = [False, True]\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "71c2cb666ff353b4e7b5c350d66179fa0af5c84ce239ad9fa105d94543f3ad59"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('nglm-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
