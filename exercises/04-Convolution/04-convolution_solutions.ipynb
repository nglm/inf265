{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a convolutional layer\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The objective of this exercise is get an in-depth understanding of the convolutional layer as it is a crucial layer in Deep Neural Networks, especially when applied to image datasets. To achieve this, you will simply implement your own custom convolutional layer ``MyConv2d`` and make sure that it yields the same results as [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv2d#torch.nn.Conv2d).\n",
    "\n",
    "\n",
    "## Contents:\n",
    "\n",
    "1. Utils\n",
    "2. Implement a custom layer in Pytorch: ``MyConv2d``\n",
    "3. Use ``MyConv2d`` inside a neural network model\n",
    "4. Test ``MyConv2d`` by comparing it to [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv2d#torch.nn.Conv2d)\n",
    "\n",
    "## Andrew's videos related to this exercise:\n",
    "- [C4W1L02 Edge Detection Examples](https://www.youtube.com/watch?v=XuD4C8vJzEQ&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=2)\n",
    "- [C4W1L04 Padding](https://www.youtube.com/watch?v=smHa2442Ah4&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=4)\n",
    "- [C4W1L05 Strided Convolutions](https://www.youtube.com/watch?v=tQYZaDn_kSg&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=5)\n",
    "- [C4W1L06 Convolutions Over Volumes](https://www.youtube.com/watch?v=KTB_OFoAQcc&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=6)\n",
    "- [C4W1L07 One Layer of a Convolutional Net](https://www.youtube.com/watch?v=jPOAS7uCODQ&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from datetime import datetime\n",
    "from typing import Sequence\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "torch.manual_seed(123)\n",
    "# We use torch.double to get the same results as Pytorch\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Utils\n",
    "\n",
    "Nothing to see in the cell below, just the definition functions we'll need later, you don't even need to read them, just know that there are 2 functions:\n",
    "\n",
    "- ``train``: Train a model, save weight values of the convolutional layer for each epoch of the training. Return them, stored in 2 lists: ``weight_values`` and ``bias_values``.\n",
    "- ``int_to_pair(n)`` : Return `(n, n)` if `n` is an int or `n` if `n` is already a tuple of length 2.\n",
    "- ``relative_error(a, b)``: Compute the relative error of ``b``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "print(f\"Device {device}.\")\n",
    "\n",
    "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    \"\"\"\n",
    "    Train our model and save weight values\n",
    "    \"\"\"\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    weight_values = []\n",
    "    bias_values = []\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "\n",
    "            # We use torch.double to get the same results as Pytorch\n",
    "            imgs = imgs.to(device=device, dtype=torch.double) \n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        losses_train.append(loss_train / n_batch)\n",
    "        \n",
    "        # Here we store weight values at each step of the training process\n",
    "        with torch.no_grad():\n",
    "            weight_values.append(model.conv1.weight.data.clone().detach())\n",
    "            if model.conv1.bias is not None:\n",
    "                bias_values.append(model.conv1.bias.data.clone().detach())\n",
    "\n",
    "        print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "            datetime.now().time(), epoch, loss_train / n_batch))\n",
    "    return weight_values, bias_values\n",
    "\n",
    "def int_to_pair(n):\n",
    "    \"\"\"\n",
    "    Return `(n, n)` if `n` is an int or `n` if it is already a tuple of length 2\n",
    "    \"\"\"\n",
    "    # If n is a float or integer\n",
    "    if not isinstance(n, Sequence):\n",
    "        return (int(n), int(n))\n",
    "    elif len(n) == 1:\n",
    "        return (int(n[0]), int(n[0]))\n",
    "    elif len(n) == 2:\n",
    "        return ( int(n[0]), int(n[1]) )\n",
    "    else:\n",
    "        raise ValueError(\"Please give an int or a pair of int\")\n",
    "    \n",
    "\n",
    "def relative_error(a, b):\n",
    "    return (torch.norm(a - b) / torch.norm(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective this week is not to solve a classification task, only to implement a convolutional layer. We will then use a smaller version of the MNIST dataset in order to reduce computational time. \n",
    "\n",
    "## TODO:\n",
    "\n",
    "Write a ``load_MNIST`` function that:\n",
    "- Load and preprocess the MNIST training dataset such that images are\n",
    "  - center-cropped from 28x28 to 24x24 pixels.  \n",
    "  - transformed to tensor\n",
    "  - normalized using appropriate mean and standard deviation values. \n",
    "- return a small portion (10%) of the MNIST training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the train dataset:         6000\n"
     ]
    }
   ],
   "source": [
    "def load_MNIST(data_path='../data/', preprocessor=None):\n",
    "    \n",
    "    if preprocessor is None:\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.CenterCrop(24),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.1306], [0.3080]),\n",
    "        ])\n",
    "    \n",
    "    # load datasets\n",
    "    data_train_val = datasets.MNIST(\n",
    "        data_path,       \n",
    "        train=True,      \n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "\n",
    "    # keep only a few samples\n",
    "    n_kept = int(len(data_train_val)*0.1)\n",
    "    n_thrown =  len(data_train_val) - n_kept\n",
    "\n",
    "    data_train, _ = random_split(\n",
    "        data_train_val, \n",
    "        [n_kept, n_thrown],\n",
    "        generator=torch.Generator().manual_seed(123)\n",
    "    )\n",
    "\n",
    "    print(\"Size of the train dataset:        \", len(data_train))\n",
    "    \n",
    "    return data_train\n",
    "\n",
    "data_train = load_MNIST()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement a custom layer in Pytorch: MyConv2d \n",
    "\n",
    "In the cell below, there is a template of a ``MyConv2d`` class that would re-create a [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv2d#torch.nn.Conv2d) layer. By solving the 4 problems below you will complete this class step by step.\n",
    "\n",
    "First of all, defining a custom layer in PyTorch is very similar to defining a custom neural network or a custom block of layers: they all consist in subclassing the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=nn%20module#torch.nn.Module) class (see the 3rd tutorial for more details). As usual, we have to create a class that subclasses nn.Module and that implements a [forward](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward) method that defines what happens to a given input.\n",
    "\n",
    "\n",
    "## TODO:\n",
    "\n",
    "### 1. Compute the output shape of a convolutional layer\n",
    "\n",
    "Reminder about Pytorch notations and dimensions from the 2nd tutorial:\n",
    "\n",
    "- ``N``: batch size,         (how many inputs do you feed at the same time)\n",
    "- ``C``: number of channels, (number of color channels RGB=3, RGBA=4, etc if refering to an image or the number of filters if refering to a convolutional layer)\n",
    "- ``H``: height of the image\n",
    "- ``W``: width of the image\n",
    "\n",
    "\n",
    "Take a look at the [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv2d#torch.nn.Conv2d) documentation and scroll down to examine the output shape formula. To get an illustration of this formula, you can also watch Andrew's video  [C4W1L07 One Layer of a Convolutional Net](https://www.youtube.com/watch?v=jPOAS7uCODQ&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=7) \n",
    "\n",
    "Write a method ``get_output_size(self, x)`` (so inside the ``MyConv2d``  class), such that:\n",
    "- ``x`` is an input batch of dimension ``(N, C_in, H_in, W_in)``\n",
    "- it returns the output shape of a convolutional layer ``(N, C_out, H_out, W_out)`` (following Pytorch's notations). Note that we have:\n",
    "  - ``N = x.shape[0]``\n",
    "  - ``C_in = self.in_channels``\n",
    "  - ``H_in = x.shape[-2]``\n",
    "  - ``W_in = x.shape[-1]``\n",
    "  - ``kernel_size = self.kernel_size`` (Note: it's a tuple (kernel_size_height, kernel_size_width))\n",
    "  - ``padding = self.padding`` (Note: it's a tuple (padding_height, padding_width))\n",
    "  - ``stride = self.stride`` (Note: it's a tuple (stride_height, stride_width))\n",
    "\n",
    "### 2. Apply the padding transform to an image\n",
    "\n",
    "In this video [C4W1L04 Padding](https://www.youtube.com/watch?v=smHa2442Ah4&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=4), you learned how to apply a padding transform to an image.\n",
    "\n",
    "Write a method ``apply_padding(self, x)`` (so inside the ``MyConv2d`` class), such that: \n",
    "- ``x`` is an input batch of dimension ``(N, C_in, H_in, W_in)``\n",
    "- it returns a tensor ``x_pad``, whose center values are the same as ``x`` values but with extra zeros on the border (the numbers of zeros to add are defined by ``self.padding``). **Note:**  ``x_pad`` is then of dimension ``(N, C_in, H_in + 2*self.padding[0], W_in + 2*self.padding[1])``\n",
    "\n",
    "### 3. Apply the convolution operation to an image\n",
    "\n",
    "You learned how to apply convolution to an image, in the following videos:\n",
    "\n",
    "- [C4W1L02 Edge Detection Examples](https://www.youtube.com/watch?v=XuD4C8vJzEQ&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=2)\n",
    "- [C4W1L05 Strided Convolutions](https://www.youtube.com/watch?v=tQYZaDn_kSg&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=5)\n",
    "- [C4W1L06 Convolutions Over Volumes](https://www.youtube.com/watch?v=KTB_OFoAQcc&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=6)\n",
    "- [C4W1L07 One Layer of a Convolutional Net](https://www.youtube.com/watch?v=jPOAS7uCODQ&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=7)\n",
    "\n",
    "Write a method ``apply_conv(self, x_pad)`` (so inside the ``MyConv2d``  class), such that \n",
    "- ``x_pad`` is the padded version of the input batch ``x``.\n",
    "- it returns ``out`` (whose shape is computed by ``self.get_out_put_size``) the output of the convolutional operation applied to ``x_pad``. **Note:** since we follow Pytorch's implementation of convolution, it has to be possible choose to have a bias or not. So in your ``apply_conv`` there must be somewhere a condition ``if self.bias is None: ...... else: ....... ``\n",
    "\n",
    "**Note**: A few word about vectorization: In python you should always favour vectorized computations because it is much faster. However, the most important thing is still that you get the right result. So try to vectorize computations as much as possible but it is fine if a few for loops remain.\n",
    "\n",
    "### 4. Implementing a Convolutional layer (forward method)\n",
    "\n",
    "In the [forward](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward) method, combine your previously defined methods to:\n",
    "\n",
    "1. Figure out the output shape expected ``(N, C_out, H_out, W_out)`` by calling ``self.get_output_size``\n",
    "2. Apply padding to ``x`` by calling ``self.apply_padding``\n",
    "4. Compute ``out``, the result of the convolution operation by calling ``self.apply_conv`` (depending on point 3. you might have to have some for loops)\n",
    "5. Return ``out``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom convolutional 2d layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels:int,\n",
    "        out_channels:int,\n",
    "        kernel_size, \n",
    "        stride = (1, 1), \n",
    "        padding = (0, 0), \n",
    "        bias:bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        in_channels: Number of input channels \n",
    "        out_channels: Number of output channels (number of filters)\n",
    "        kernel_size: Filter's size (tuple of int)\n",
    "        stride: Length of the kernel's jumps  (tuple of int)\n",
    "        padding: Number of pixels to add around the image (tuple of int)\n",
    "        bias: Should a bias parameter be added to each filter or not?\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        # Will NOT be automatically added to the list\n",
    "        # of trainable parameter (see doc of nn.Parameter)\n",
    "        self.in_channels = int(in_channels)\n",
    "        self.out_channels = int(out_channels)\n",
    "        self.padding = int_to_pair(padding)\n",
    "        self.stride = int_to_pair(stride)\n",
    "        self.kernel_size = int_to_pair(kernel_size)\n",
    "\n",
    "        # Will be automatically added to the list of \n",
    "        # model's trainable parameters (see doc quoted above)\n",
    "        # Dim = (C_out, C_in, kernel_height, kernel_width)\n",
    "        self.weight = nn.Parameter(torch.Tensor(1, self.out_channels, self.in_channels, self.kernel_size[0], self.kernel_size[1]))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(self.out_channels))\n",
    "        else:\n",
    "            self.bias = None\n",
    "            #print(self.bias) # Uncomment this to make sure that the printed output starts with  \"Parameter containing: ...\"\n",
    "        #print(self.weight) # Uncomment this to make sure that the printed output starts with  \"Parameter containing: ...\"\n",
    "\n",
    "\n",
    "    def get_output_size(self, x):\n",
    "        # shape of x: (N, C_in, H_in, W_in)\n",
    "        # Note: batch_size is the only dimension that is not influenced\n",
    "        # by the convolution operation\n",
    "        H_out = (x.shape[-2] + 2*self.padding[0] - self.kernel_size[0]) // self.stride[0] + 1 \n",
    "        W_out = (x.shape[-1] + 2*self.padding[1] - self.kernel_size[1]) // self.stride[1] + 1 \n",
    "        return (int(x.shape[0]), self.out_channels, H_out, W_out)\n",
    "\n",
    "    def apply_padding(self, x):\n",
    "        # shape of x: (N, C_in, H_in, W_in)\n",
    "        # shape of x_pad: (N, 1, C_in, H_in + 2*pad_height, W_in + 2*pad_width)\n",
    "        x_pad = torch.zeros(x.shape[0], 1, x.shape[1], x.shape[2]+2*self.padding[0], x.shape[3]+2*self.padding[1])\n",
    "        x_pad[:, 0, :, self.padding[0]:(x.shape[2]+self.padding[0]), self.padding[1]:(x.shape[3]+self.padding[1])] = torch.clone(x)\n",
    "        return x_pad\n",
    "\n",
    "    def apply_conv(self, x_pad, i, j):\n",
    "        start_h = i*self.stride[0]\n",
    "        end_h = min(start_h + self.kernel_size[0], x_pad.shape[-2])\n",
    "        start_w = j*self.stride[1]\n",
    "        end_w = min(start_w + self.kernel_size[1], x_pad.shape[-1])\n",
    "        # shape of x_pad: (N, 1, C_in, H_in + 2*pad_height, W_in + 2*pad_width)\n",
    "        # shape of weight: (C_out, C_in, kernel_height, kernel_width)\n",
    "        # shape of tmp_mul: (N, C_out, C_in, kernel_height, kernel_width)\n",
    "        # shape of output: (N, C_out)\n",
    "        if self.bias is not None:\n",
    "            return torch.sum(\n",
    "                    self.weight[:, :, 0:(end_h-start_h), 0:(end_w-start_w)] * x_pad[:, :, :, start_h:end_h, start_w:end_w],\n",
    "                    dim=(2,3,4) ,keepdim=False\n",
    "                ) + self.bias\n",
    "        else:\n",
    "            return torch.sum(\n",
    "                    self.weight[:, :, 0:(end_h-start_h), 0:(end_w-start_w)] * x_pad[:, :, :, start_h:end_h, start_w:end_w],\n",
    "                    dim=(2,3,4) ,keepdim=False\n",
    "                )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Required method for any nn.Module class\n",
    "        \"\"\"\n",
    "        # shape of x: (N, C_in, H_in, W_in)\n",
    "        (batch_size, C_out, H_out, W_out) = self.get_output_size(x)\n",
    "        x_pad = self.apply_padding(x)\n",
    "        out = torch.empty((batch_size, C_out, H_out, W_out))\n",
    "        for i in range(H_out):\n",
    "            for j in range(W_out):\n",
    "                out[:,:,i, j] = self.apply_conv(x_pad, i, j)\n",
    "        return out\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Standard python method to implement if you want to custom your ``print(MyConv2d(...))``\n",
    "\n",
    "        This method is not mandotory, it's just so that you can print MyConv2d\n",
    "        the same way a Conv2d layer is printed\n",
    "        \"\"\"\n",
    "        string = (\n",
    "            \"MyConv2d(\" + str(self.in_channels) + \", \" + str(self.out_channels)\n",
    "            +\", kernel size=\" + str(self.kernel_size) + \", stride=\" + str(self.stride)\n",
    "            +\", padding=\" + str(self.padding) + \", bias=\" +str(self.bias)\n",
    "        )\n",
    "        return string\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use MyConv2d inside a neural network model\n",
    "\n",
    "Here is defined a very basic neural network to test ``MyConv2d`` on MNIST data. It consists of: \n",
    "\n",
    "- a 2D-Convolutional layer (``MyConv2d`` or ``nn.Conv2d``) (see ``conv_type`` parameter)\n",
    "- a tanh activation function\n",
    "- a 2D-MaxPooling layer, to reduce the size of the image and therefore reduce the number of parameters\n",
    "- a fully connected layer with 10 outputs for the 10 classes of the MNIST dataset\n",
    "\n",
    "You don't have to do anything here, simply run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple net with only one conv layer and one fc layer. \n",
    "    \n",
    "    The convolutional layer can be ``MyConv2d`` or ``nn.Conv2d``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels:int,\n",
    "        out_channels:int,\n",
    "        kernel_size = (3, 3), \n",
    "        stride = (1, 1), \n",
    "        padding = (0, 0), \n",
    "        bias:bool = False,\n",
    "        conv_type:str = 'custom',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        in_channels: Number of input channels \n",
    "        out_channels: Number of output channels (number of filters)\n",
    "        kernel_size: Filter's size (tuple of int)\n",
    "        stride: Length of the kernel's jumps  (tuple of int)\n",
    "        padding: Number of pixels to add around the image (tuple of int)\n",
    "        bias: Should a bias parameter be added to each filter or not?\n",
    "        conv_type: Should MyConv2d or nn.Conv2d be used? \n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        # Make sure these parameters are all pairs of int\n",
    "        kernel_size = int_to_pair(kernel_size)\n",
    "        stride = int_to_pair(stride)\n",
    "        padding = int_to_pair(padding)\n",
    "        # Use MyConv2d\n",
    "        if conv_type == 'custom':\n",
    "            self.conv1 = MyConv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        # or use pytorch's Conv2d\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        # Output shape\n",
    "        H_out = (24 + 2*padding[0] - kernel_size[0]) // stride[0] + 1 \n",
    "        W_out = (24 + 2*padding[1] - kernel_size[1]) // stride[1] + 1 \n",
    "        \n",
    "        # Divide by 2 here because we will apply a pooling layer\n",
    "        self.fc2 = nn.Linear((H_out//2)*(W_out//2)*out_channels, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, C_in, H, W)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = out.view(-1, out.shape[-3]*out.shape[-2]*out.shape[-1])\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test MyConv2d by comparing it to nn.Conv2d\n",
    "\n",
    "MyConv2d and nn.Conv2d are compared by storing after each epoch:\n",
    "\n",
    "- Weight and biases values of the convolutional layer\n",
    "- Training loss\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "- Your implementation will probably be much slower than nn.Conv2d.\n",
    "- If your implementation is correct, the training loss printed should be identical and the expected relative error should be around 1e-16.\n",
    "- As reminded below: You can play with the following parameters if you want, especially for debugging purpose, but before submitting your notebook:\n",
    "  - set ``n_epochs`` to ``4`` \n",
    "  - set ``c_out`` to  ``>= 2`` \n",
    "  - set ``kernel = (n1, n2)`` such that ``n1 != n2`` \n",
    "  - set ``stride = (n3, n4)`` such that ``n3 != n4``\n",
    "  - set ``padding = (n5, n6)`` such that ``n5 != n6``\n",
    "  - set ``bias`` to ``True``\n",
    "\n",
    "## TODO\n",
    "\n",
    "1. According to you, why is your implementation slower? (There could be multiple reasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n",
      "\n",
      " ========= Training using MyConv2d =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/natacha/anaconda3/envs/nglm-env/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:04:36.283716  |  Epoch 1  |  Training loss 1.65041\n",
      "11:04:49.787356  |  Epoch 2  |  Training loss 0.87041\n",
      "11:05:02.959676  |  Epoch 3  |  Training loss 0.63724\n",
      "11:05:17.267263  |  Epoch 4  |  Training loss 0.53621\n",
      "\n",
      " ========= Training using nn.Conv2d =========\n",
      "11:05:18.885795  |  Epoch 1  |  Training loss 1.65041\n",
      "11:05:20.759910  |  Epoch 2  |  Training loss 0.87041\n",
      "11:05:22.560617  |  Epoch 3  |  Training loss 0.63724\n",
      "11:05:24.338566  |  Epoch 4  |  Training loss 0.53621\n",
      "\n",
      " ======= Relative error:     nn.Conv2d    VS    MyConv2d   =========\n",
      "weight:   [tensor(1.2782e-16), tensor(1.2495e-16), tensor(1.5564e-16), tensor(1.4324e-16)]\n",
      "bias:     [tensor(2.0313e-16), tensor(1.9168e-16), tensor(1.5030e-16), tensor(1.9176e-16)]\n"
     ]
    }
   ],
   "source": [
    "# DONT USE GPU!!! IT WOULD REQUIRE USING register_buffer TO MOVE THE MODEL CORRECTLY TO\n",
    "# THE GPU. (See https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer)\n",
    "# And you probably don't want to spend the entire week learning how to use this properly\n",
    "device = torch.device('cpu')\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "# We set shuffle to False so that we can compare both models more accurately\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=512, shuffle=False)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# These parameters don't matter much in this assignment\n",
    "n_epochs = 4    \n",
    "lr = 0.1\n",
    "c_in = 1        # Grey scale images so c_in = 1\n",
    "\n",
    "# You can play with the following parameters if you want, especially for debugging purpose but before submitting your notebook:\n",
    "# - set c_out to  >= 2 \n",
    "# - set kernel = (n1, n2) such that n1 != n2 \n",
    "# - set stride = (n3, n4) such that n3 != n4\n",
    "# - set padding = (n5, n6) such that n5 != n6\n",
    "# - set bias to True\n",
    "c_out = 8    \n",
    "kernel = (3,4)\n",
    "stride = (2,1)\n",
    "padding = (1,2)\n",
    "bias = True \n",
    "\n",
    "torch.manual_seed(265)\n",
    "# Using MyConv2d\n",
    "model01 = MyNet(c_in, c_out, kernel_size=kernel, padding=padding, stride=stride, bias=bias, conv_type='custom').to(device=device) \n",
    "# Using nn.Conv2d\n",
    "model02 = MyNet(c_in, c_out, kernel_size=kernel, padding=padding, stride=stride, bias=bias, conv_type='pytorch').to(device=device) \n",
    "\n",
    "# Make both models start with the same weight values\n",
    "# Note that contrary to project 1, setting a manual seed before each\n",
    "# model instanciation is not enough because the convolutional layers\n",
    "# are implemented differently\n",
    "with torch.no_grad():\n",
    "    model01.conv1.weight.data = model02.conv1.weight.data.clone()\n",
    "    if model01.conv1.bias is not None:\n",
    "        model01.conv1.bias.data = model02.conv1.bias.data.clone()\n",
    "    model01.fc2.bias.data = model02.fc2.bias.data.clone()\n",
    "    model01.fc2.weight.data = model02.fc2.weight.data.clone()\n",
    "\n",
    "print(\"\\n ========= Training using MyConv2d =========\")\n",
    "\n",
    "optimizer = optim.SGD(model01.parameters(), lr=lr)\n",
    "weights01, biases01 = train(\n",
    "    n_epochs = n_epochs,\n",
    "    optimizer = optimizer, \n",
    "    model = model01,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "print(\"\\n ========= Training using nn.Conv2d =========\")\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model02.parameters(), lr=lr)\n",
    "weights02, biases02 = train(\n",
    "    n_epochs = n_epochs,\n",
    "    optimizer = optimizer, \n",
    "    model = model02,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "print(\"\\n ======= Relative error:     nn.Conv2d    VS    MyConv2d   =========\")\n",
    "print(\"weight:  \", [relative_error(weights02[i], weights01[i]) for i in range(len(weights02))] )\n",
    "if bias:\n",
    "    print(\"bias:    \", [relative_error(biases02[i], biases01[i]) for i in range(len(biases02))] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[ 0.0919,  0.1978, -0.1818,  0.1813],\n",
      "          [ 0.0879,  0.0478,  0.1608, -0.2375],\n",
      "          [ 0.3274,  0.0829,  0.1484,  0.0470]]],\n",
      "\n",
      "\n",
      "        [[[-0.1470,  0.2617,  0.1812,  0.3049],\n",
      "          [ 0.3262,  0.0980,  0.2830,  0.0120],\n",
      "          [ 0.2943, -0.1640, -0.2505, -0.0157]]],\n",
      "\n",
      "\n",
      "        [[[-0.2092, -0.0616, -0.0547,  0.0385],\n",
      "          [ 0.1728,  0.2239, -0.0740,  0.1061],\n",
      "          [-0.0549,  0.1786, -0.0521, -0.0992]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1670, -0.0755, -0.1079,  0.0503],\n",
      "          [ 0.0337, -0.0064,  0.0045,  0.2014],\n",
      "          [ 0.2909,  0.2476,  0.3405,  0.0170]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2229,  0.1066, -0.3072, -0.1484],\n",
      "          [-0.1369, -0.2767,  0.0317, -0.2690],\n",
      "          [ 0.0960, -0.0063, -0.2061,  0.0387]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2009,  0.2604, -0.0370,  0.1221],\n",
      "          [ 0.1912, -0.0198, -0.1543,  0.2744],\n",
      "          [-0.1815,  0.2016, -0.1290,  0.1012]]],\n",
      "\n",
      "\n",
      "        [[[-0.1920,  0.0951, -0.2677,  0.1269],\n",
      "          [-0.2961, -0.1194, -0.1663, -0.0071],\n",
      "          [ 0.1783,  0.0483, -0.2019, -0.0959]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3427, -0.1069,  0.1790,  0.1681],\n",
      "          [ 0.2251,  0.0203, -0.0429, -0.2378],\n",
      "          [-0.0161,  0.2918, -0.1960, -0.2282]]]]), tensor([[[[ 0.0989,  0.2089, -0.1779,  0.1702],\n",
      "          [ 0.1235,  0.0839,  0.1813, -0.2288],\n",
      "          [ 0.3722,  0.1210,  0.1695,  0.0586]]],\n",
      "\n",
      "\n",
      "        [[[-0.1193,  0.2907,  0.2111,  0.3264],\n",
      "          [ 0.3584,  0.1285,  0.3084,  0.0288],\n",
      "          [ 0.3170, -0.1456, -0.2379, -0.0111]]],\n",
      "\n",
      "\n",
      "        [[[-0.1970, -0.0480, -0.0386,  0.0454],\n",
      "          [ 0.1986,  0.2505, -0.0501,  0.1210],\n",
      "          [-0.0238,  0.2093, -0.0285, -0.0854]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1422, -0.0844, -0.1097,  0.0443],\n",
      "          [ 0.0518,  0.0199,  0.0334,  0.2238],\n",
      "          [ 0.3320,  0.2933,  0.3873,  0.0561]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2263,  0.0894, -0.3299, -0.1788],\n",
      "          [-0.1436, -0.3002,  0.0119, -0.2943],\n",
      "          [ 0.0848, -0.0348, -0.2359,  0.0085]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2273,  0.2939, -0.0049,  0.1499],\n",
      "          [ 0.2172,  0.0097, -0.1261,  0.2998],\n",
      "          [-0.1670,  0.2157, -0.1174,  0.1063]]],\n",
      "\n",
      "\n",
      "        [[[-0.2412,  0.0564, -0.2926,  0.0964],\n",
      "          [-0.3325, -0.1532, -0.1992, -0.0519],\n",
      "          [ 0.1580,  0.0260, -0.2297, -0.1338]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3865, -0.0675,  0.2126,  0.1975],\n",
      "          [ 0.2571,  0.0438, -0.0263, -0.2246],\n",
      "          [-0.0074,  0.2935, -0.1963, -0.2285]]]]), tensor([[[[ 0.0965,  0.2111, -0.1790,  0.1582],\n",
      "          [ 0.1405,  0.1036,  0.1907, -0.2278],\n",
      "          [ 0.3966,  0.1419,  0.1789,  0.0625]]],\n",
      "\n",
      "\n",
      "        [[[-0.1058,  0.3066,  0.2282,  0.3384],\n",
      "          [ 0.3747,  0.1448,  0.3205,  0.0353],\n",
      "          [ 0.3275, -0.1390, -0.2364, -0.0142]]],\n",
      "\n",
      "\n",
      "        [[[-0.1912, -0.0419, -0.0312,  0.0462],\n",
      "          [ 0.2128,  0.2657, -0.0361,  0.1283],\n",
      "          [-0.0055,  0.2284, -0.0148, -0.0779]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1243, -0.0912, -0.1130,  0.0370],\n",
      "          [ 0.0559,  0.0303,  0.0456,  0.2307],\n",
      "          [ 0.3531,  0.3184,  0.4131,  0.0767]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2289,  0.0789, -0.3444, -0.1997],\n",
      "          [-0.1504, -0.3166, -0.0015, -0.3119],\n",
      "          [ 0.0742, -0.0553, -0.2563, -0.0121]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2408,  0.3123,  0.0131,  0.1653],\n",
      "          [ 0.2289,  0.0250, -0.1110,  0.3131],\n",
      "          [-0.1624,  0.2210, -0.1140,  0.1054]]],\n",
      "\n",
      "\n",
      "        [[[-0.2770,  0.0344, -0.3039,  0.0790],\n",
      "          [-0.3575, -0.1731, -0.2192, -0.0849],\n",
      "          [ 0.1517,  0.0181, -0.2459, -0.1623]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4112, -0.0442,  0.2319,  0.2143],\n",
      "          [ 0.2733,  0.0552, -0.0188, -0.2182],\n",
      "          [-0.0075,  0.2896, -0.2015, -0.2332]]]]), tensor([[[[ 0.0920,  0.2109, -0.1816,  0.1473],\n",
      "          [ 0.1504,  0.1170,  0.1957, -0.2289],\n",
      "          [ 0.4131,  0.1565,  0.1846,  0.0639]]],\n",
      "\n",
      "\n",
      "        [[[-0.0979,  0.3173,  0.2400,  0.3466],\n",
      "          [ 0.3848,  0.1553,  0.3276,  0.0387],\n",
      "          [ 0.3344, -0.1359, -0.2379, -0.0184]]],\n",
      "\n",
      "\n",
      "        [[[-0.1891, -0.0390, -0.0265,  0.0455],\n",
      "          [ 0.2210,  0.2760, -0.0265,  0.1320],\n",
      "          [ 0.0070,  0.2427, -0.0054, -0.0741]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1098, -0.0972, -0.1169,  0.0294],\n",
      "          [ 0.0556,  0.0351,  0.0522,  0.2325],\n",
      "          [ 0.3664,  0.3356,  0.4310,  0.0903]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2321,  0.0728, -0.3542, -0.2153],\n",
      "          [-0.1560, -0.3288, -0.0115, -0.3254],\n",
      "          [ 0.0647, -0.0714, -0.2712, -0.0268]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2492,  0.3251,  0.0258,  0.1760],\n",
      "          [ 0.2353,  0.0350, -0.1007,  0.3219],\n",
      "          [-0.1609,  0.2239, -0.1127,  0.1035]]],\n",
      "\n",
      "\n",
      "        [[[-0.3048,  0.0204, -0.3094,  0.0673],\n",
      "          [-0.3765, -0.1874, -0.2334, -0.1115],\n",
      "          [ 0.1507,  0.0163, -0.2554, -0.1844]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4281, -0.0279,  0.2449,  0.2256],\n",
      "          [ 0.2830,  0.0618, -0.0151, -0.2156],\n",
      "          [-0.0098,  0.2847, -0.2076, -0.2394]]]])]\n",
      "[tensor([-0.2516, -0.0220,  0.1484, -0.1901, -0.2598,  0.0529,  0.1405, -0.1817]), tensor([-0.2975, -0.0866,  0.1298, -0.2509, -0.2378,  0.0047,  0.1540, -0.2233]), tensor([-0.3273, -0.1249,  0.1126, -0.2879, -0.2185, -0.0265,  0.1687, -0.2517]), tensor([-0.3491, -0.1511,  0.0975, -0.3141, -0.2018, -0.0490,  0.1815, -0.2727])]\n"
     ]
    }
   ],
   "source": [
    "print(weights01)\n",
    "print(biases01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nglm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "71c2cb666ff353b4e7b5c350d66179fa0af5c84ce239ad9fa105d94543f3ad59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
