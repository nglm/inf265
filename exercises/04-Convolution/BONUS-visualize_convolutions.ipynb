{
 "cells": [
  {
   "source": [
    "# CNN: visualization and interpretation\n",
    "\n",
    "#### General instructions\n",
    "\n",
    "You have roughly **2 WEEKS** to complete and submit each of the remaining assigments. There are 3 weekly group sessions available to help you complete the assignments. Attendance is not mandatory but recommended. However, assignments are graded each week and not submitting them or submitting them after the deadline will give you no points\n",
    "\n",
    "**FORMAT**: Jupyter notebook  \n",
    "**DEADLINE**: Sunday 21st March, 23:59\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Deep learning models are often criticized for being a black box. To assert this problem many visualization techniques have been developped in order to get a better understanding of how the model is learning and how to interpret its results. In this assignment we will go through some of these techniques, based on this week's module's video *[Lecture 12 | Visualizing and Understanding](https://www.youtube.com/watch?v=6wcs6szJWMY&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=15)* \n",
    "\n",
    "\n",
    "## Contents:\n",
    "\n",
    "1. Preliminary  \n",
    "  1.1 Back to pre-trained networks   \n",
    "  1.2 Accessing layers and weights of the ResNet model  \n",
    "2. First convolutional layer  \n",
    "3. Last layer: Nearest neighbors  \n",
    "  3.1 Comparing images pixel wise  \n",
    "  3.2 Comparing images in the feature space of the last layer of resnet   \n",
    "4. Last layer: Dimensionality reduction and distinguishing between kitchens and trains  \n",
    "5. Hidden convolutional layers: Maximum activations  \n",
    "6. Occlusion experiments  \n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from os import listdir\n",
    "from math import ceil, sqrt\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.nn.functional as F\n",
    "import seaborn  as sns\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preliminary\n",
    "\n",
    "### 1.1 Back to pre-trained networks\n",
    "\n",
    "In this assignment we will use the very same model we used in the first assignment of this course: [ResNet101](https://pytorch.org/vision/stable/models.html#torchvision.models.resnet101). This network is a deep neural network that has been trained with the [ImageNet](http://image-net.org/) dataset. In this assignmnent though, we will use a different dataset, a subset of the [COCO](https://cocodataset.org/#download) dataset and we will see that ResNet can still do amazing things with this dataset! \n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "Take a deep breath and enjoy how more familiar this output looks compared to when you first met it 6 weeks ago. :) Now Conv2d has no secret for you, nor has BatchNorm2d, MaxPool2d, Linear and Sequential. Even the PyTorch concept of block of layers (as Bottleneck is here) should ring a bell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet101(pretrained=True)   # 101 means that we choose the ResNet architecture with 101 layers\n",
    "print(resnet)                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One important thing we have seen so far in our course seems to be is missing in the above output though. It's called ResNet but there is no mention of residual layer.... The pytorch implementation of ResNet is of course available and can be found ([here](https://github.com/pytorch/vision/blob/7d4154735f421b254c408c16e0980b1ca0dd9b8e/torchvision/models/resnet.py)) and more specifically we can also inspect the BottleNeck block [here](https://github.com/pytorch/vision/blob/7d4154735f421b254c408c16e0980b1ca0dd9b8e/torchvision/models/resnet.py#L86).\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "Copy paste below the 2 lines inside the BottleNeck class that are responsible for the residual connection. (You can comment them if you want to avoid error when running the entire notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First line here\n",
    "# [...] some stuff in between\n",
    "# Second line here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Accessing layers and weights of the ResNet model\n",
    "\n",
    "Recalling that we can access the named modules of a network using ``network.named_module`` and that if the selected module is a trainable layer, the weigths can be accessed using ``network.named_module.weight``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, m in resnet.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some modules are not directly layers but blocks of layers. For instance ``layer1, layer2, layer3`` are Sequential objects and the layers that compose them are their children (see [nn.Module.named_children](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=named%20children#torch.nn.Module.named_children)). We can access them as we would access an element of a list or a tensor/array. For example to access the ``conv2`` sublayer of the child at index ``21`` of ``layer3`` (so what is called ``layer3.21.conv1`` in the output above) we write ``resnet.layer3[21].conv1``\n",
    "\n",
    "More generally if we want to access ``sublayer`` from the ``ith`` child of ``layer`` in the model resnet we use:\n",
    "\n",
    "``resnet.layer[i].sublayer``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show that layer 3 is not directly a layer, but a block of layer\n",
    "print(\"resnet.layer3 has %d children. Each of them are Bottleneck blocks\" %len(resnet.layer3))\n",
    "print(\"resnet.layer3[0] is a Bottleneck block whose layers can be easily accessed\" )\n",
    "print(\"resnet.layer3[0].conv1: \", resnet.layer3[0].conv1 )\n",
    "print(resnet.layer3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "## TODO\n",
    "\n",
    "1. Write a function ``get_layer(model, layer_name, i_child=None, sublayer_name=None)`` that returns the wanted layer, i.e it returns ``resnet.layer`` if ``sublayer_name`` is ``None`` and it returns ``resnet.layer[i].sublayer`` otherwise\n",
    "\n",
    "**Hint**, you can use ``getattr(object, string variable)`` to access ``object.myattribute`` when your string variable contains \"myattribute\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To illustrate how to use getattr\n",
    "print(resnet.bn1 ==  getattr(resnet, \"bn1\"))\n",
    "\n",
    "\n",
    "def get_layer(model, layer_name, i_child=None, sublayer_name=None):\n",
    "    # TODO!\n",
    "    ...\n",
    "\n",
    "\n",
    "# To test your function\n",
    "hidden_sublayer = get_layer(resnet, layer_name='layer4', i_child=0, sublayer_name='bn3')\n",
    "fc_layer = get_layer(resnet, layer_name='fc')\n",
    "\n",
    "print(\"\\nresnet.layer4[0].bn3 has successfully been extracted:  \",  hidden_sublayer == resnet.layer4[0].bn3)\n",
    "print(hidden_sublayer)\n",
    "print(hidden_sublayer.weight.shape)\n",
    "\n",
    "print(\"\\nresnet.fc has successfully been extracted:             \", fc_layer == resnet.fc)\n",
    "print(fc_layer)\n",
    "print(fc_layer.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. First convolutional layer\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "1. Write a function ``rescale(w)`` that takes as input a tensor ``w`` and returns a rescaled version of ``w`` such as all its elements are now in range $[0, 1]$ \n",
    "\n",
    "**Hint** You can use the ``min`` and ``max`` elements (that are floats, regardless of ``w.shape``) and [torch.Tensor.abs](https://pytorch.org/docs/stable/generated/torch.abs.html#torch.abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nrows_ncols(nplots):\n",
    "    \"\"\"\n",
    "    To get the right number of cols and rows in a plot\n",
    "    \"\"\"\n",
    "    ncols = ceil(sqrt(nplots))\n",
    "    nrows = ceil(nplots/ncols)\n",
    "    return (nrows,ncols)\n",
    "\n",
    "def rescale(w):\n",
    "    # TODO!\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter weight visualization\n",
    "\n",
    "*(Starting at 5:30 of the video lecture)*\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "In the cell below we will visualize the weights of each filter of the first convolutional layer. Reminder: shape of the weights of a convolutional layer is as follows ``(C_out, C_in, kernel_size_h, kernel_size_w``) with ``C_in`` number of input channels (so here 3 because it's the first layer and the inputs are RGB images) and  with ``C_out`` number of outputs channels so what we call number of filters here.\n",
    "\n",
    "1. Why do we need the rescale function in this function? Try to run the cell below without rescaling the weights (and then go back to the original code)\n",
    "1. If an input patch is exactly similar to the visualization of a given filter, what will be the corresponding filter's activation value? (Positive and high value?, negative and high value?, around 0?)\n",
    "1. If an input patch is the exact opposite of the visualization of a given filter (like R is high in the image when R is low in the filter, and same holds for G and B), what will be its corresponding filter's activation? (Positive and high value?, negative and high value?, around 0?)\n",
    "1. If an input patch is neither similar to the visualization of a given filter nor its exact opposite but just something that has nothing to do with it, what will be its corresponding filter's activation? (Positive and high value?, negative and high value?, around 0?)\n",
    "1. Can we visualize hidden convolutional layers as easily as the first one? Why?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_first_conv(model):\n",
    "    first_conv = model.conv1\n",
    "    first_conv_w = first_conv.weight.data\n",
    "\n",
    "    C_out = first_conv_w.shape[0]\n",
    "    (nrows,ncols) = get_nrows_ncols(C_out)\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15,15), sharex=True, sharey=True)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # For each filter in the first conv layer, print its weight values\n",
    "    for c in range(C_out):\n",
    "        rescaled = rescale(first_conv_w[c])\n",
    "        axs.flat[c].imshow(rescaled.permute(2,1,0)) \n",
    "    plt.show()\n",
    "visualize_first_conv(resnet)"
   ]
  },
  {
   "source": [
    "### Load a subset of the COCO dataset \n",
    "\n",
    "We will use ``CocoSubset_trains_kitchens`` and ``CocoSubset_zebras`` later\n",
    "\n",
    "``get_activation`` is a utility function that will allow us to store the activation values of a given layer for each input we will give. (Copied from the [pytorch forum](https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/6))"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note we don't normalize here to make the visualization easier \n",
    "# and because we don't really care about the performance\n",
    "preprocessor = transforms.Compose([\n",
    "    transforms.Resize(256),     \n",
    "    transforms.CenterCrop(224), \n",
    "    transforms.ToTensor(),\n",
    "])  \n",
    "\n",
    "def load_CocoSubset(data_path = '../data/CocoSubset/', transform = preprocessor):\n",
    "    imgs_t = []                       # Where input tensors will be stored  \n",
    "    list_files = listdir(data_path)   # Find all filenames in the 'imgs/' folder\n",
    "    n_imgs = len(list_files)\n",
    "    \n",
    "    for f in list_files:\n",
    "        img = Image.open(data_path + f)\n",
    "        img = img.convert('RGB')\n",
    "        img_t = transform(img) \n",
    "        imgs_t.append(torch.unsqueeze(img_t, 0))\n",
    "    print('Size of the dataset: ', n_imgs)\n",
    "    return torch.cat(imgs_t, dim=0)\n",
    "\n",
    "def load_CocoSubset_trains_kitchens(data_path_root= '../data/', transform = preprocessor):\n",
    "    imgs_kitchens = load_CocoSubset(data_path = data_path_root + 'CocoSubset_Kitchens/', transform = transform)\n",
    "    imgs_trains = load_CocoSubset(data_path = data_path_root + 'CocoSubset_Trains/', transform = transform)\n",
    "    return imgs_kitchens, imgs_trains\n",
    "\n",
    "def load_CocoSubset_zebras(data_path_root= '../data/', transform = preprocessor):\n",
    "    imgs_horses = load_CocoSubset(data_path = data_path_root + 'CocoSubset_Zebras/', transform = transform)\n",
    "    return imgs_horses\n",
    "\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    \"\"\"\n",
    "    To store activation values\n",
    "    \"\"\"\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "imgs = load_CocoSubset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Last layer: Nearest neighbors\n",
    "\n",
    "*(Starting at 12:30 of the video lecture)*\n",
    "\n",
    "### Pixel wise distance\n",
    "\n",
    "A naive method for comparing 2 images could be to compute their pixelwise distance. That is to say, if 2 images ``img1 , img2`` are represented by 2 arrays of dimension ``(C, H, W)``, their distance could be computed as: $$\\sqrt{\\sum_{h} \\sum_{w} \\sum_{c} (img1[c, h, w] - img2[c, h, w])^2}$$\n",
    "\n",
    "Which can be interpreted as considering that an image is one very long vector of dimension ``C*H*W``\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "1. Write a function ``compute_mean_pixel_value_image(img)`` that takes as input ``img``, a tensor of dimension ``(C, H, W)`` and that:   \n",
    "  1.1 Compute the mean value pixel value ``mean_pixel`` of ``img`` (so the mean is a tensor of 3 elements)  \n",
    "  2.1 Create and return a tensor of the same shape of ``img`` whose pixels values are all equal to ``mean_pixel``  \n",
    "\n",
    "**Hint** You can use [torch.mean](https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean) and [torch.ones_like](https://pytorch.org/docs/stable/generated/torch.ones_like.html?highlight=ones_like#torch.ones_like)\n",
    "\n",
    "1. Write a function ``compute_pixel_wise_distance(img1, img2)`` that takes as input ``img1 , img2``, 2 tensors of dimension ``(C, H, W)`` and that returns the pixelwise distance between  ``img1`` and ``img2``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_pixel_value_image(img):\n",
    "    # TODO!\n",
    "    ...\n",
    "\n",
    "def compute_pixel_wise_distance(img1, img2):\n",
    "    # TODO!\n",
    "    ..."
   ]
  },
  {
   "source": [
    "### 3.1 Comparing images pixel wise\n",
    "\n",
    "In the cell below we do the following:\n",
    "\n",
    "1. Compute pairwise distances in the image space (input space) of all the images in our dataset\n",
    "2. Sort these pairwise distances so that for each image in the dataset, we find the other images that are the closest to it in the image space\n",
    "3. For a each image in a given set of reference images, plot the images that are the closest to it.\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "Run the cell below and answer the following questions\n",
    "\n",
    "1. Are there common objects in the images that are considered 'close' to the reference image? \n",
    "1. If there are common objects, are they always in the same place?\n",
    "1. Is the background similar (in terms of pixel values)?\n",
    "1. Would you say that the images are similar in terms of content / context ?\n",
    "1. If we take an image and the exact same one but horizontally flipped , would they be close to each other pixel wise?\n",
    "1. Are there any reference images whose their corresponding mean pixel value image is actually closer than the closest images found in the dataset?\n",
    "1. Would you say that pixel-wise distance is a good measure of how similar the content of 2 images can be? "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_closest_images(imgs, distance, argsort_distance, n_imgs = 10, n_closest = 4, seed=42, show_mean=True):\n",
    "    \"\"\"\n",
    "    Given pairwise distances and their argsort, for each randomly selected reference image, \n",
    "    plot the images that are the closest to it\n",
    "    \"\"\"\n",
    "    # Get a random subset of the indices of imgs\n",
    "    idx = np.arange(len(imgs))\n",
    "    np.random.seed(seed) \n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:n_imgs]\n",
    "\n",
    "    ncols = n_closest\n",
    "    # Add an extra image composed of the mean pixel value\n",
    "    if show_mean:\n",
    "        ncols += 1\n",
    "\n",
    "\n",
    "    for i in idx:\n",
    "        \n",
    "        fig, axs = plt.subplots(nrows=1, ncols=ncols, figsize=(15,4))\n",
    "        fig.tight_layout()\n",
    "        for n in range(n_closest):\n",
    "            if n == 0:\n",
    "                ax_title = \"Reference\"\n",
    "            else:\n",
    "                ax_title = \"closest image nÂ°\" + str(n)  +\"\\n Dist: %.3f\"%distance[i, argsort_distance[i,n]]\n",
    "\n",
    "            # For each reference image, plots the images that are the closest to it in the image space\n",
    "            axs.flat[n].imshow(imgs[argsort_distance[i,n]].permute(1,2,0)) \n",
    "            axs.flat[n].axes.xaxis.set_visible(False)\n",
    "            axs.flat[n].axes.yaxis.set_visible(False)\n",
    "            axs.flat[n].axes.set_title(ax_title)  \n",
    "\n",
    "        # UNCOMMENT THIS PART ONLY IF YOU HAVE SUCCEEDED IN WRITING THE FUNCTIONS IN THE PREVIOUS CELL\n",
    "        # **************************************************************************************\n",
    "        # Plot the mean pixel value image of the reference image\n",
    "        # if show_mean:\n",
    "        #     mean_img = compute_mean_pixel_value_image(imgs[argsort_distance[i,0]])\n",
    "        #     axs.flat[-1].imshow(mean_img.permute(1,2,0)) \n",
    "        #     axs.flat[-1].axes.xaxis.set_visible(False)\n",
    "        #     axs.flat[-1].axes.yaxis.set_visible(False)\n",
    "        #     dist = compute_pixel_wise_distance(mean_img, imgs[argsort_distance[i,0]])\n",
    "        #     ax_title = \"Mean pixel value of ref image \\n Dist: %.3f\"%dist\n",
    "        #     axs.flat[-1].axes.set_title(ax_title)  \n",
    "        # **************************************************************************************\n",
    "\n",
    "# Both following cells share the same seed so that if they are run successively they share the same reference images\n",
    "# And you can re-run successively the 2 following cells to start with new reference images\n",
    "seed = randint(0,221) \n",
    "\n",
    "# Compute pairwise distance in the image space (input space) of all the images in our dataset\n",
    "img_space_distance = pairwise_distances(imgs.flatten(1))\n",
    "# Sort these pairwise distances so that for each image in the dataset, we find the other images that are the closest to it in the image space\n",
    "argsort_img_space_distances = np.argsort(img_space_distance, axis=1)\n",
    "# Plot all that :) \n",
    "plot_closest_images(imgs, img_space_distance, argsort_img_space_distances, seed=seed)"
   ]
  },
  {
   "source": [
    "### 3.2 Comparing images in the feature space of the last layer of resnet\n",
    "\n",
    "In the cell below we will use the same reference images as in the cell above, but this time we will compare them in the feature space of the last layer of the model. To do so we will give the images as input to the model and store the activations values of the last layer corresponding to each input image into a tensor ``activations_last_layer``\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "In the cell below do the following (you can find some inspiration in the cell above):\n",
    "\n",
    "1. Compute pairwise distances in the feature space of all the images in our dataset. That is to say, compute pairwise distances between all the activations values (stored in ``activations_last_layer``) of the last layer. \n",
    "2. Sort these pairwise distances so that for each image in the dataset, we find the other images that are the closest to it in the feature space\n",
    "3. For a each image in a given set of reference images, plot the images that are the closest to it. (use ``plot_closest_images`` from the cell above)\n",
    "\n",
    "\n",
    "Run the cell below and answer the following questions\n",
    "\n",
    "1. Are there common objects in the images that are considered 'close' to the reference image? \n",
    "1. If there are common objects, are they always in the same place?\n",
    "1. Is the background similar (in terms of pixel values)?\n",
    "1. Would you say that the images are similar in terms of content / context ?\n",
    "1. If we take an image and the exact same one but horizontally flipped , would they be close to each other in the feature space?\n",
    "1. Are there any reference images whose their corresponding mean pixel value image is actually closer than the closest images found in the dataset?\n",
    "1. Would you say that the model is able to associate images whose contents are similar? (Again this model has not been trained with this dataset nor with the type of objects in these images)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_space_values(model, imgs, layer_names=['avgpool'], i_children=[None], sublayer_names=[None]):\n",
    "    \"\"\"\n",
    "    Returns a list of activation values for each input image and each layer given as parameters\n",
    "    \"\"\"\n",
    "    # Where we will store all the layers to be registered\n",
    "    layers_to_register = []\n",
    "    \n",
    "    # We tell our model to keep track of the output values of the given layers\n",
    "    for l, (layer_name, i_child, sublayer_name) in enumerate(zip(layer_names, i_children, sublayer_names)):\n",
    "        layers_to_register.append(get_layer(model, layer_name=layer_name, i_child=i_child, sublayer_name=sublayer_name))\n",
    "        layers_to_register[-1].register_forward_hook(get_activation(layers_to_register[-1]))\n",
    "\n",
    "    # Where we will store the activations of each layer for each input\n",
    "    activations = [[] for _ in range(len(layers_to_register))]\n",
    "    \n",
    "    resnet.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Forward pass\n",
    "        # We don't have that many images, so we can pass the all at once\n",
    "        resnet(imgs)\n",
    "\n",
    "    # Get the output values of the registered layers corresponding to each input images \n",
    "    for l, layer_to_register in enumerate(layers_to_register):\n",
    "        activations[l] = activation[layer_to_register].squeeze()\n",
    "\n",
    "    return activations\n",
    "\n",
    "\n",
    "# This if condition is there because it's quite slow to get the activations values but I wanted to keep this line in this cell for clarity purpose\n",
    "if 'activations_last_layer' not in globals():\n",
    "  activations_last_layer = get_feature_space_values(resnet, imgs, layer_names=['avgpool'])[0]\n",
    "\n",
    "# (Batch-size, last layer shape)\n",
    "print(activations_last_layer.shape)\n",
    "\n",
    "# Compute pairwise distance in the feature space (last layer)\n",
    "# TODO!\n",
    "...\n",
    "# For each reference image, find the images that are the closest to it in the feature space\n",
    "# TODO!\n",
    "...\n",
    "# Plot all that :) \n",
    "# TODO!\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Last layer: Dimensionality reduction and distinguishing between kitchens and trains\n",
    "\n",
    "*(Starting at 17:20 of the video lecture)*\n",
    "\n",
    "Visualizing tensors that do not represent images and that are arrays or vectors of high dimension is really hard. Most of the time we are compelled to go back to $\\mathrm{R}^2$ or $\\mathrm{R}^3$ by using a dimensionality reduction method. However we might loose crucial information when projecting a high dimensional space onto $\\mathrm{R}^2$ or $\\mathrm{R}^3$ and most of the time it is impossible to extract any meaningful information from raw high-dimensional data directly projected  onto a 2D space. However in this section we will see that we can project the last layer activations onto $\\mathrm{R}^2$ and that in this projection we can see that the model has managed to distinguish between 2 classes of images.\n",
    "\n",
    "Here we will play with only 2 types of images: kitchens and trains. Both labels were NOT among the labels Resnet has been trained with.\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "In the ``Projection from the input space`` section:\n",
    "\n",
    "1. Instantiate a PCA model that would keep only the first 2 principal components (see [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html?highlight=pca#sklearn.decomposition.PCA))\n",
    "1. Fit the PCA model with the tensor containing both kitchens and trains images (see [PCA.fit](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html?highlight=pca#sklearn.decomposition.PCA.fit)) **Note** you must first see each image as a vector of dimension ``C*H*W`` see [torch.flatten](https://pytorch.org/docs/stable/tensors.html?highlight=flatten#torch.Tensor.flatten) and its ``start dim`` parameter)\n",
    "1. Finally, project  ``imgs_kitchens`` and then ``imgs_trains`` onto $\\mathrm{R}^2$ using your PCA model (see [PCA.transform](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html?highlight=pca#sklearn.decomposition.PCA.transform)). Store the output in ``imgs_kitchens_transformed`` and ``imgs_trains_transformed``\n",
    "\n",
    "In the ``Projection from the feature space`` section:\n",
    "\n",
    "1. Instantiate another PCA model that would keep only the first 2 principal components \n",
    "1. Fit this second PCA model with the tensor containing both the kitchens and the trains activations values in the last layer.\n",
    "1. Finally, project  ``act_kitchens`` and then ``act_trains`` onto $\\mathrm{R}^2$ using your PCA model. Store the output in ``act_kitchens_transformed`` and ``act_trains_transformed``\n",
    "\n",
    "Visualization: \n",
    "\n",
    "1. Could you distinguish between kitchen and trains in the 2D projection of the input space?\n",
    "1. Could you distinguish between kitchen and trains in the 2D projection of the feature space?\n",
    "1. The model has not been trained to distinguish between trains and kitchen but do you think that the model is after all able to distinguish between trains and kitchens?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pca(imgs_kitchens_transformed, imgs_trains_transformed, act_kitchens_transformed, act_trains_transformed):\n",
    "    \"\"\"\n",
    "    Plot the PCA projections from the input space and from the feature space onto R^2\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(ncols = 2, figsize=(15,7))\n",
    "    fig.tight_layout()\n",
    "    axs[0].scatter(imgs_kitchens_transformed[:,0], imgs_kitchens_transformed[:,1], c='orange', label='kitchens')\n",
    "    axs[0].scatter(imgs_trains_transformed[:,0], imgs_trains_transformed[:,1], label='trains')\n",
    "    axs[0].axes.set_title('PCA in the input image space')  \n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].scatter(act_kitchens_transformed[:,0], act_kitchens_transformed[:,1], c='orange', label='kitchens')\n",
    "    axs[1].scatter(act_trains_transformed[:,0], act_trains_transformed[:,1], label='trains')\n",
    "    axs[1].axes.set_title('PCA in the feature space (last layer)')  \n",
    "    axs[1].legend()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Projection from the input space\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Load the kitchens and trains images \n",
    "imgs_kitchens, imgs_trains = load_CocoSubset_trains_kitchens()\n",
    "# Create a big tensor contaning both kitchens and trains\n",
    "imgs_2 = torch.cat([imgs_kitchens, imgs_trains])\n",
    "\n",
    "# Fit a PCA model that keeps only the first 2 principal component\n",
    "# TODO!\n",
    "...\n",
    "# Project images from the input space onto R2 using the PCA model  \n",
    "# TODO!\n",
    "...\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Projection from the feature space\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Get the activations values for both kitchens and trains\n",
    "act_kitchens = get_feature_space_values(resnet, imgs=imgs_kitchens, layer_names=['avgpool'])[0] # [0] Because the function returns a list of activations for each layer\n",
    "act_trains = get_feature_space_values(resnet, imgs=imgs_trains, layer_names=['avgpool'])[0]\n",
    "# Create a big tensor contaning both kitchens and trains activations values \n",
    "act_2 = torch.cat([act_kitchens, act_trains])\n",
    "\n",
    "# Fit a PCA model that keeps only the first 2 principal component\n",
    "# TODO!\n",
    "... \n",
    "# Project activations from the feature space onto R2 using the PCA model  \n",
    "# TODO!\n",
    "...\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Visualization\n",
    "# -------------------------------------------------------\n",
    "\n",
    "visualize_pca(imgs_kitchens_transformed, imgs_trains_transformed, act_kitchens_transformed, act_trains_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hidden convolutional layers: Maximum activations\n",
    "\n",
    "*(Starting at 22:00 of the video lecture)*\n",
    "\n",
    "In the cell below (which might take some time to run) we get the activation values for all input images (back to the entire COCO_Subset) in 3 different hidden convolutional layers: the first one we keep track of is at the very beginning of the model, the second one in the middle of the model and the last one near the end of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# Getting the activation values\n",
    "# ----------------------------------\n",
    "layer_names = ['layer1', 'layer2', 'layer3']\n",
    "i_children = [0, 1, 22]\n",
    "sublayer_names = ['conv2']*3\n",
    "\n",
    "# Again, just in case you re-run this cell accidentally\n",
    "if 'activations_convs' not in globals():\n",
    "    activations_convs = get_feature_space_values(resnet, imgs, layer_names=layer_names, i_children=i_children, sublayer_names=sublayer_names)\n",
    "\n",
    "for activations_conv in activations_convs:\n",
    "    print(activations_conv.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_max_act(imgs, activations, argsort_act, n_imgs = 9):\n",
    "    \"\"\"\n",
    "    Plot images alongside the activations of the filters\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(nrows = n_imgs , ncols = 2*len(activations), figsize=(15,20))\n",
    "    fig.tight_layout()\n",
    "    for i in range(n_imgs):\n",
    "        for c, (act_c, argsort_act_c) in enumerate(zip(activations, argsort_act)):\n",
    "\n",
    "            # Plot the original image\n",
    "            axs[i, 2*c].imshow(imgs[ argsort_act_c[i][0] ].permute(1,2,0)) \n",
    "            axs[i, 2*c].axes.xaxis.set_visible(False)\n",
    "            axs[i, 2*c].axes.yaxis.set_visible(False)\n",
    "            axs[i, 2*c].axes.set_title(\"Original image\")\n",
    "\n",
    "            # Plot the activations of the filter that was the most activated in average by this image\n",
    "            axs[i, 1+2*c].imshow(act_c[ argsort_act_c[i][0], argsort_act_c[i][1] ]) \n",
    "            axs[i, 1+2*c].axes.xaxis.set_visible(False)\n",
    "            title = \"Filter #%d \\nUnits activations\" %argsort_act_c[i][1]\n",
    "            if i == 0:\n",
    "                title = \"Layer #%d\\n\\n\" %(c+1) + title\n",
    "            axs[i, 1+2*c].axes.yaxis.set_visible(False)\n",
    "            axs[i, 1+2*c].axes.set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "In the cell below we try to find the images and filters that resulted in the max activations. We then plots for each layer the input images that resulted in a high activation alongside their associated filter activations.\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "1. Do the 3 convolutional layers inspected seem to look for the same kind of thing in the image?\n",
    "1. Do you think that the depth of the convolutional layer impacts its behaviour? \n",
    "1. In the layer #1, what seems to activate the filter #57 \n",
    "1. In the layer #3, what seems to activate the filter #150? And the filter #139?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# Visualzing filter activations \n",
    "# ----------------------------------\n",
    "\n",
    "n_conv = len(activations_convs)\n",
    "argsort_act_convs = [[] for _ in range(n_conv)]\n",
    "act_convs = [[] for _ in range(n_conv)]\n",
    "\n",
    "for i in range(n_conv):\n",
    "    # We don't take the borders element because they are a bit biased\n",
    "    act_convs[i] = rescale(activations_convs[i][:,:,1:-1,1:-1])\n",
    "\n",
    "    # For each input image and each filter, find its mean activation\n",
    "    mean_activation = torch.mean(act_convs[i], dim=(2,3))\n",
    "\n",
    "    # Sort in descending order so that we can find the images and filters that resulted in the max mean activation\n",
    "    argsort_act_convs[i] = torch.argsort(mean_activation, descending=True)\n",
    "    argsort_act_convs[i] = np.dstack(np.unravel_index(\n",
    "            np.argsort(np.array(mean_activation).ravel()), (mean_activation.shape[0], mean_activation.shape[1])\n",
    "        )).squeeze()\n",
    "\n",
    "# Plot images alongside the activations of the filters that maximized the activation\n",
    "plot_max_act(imgs, act_convs, argsort_act_convs, n_imgs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Occlusion experiments\n",
    "\n",
    "*(Starting at 29:00 of the video lecture)*\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "\n",
    "1. Describe as thoroughly as possible the experiment implemented in the following cells.   \n",
    "  1.1 What is the objective pf the experiment?  \n",
    "  1.2 What is ``occlusion`` doing?  \n",
    "  1.3 Do the model has to be trained to classify the type of image used for the experiment to work properly? Why?  \n",
    "1. Which parts of the zebra seem to be the most essential to the model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occlusion(model, image, label, occ_size = 50, occ_stride = 50, occ_pixel = 0., device=torch.device('cpu')):\n",
    "    model.eval()\n",
    "  \n",
    "    # get the width and height of the image\n",
    "    H_in, W_in = image.shape[-2], image.shape[-1]\n",
    "    pad = occ_size - occ_stride\n",
    "  \n",
    "    # setting the output image width and height\n",
    "    H_out = (H_in + 2*pad -occ_size) // occ_stride + 1\n",
    "    W_out = (W_in + 2*pad - occ_size) // occ_stride + 1\n",
    "  \n",
    "    # Instantiante heatmap\n",
    "    heatmap = torch.zeros((H_out, W_out))\n",
    "\n",
    "    # Apply some padding so that the heatmap locations seem more accurate\n",
    "    input_image = F.pad(image.clone().detach(), (pad, pad)).to(device=device)\n",
    "    \n",
    "    for h in range(H_out):\n",
    "\n",
    "        images_w = [input_image.clone() for _ in range(W_out)]\n",
    "\n",
    "        for w in range(W_out):            \n",
    "            h_start = h*occ_stride\n",
    "            w_start = w*occ_stride\n",
    "            h_end = min(H_in, h_start + occ_size)\n",
    "            w_end = min(W_in, w_start + occ_size)\n",
    "\n",
    "            # replacing all the pixel in the image with occ_pixel (default black) in the specified location\n",
    "            images_w[w][:,:, h_start:h_end, w_start:w_end] = occ_pixel\n",
    "        \n",
    "        # Create a batch containing all the occluded images at a given height\n",
    "        images_w = torch.cat(images_w).to(device=device)\n",
    "            \n",
    "        # Make predictions and get the probability of the given label\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images_w)\n",
    "            outputs = nn.functional.softmax(outputs, dim=1)\n",
    "        _, index = torch.max(outputs, 1)\n",
    "        probs = outputs[:,label]\n",
    "        \n",
    "        #setting the heatmap location to probability value\n",
    "        heatmap[h, :] = probs\n",
    "\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images containing exclusively zebras  \n",
    "imgs_zebras = load_CocoSubset_zebras()\n",
    "print(imgs_zebras.shape)\n",
    "\n",
    "# Using GPU if possible as this experience is a bit slow...\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"On device {device}.\")\n",
    "resnet_gpu = models.resnet101(pretrained=True).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_zebra = 340 # Label of zebra in the ImageNet dataset\n",
    "\n",
    "for img in imgs_zebras:\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Plot original image\n",
    "    axs.flat[0].imshow(img.permute(1,2,0)) \n",
    "    axs.flat[0].axes.xaxis.set_visible(False)\n",
    "    axs.flat[0].axes.yaxis.set_visible(False)\n",
    "    axs.flat[0].set_title(\"Original image\")\n",
    "\n",
    "    # Compute occlusion map\n",
    "    # This might be slow. You can set a higher stride if you need. But the experience will be much less interesting if the strides goes higher\n",
    "    # Try first with fewer images if it is really slow\n",
    "    heatmap = occlusion(model=resnet_gpu, image=img.unsqueeze(dim=0), label=label_zebra, occ_size=60, occ_stride=10, device=device)\n",
    "\n",
    "    # Plot the occlusion map\n",
    "    sns.heatmap(heatmap, cbar=True, xticklabels=False, yticklabels=False, ax=axs.flat[1], vmax=1)\n",
    "    axs.flat[1].set_title(\"Occlusion map\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}