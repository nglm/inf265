{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Size of the training dataset:  9017\n",
      "Size of the validation dataset:  983\n",
      "Size of the test dataset:  2000\n"
     ]
    }
   ],
   "source": [
    "def load_cifar(train_val_split=0.9, data_path='../data/', preprocessor=None):\n",
    "    \n",
    "    # Define preprocessor if not already given\n",
    "    if preprocessor is None:\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                                (0.2470, 0.2435, 0.2616))\n",
    "        ])\n",
    "    \n",
    "    # load datasets\n",
    "    data_train_val = datasets.CIFAR10(\n",
    "        data_path,       \n",
    "        train=True,      \n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "\n",
    "    data_test = datasets.CIFAR10(\n",
    "        data_path, \n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "\n",
    "    # train/validation split\n",
    "    n_train = int(len(data_train_val)*train_val_split)\n",
    "    n_val =  len(data_train_val) - n_train\n",
    "\n",
    "    # Add seed so that we get the same dataloaders\n",
    "    data_train, data_val = random_split(\n",
    "        data_train_val, \n",
    "        [n_train, n_val],\n",
    "        generator=torch.Generator().manual_seed(123)\n",
    "    )\n",
    "    \n",
    "    # Now define a lighter version of CIFAR10: cifar\n",
    "    label_map = {0: 0, 2: 1}\n",
    "\n",
    "    # For each dataset, keep only airplanes and birds\n",
    "    cifar2_train = [(img, label_map[label]) for img, label in data_train if label in [0, 2]]\n",
    "    cifar2_val = [(img, label_map[label]) for img, label in data_val if label in [0, 2]]\n",
    "    cifar2_test = [(img, label_map[label]) for img, label in data_test if label in [0, 2]]\n",
    "\n",
    "    print('Size of the training dataset: ', len(cifar2_train))\n",
    "    print('Size of the validation dataset: ', len(cifar2_val))\n",
    "    print('Size of the test dataset: ', len(cifar2_test))\n",
    "    \n",
    "    return (cifar2_train, cifar2_val, cifar2_test)\n",
    "\n",
    "data_train, data_val, data_test = load_cifar()\n",
    "\n",
    "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    \n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "\n",
    "            imgs = imgs.to(device=device, dtype=torch.double) \n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "    return losses_train\n",
    "\n",
    "def compute_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device=device, dtype=torch.double) \n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "\n",
    "    acc =  correct / total\n",
    "    print(\"Accuracy: {:.2f}\".format(acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # No need to declare activation functions nor maxpool layers anymore\n",
    "        self.fc1 = nn.Linear(32*32*3, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Activation functions now come from the functional API \n",
    "        out = torch.flatten(x, 1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.relu(self.fc3(out))\n",
    "        # Note that we don't need a softmax function in the output layer if we\n",
    "        # use nn.CrossEntropyLoss as the loss function\n",
    "        out = self.fc4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_manual_update(n_epochs, model, loss_fn, train_loader, lr=1e-2, momentum_coeff=0., weight_decay=0.):\n",
    "    \n",
    "    model.train()\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    \n",
    "    # To store the previous gradients\n",
    "    dict_momentum = {}\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            \n",
    "            imgs = imgs.to(device=device, dtype=torch.double) \n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for name, p in model.named_parameters():\n",
    "                    if p.grad is not None:\n",
    "                        \n",
    "                            grad = p.grad\n",
    "\n",
    "                            # L2 regularization\n",
    "                            if weight_decay:\n",
    "                                grad = grad + weight_decay * p.data\n",
    "\n",
    "                            # Momentum version\n",
    "                            if momentum_coeff:\n",
    "                                # If previous gradients available\n",
    "                                # then grad = grad + previous_grad * momentum_coeff\n",
    "                                if name in dict_momentum:\n",
    "                                    grad = grad + dict_momentum[name] * momentum_coeff\n",
    "                                dict_momentum[name] = grad\n",
    "                                \n",
    "                            # Weight update formula here\n",
    "                            p.data = p.data - lr*grad\n",
    "\n",
    "                            # Still need to zero out the gradient \n",
    "                            p.grad = torch.zeros_like(p.grad)                     \n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        losses_train.append(loss_train / n_batch)\n",
    "        \n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "                datetime.now().time(), epoch,\n",
    "                loss_train / n_batch))\n",
    "    return losses_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/natacha/anaconda3/envs/nglm-env/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n",
      "\n",
      "   Global parameters:\n",
      "batch_size =  256\n",
      "n_epoch =  30\n",
      "loss_fn =  CrossEntropyLoss()\n",
      "seed =  265\n",
      "\n",
      " ========================================================= \n",
      "   Current parameters: \n",
      "lr = 0.01\n",
      "mom = 0\n",
      "decay = 0\n",
      "\n",
      " --------- Using Pytorch's SGD --------- \n",
      "15:27:35.605270  |  Epoch 1  |  Training loss 0.68093\n",
      "15:27:40.688064  |  Epoch 5  |  Training loss 0.54943\n",
      "15:27:47.054778  |  Epoch 10  |  Training loss 0.46524\n",
      "15:27:53.257568  |  Epoch 15  |  Training loss 0.42144\n",
      "15:27:59.615822  |  Epoch 20  |  Training loss 0.38399\n",
      "15:28:04.971721  |  Epoch 25  |  Training loss 0.35094\n",
      "15:28:10.766444  |  Epoch 30  |  Training loss 0.31793\n",
      "\n",
      " --- Accuracies --- \n",
      "Training\n",
      "Accuracy: 0.88\n",
      "Validation\n",
      "Accuracy: 0.83\n",
      "\n",
      " --------- Using manual update ---------- \n",
      "15:28:13.014504  |  Epoch 1  |  Training loss 0.68093\n",
      "15:28:19.681324  |  Epoch 5  |  Training loss 0.54943\n",
      "15:28:26.238622  |  Epoch 10  |  Training loss 0.46524\n",
      "15:28:32.575133  |  Epoch 15  |  Training loss 0.42144\n",
      "15:28:39.409641  |  Epoch 20  |  Training loss 0.38399\n",
      "15:28:47.866196  |  Epoch 25  |  Training loss 0.35094\n",
      "15:28:54.901863  |  Epoch 30  |  Training loss 0.31793\n",
      "\n",
      " --- Accuracies --- \n",
      "Training\n",
      "Accuracy: 0.88\n",
      "Validation\n",
      "Accuracy: 0.83\n",
      "\n",
      " ========================================================= \n",
      "   Current parameters: \n",
      "lr = 0.01\n",
      "mom = 0\n",
      "decay = 0.01\n",
      "\n",
      " --------- Using Pytorch's SGD --------- \n",
      "15:28:56.584109  |  Epoch 1  |  Training loss 0.68103\n",
      "15:29:02.281089  |  Epoch 5  |  Training loss 0.55376\n",
      "15:29:09.971498  |  Epoch 10  |  Training loss 0.47008\n",
      "15:29:19.032535  |  Epoch 15  |  Training loss 0.42922\n",
      "15:29:28.241046  |  Epoch 20  |  Training loss 0.39408\n",
      "15:29:36.335902  |  Epoch 25  |  Training loss 0.36438\n",
      "15:29:44.158214  |  Epoch 30  |  Training loss 0.33567\n",
      "\n",
      " --- Accuracies --- \n",
      "Training\n",
      "Accuracy: 0.87\n",
      "Validation\n",
      "Accuracy: 0.83\n",
      "\n",
      " --------- Using manual update ---------- \n",
      "15:29:47.163674  |  Epoch 1  |  Training loss 0.68103\n",
      "15:29:56.388759  |  Epoch 5  |  Training loss 0.55376\n",
      "15:30:05.207449  |  Epoch 10  |  Training loss 0.47008\n",
      "15:30:13.610137  |  Epoch 15  |  Training loss 0.42922\n",
      "15:30:21.929857  |  Epoch 20  |  Training loss 0.39408\n",
      "15:30:30.404753  |  Epoch 25  |  Training loss 0.36438\n",
      "15:30:39.078477  |  Epoch 30  |  Training loss 0.33567\n",
      "\n",
      " --- Accuracies --- \n",
      "Training\n",
      "Accuracy: 0.87\n",
      "Validation\n",
      "Accuracy: 0.83\n",
      "\n",
      " ========================================================= \n",
      "   Current parameters: \n",
      "lr = 0.01\n",
      "mom = 0.9\n",
      "decay = 0\n",
      "\n",
      " --------- Using Pytorch's SGD --------- \n",
      "15:30:41.131709  |  Epoch 1  |  Training loss 0.61787\n",
      "15:30:46.561217  |  Epoch 5  |  Training loss 0.36053\n",
      "15:30:53.532891  |  Epoch 10  |  Training loss 0.22891\n",
      "15:31:00.975329  |  Epoch 15  |  Training loss 0.25503\n",
      "15:31:07.768451  |  Epoch 20  |  Training loss 0.15373\n",
      "15:31:14.622177  |  Epoch 25  |  Training loss 0.16546\n",
      "15:31:21.671228  |  Epoch 30  |  Training loss 0.07555\n",
      "\n",
      " --- Accuracies --- \n",
      "Training\n",
      "Accuracy: 0.97\n",
      "Validation\n",
      "Accuracy: 0.83\n",
      "\n",
      " --------- Using manual update ---------- \n",
      "15:31:23.931476  |  Epoch 1  |  Training loss 0.61787\n",
      "15:31:30.849225  |  Epoch 5  |  Training loss 0.36053\n",
      "15:31:39.036897  |  Epoch 10  |  Training loss 0.22891\n",
      "15:31:47.561861  |  Epoch 15  |  Training loss 0.25503\n",
      "15:31:56.124781  |  Epoch 20  |  Training loss 0.15373\n",
      "15:32:04.366268  |  Epoch 25  |  Training loss 0.16546\n",
      "15:32:14.073238  |  Epoch 30  |  Training loss 0.07555\n",
      "\n",
      " --- Accuracies --- \n",
      "Training\n",
      "Accuracy: 0.97\n",
      "Validation\n",
      "Accuracy: 0.83\n",
      "\n",
      " ========================================================= \n",
      "   Current parameters: \n",
      "lr = 0.01\n",
      "mom = 0.9\n",
      "decay = 0.01\n",
      "\n",
      " --------- Using Pytorch's SGD --------- \n",
      "15:32:16.341249  |  Epoch 1  |  Training loss 0.61972\n",
      "15:32:23.350286  |  Epoch 5  |  Training loss 0.37262\n",
      "15:32:31.482353  |  Epoch 10  |  Training loss 0.26217\n",
      "15:32:40.051164  |  Epoch 15  |  Training loss 0.20043\n",
      "15:32:49.058091  |  Epoch 20  |  Training loss 0.19053\n",
      "15:32:57.452878  |  Epoch 25  |  Training loss 0.15727\n",
      "15:33:05.959197  |  Epoch 30  |  Training loss 0.08699\n",
      "\n",
      " --- Accuracies --- \n",
      "Training\n",
      "Accuracy: 0.96\n",
      "Validation\n",
      "Accuracy: 0.84\n",
      "\n",
      " --------- Using manual update ---------- \n",
      "15:33:08.830914  |  Epoch 1  |  Training loss 0.61972\n",
      "15:33:17.195033  |  Epoch 5  |  Training loss 0.37262\n",
      "15:33:27.630327  |  Epoch 10  |  Training loss 0.26217\n",
      "15:33:39.487334  |  Epoch 15  |  Training loss 0.20043\n",
      "15:33:49.997598  |  Epoch 20  |  Training loss 0.19053\n",
      "15:33:59.917693  |  Epoch 25  |  Training loss 0.15727\n",
      "15:34:09.844068  |  Epoch 30  |  Training loss 0.08699\n",
      "\n",
      " --- Accuracies --- \n",
      "Training\n",
      "Accuracy: 0.96\n",
      "Validation\n",
      "Accuracy: 0.84\n",
      "\n",
      " ========================================================= \n",
      "   Current parameters: \n",
      "lr = 0.01\n",
      "mom = 0.9\n",
      "decay = 0.001\n",
      "\n",
      " --------- Using Pytorch's SGD --------- \n",
      "15:34:11.804257  |  Epoch 1  |  Training loss 0.61805\n",
      "15:34:17.670192  |  Epoch 5  |  Training loss 0.36084\n",
      "15:34:25.666121  |  Epoch 10  |  Training loss 0.22181\n",
      "15:34:33.149762  |  Epoch 15  |  Training loss 0.24899\n",
      "15:34:41.312123  |  Epoch 20  |  Training loss 0.11224\n",
      "15:34:49.423027  |  Epoch 25  |  Training loss 0.12177\n",
      "15:34:57.055142  |  Epoch 30  |  Training loss 0.05682\n",
      "\n",
      " --- Accuracies --- \n",
      "Training\n",
      "Accuracy: 0.98\n",
      "Validation\n",
      "Accuracy: 0.83\n",
      "\n",
      " --------- Using manual update ---------- \n",
      "15:34:59.260751  |  Epoch 1  |  Training loss 0.61805\n",
      "15:35:06.841876  |  Epoch 5  |  Training loss 0.36084\n",
      "15:35:16.691645  |  Epoch 10  |  Training loss 0.22181\n",
      "15:35:28.186123  |  Epoch 15  |  Training loss 0.24899\n",
      "15:35:38.790608  |  Epoch 20  |  Training loss 0.11224\n",
      "15:35:48.491566  |  Epoch 25  |  Training loss 0.12177\n",
      "15:35:57.542884  |  Epoch 30  |  Training loss 0.05682\n",
      "\n",
      " --- Accuracies --- \n",
      "Training\n",
      "Accuracy: 0.98\n",
      "Validation\n",
      "Accuracy: 0.83\n",
      "\n",
      " ========================================================= \n",
      "   Current parameters: \n",
      "lr = 0.01\n",
      "mom = 0.8\n",
      "decay = 0.01\n",
      "\n",
      " --------- Using Pytorch's SGD --------- \n",
      "15:35:59.448439  |  Epoch 1  |  Training loss 0.64125\n",
      "15:36:05.807860  |  Epoch 5  |  Training loss 0.40867\n",
      "15:36:15.054189  |  Epoch 10  |  Training loss 0.31498\n",
      "15:36:25.051344  |  Epoch 15  |  Training loss 0.23144\n",
      "15:36:32.358912  |  Epoch 20  |  Training loss 0.18008\n",
      "15:36:39.851710  |  Epoch 25  |  Training loss 0.19677\n",
      "15:36:47.477805  |  Epoch 30  |  Training loss 0.14508\n",
      "\n",
      " --- Accuracies --- \n",
      "Training\n",
      "Accuracy: 0.88\n",
      "Validation\n",
      "Accuracy: 0.80\n",
      "\n",
      " --------- Using manual update ---------- \n",
      "15:36:49.914018  |  Epoch 1  |  Training loss 0.64125\n",
      "15:36:57.391801  |  Epoch 5  |  Training loss 0.40867\n",
      "15:37:06.490094  |  Epoch 10  |  Training loss 0.31498\n",
      "15:37:16.265842  |  Epoch 15  |  Training loss 0.23144\n",
      "15:37:25.813427  |  Epoch 20  |  Training loss 0.18008\n",
      "15:37:34.834043  |  Epoch 25  |  Training loss 0.19677\n",
      "15:37:44.176163  |  Epoch 30  |  Training loss 0.14508\n",
      "\n",
      " --- Accuracies --- \n",
      "Training\n",
      "Accuracy: 0.88\n",
      "Validation\n",
      "Accuracy: 0.80\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "n_epochs = 30\n",
    "batch_size = 256\n",
    "seed = 265\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(data_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "list_lr = [0.01]*6\n",
    "list_momentum = [0, 0, 0.9, 0.9, 0.9, 0.8]\n",
    "list_decay = [0, 0.01, 0, 0.01, 0.001, 0.01]\n",
    "\n",
    "params = [{\n",
    "        \"lr\" : list_lr[i],\n",
    "        'mom' : list_momentum[i],\n",
    "        'decay' : list_decay[i],\n",
    "    } for i in range(len(list_lr))]\n",
    "\n",
    "print(\"\\n   Global parameters:\")\n",
    "print(\"batch_size = \", batch_size)\n",
    "print(\"n_epoch = \", n_epochs)\n",
    "print(\"loss_fn = \", nn.CrossEntropyLoss())\n",
    "print(\"seed = \", seed)\n",
    "\n",
    "accuracies = []\n",
    "models = []\n",
    "\n",
    "for i in range(len(list_lr)):\n",
    "    \n",
    "    print(\"\\n ========================================================= \")\n",
    "    \n",
    "    print(\"   Current parameters: \")\n",
    "    print(\"\".join(['%s = %s\\n' % (key, value) for (key, value) in params[i].items()]))\n",
    "    \n",
    "    print(\" --------- Using Pytorch's SGD --------- \")\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    model = MyNet()\n",
    "    model.to(device=device)\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(), \n",
    "        lr= params[i][\"lr\"], \n",
    "        momentum = params[i][\"mom\"], \n",
    "        weight_decay = params[i][\"decay\"]\n",
    "    )\n",
    "\n",
    "    loss_train = train(\n",
    "        n_epochs = n_epochs,\n",
    "        optimizer = optimizer, \n",
    "        model = model,\n",
    "        loss_fn = loss_fn,\n",
    "        train_loader = train_loader,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n --- Accuracies --- \")\n",
    "    print(\"Training\")\n",
    "    compute_accuracy(model, train_loader)\n",
    "    print(\"Validation\")\n",
    "    acc = compute_accuracy(model, val_loader)\n",
    "\n",
    "    print(\"\\n --------- Using manual update ---------- \")\n",
    "    torch.manual_seed(seed)\n",
    "    model = MyNet()\n",
    "    model.to(device=device) \n",
    "\n",
    "    loss_train = train_manual_update(\n",
    "        n_epochs = n_epochs,\n",
    "        model = model,\n",
    "        loss_fn = loss_fn,\n",
    "        train_loader = train_loader,\n",
    "        lr = params[i][\"lr\"],\n",
    "        momentum_coeff = params[i][\"mom\"], \n",
    "        weight_decay = params[i][\"decay\"],\n",
    "    )\n",
    "\n",
    "    print(\"\\n --- Accuracies --- \")\n",
    "    print(\"Training\")\n",
    "    compute_accuracy(model, train_loader)\n",
    "    print(\"Validation\")\n",
    "    acc = compute_accuracy(model, val_loader)\n",
    "    \n",
    "    # For model selection\n",
    "    accuracies.append(acc)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The best model was trained with \n",
      "    lr = 0.01\n",
      "    mom = 0.9\n",
      "    decay = 0.01\n",
      "Training accuracy of the best model: \n",
      "Accuracy: 0.96\n",
      "Validation accuracy of the best model: \n",
      "Accuracy: 0.84\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8392675483214649"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_best_model = np.argmax(accuracies)\n",
    "best_model = models[i_best_model]\n",
    "params_best_model = params[i_best_model]\n",
    "print(\n",
    "    \"\\nThe best model was trained with\",\n",
    "    \"\".join(['\\n    %s = %s' % (key, value) for (key, value) in params[i_best_model].items()]))\n",
    "\n",
    "print(\"Training accuracy of the best model: \")\n",
    "compute_accuracy(best_model, train_loader)\n",
    "print(\"Validation accuracy of the best model: \")\n",
    "compute_accuracy(best_model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of the best model: \n",
      "Accuracy: 0.84\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8435"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Test accuracy of the best model: \")\n",
    "compute_accuracy(best_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nglm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "71c2cb666ff353b4e7b5c350d66179fa0af5c84ce239ad9fa105d94543f3ad59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
