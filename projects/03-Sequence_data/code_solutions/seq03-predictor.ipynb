{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from attention import MultiHeadAttention\n",
    "from utils import model_selection, model_evaluation, set_device\n",
    "from cbow import create_dataset, CBoW\n",
    "from embedding_utils import similarity_matrix, find_N_closest\n",
    "\n",
    "seed = 265\n",
    "torch.manual_seed(seed)\n",
    "device = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the dataset:    2684706\n",
      "Total number of words in the dataset:    49526\n",
      "Number of distinct words kept:           1879\n"
     ]
    }
   ],
   "source": [
    "# List of words contained in the dataset\n",
    "generated_path = '../generated/'\n",
    "list_words_train = torch.load(generated_path + 'books_train.pt')\n",
    "list_words_val = torch.load(  generated_path + 'books_val.pt')\n",
    "list_words_test = torch.load( generated_path + 'books_test.pt')\n",
    "\n",
    "# vocab contains the vocabulary found in the data, associating an index to each word\n",
    "vocab = torch.load( generated_path + 'vocab.pt')\n",
    "weight = torch.load(generated_path + 'weight.pt')\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Total number of words in the dataset:   \", len(list_words_train))\n",
    "print(\"Total number of words in the dataset:   \", len(list_words_val))\n",
    "print(\"Number of distinct words kept:          \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBoW(\n",
      "  (embeddings): Embedding(1879, 16)\n",
      "  (fc1): Linear(in_features=64, out_features=1879, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_cbow = torch.load(generated_path + 'CBoW.pt')\n",
    "context_size = 4\n",
    "embedding_dim = model_cbow.embedding_dim\n",
    "print(model_cbow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123651\n",
      "2584\n",
      "4735\n"
     ]
    }
   ],
   "source": [
    "be = [\"was\", \"were\", 'be', \"is\", 'are', \"am\", \"been\", 'being']\n",
    "have = [\"have\", \"has\", 'had', 'having']\n",
    "\n",
    "white_list = be + have\n",
    "n_out = len(white_list)\n",
    "map_tokens = {vocab[w]:i for i,w in enumerate(white_list)}\n",
    "\n",
    "\n",
    "data_train = create_dataset(\n",
    "    list_words_train, vocab, context_size, white_list=white_list,\n",
    "    occ_max=np.inf, map_target=map_tokens, bidirectional=True)\n",
    "data_val = create_dataset(list_words_val, vocab, context_size, white_list=white_list,\n",
    "    occ_max=np.inf, map_target=map_tokens, bidirectional=True)\n",
    "data_test = create_dataset(list_words_test, vocab, context_size, white_list=white_list,\n",
    "    occ_max=np.inf, map_target=map_tokens, bidirectional=True)\n",
    "\n",
    "print(len(data_train))\n",
    "print(len(data_val))\n",
    "print(len(data_test))\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 1024\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "\n",
    "    def __init__(self, out_size, embedding, context_size=10):\n",
    "        super().__init__()\n",
    "\n",
    "        (vocab_size, embedding_dim) = embedding.weight.shape\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.load_state_dict(embedding.state_dict())\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.fc1 = nn.Linear(embedding_dim*context_size*2, 128)\n",
    "        self.fc2 = nn.Linear(128, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            self.emb = self.embedding(x)\n",
    "        out = F.relu(self.fc1(torch.flatten(self.emb, 1)))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "class MyRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, out_size, embedding, context_size=10, L=1, hidden_size=None):\n",
    "        super().__init__()\n",
    "\n",
    "        (vocab_size, embedding_dim) = embedding.weight.shape\n",
    "        if hidden_size is None:\n",
    "            self.hidden_size = embedding_dim*2\n",
    "        else:\n",
    "            self.hidden_size = hidden_size\n",
    "            \n",
    "        self.context_size = context_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.load_state_dict(embedding.state_dict())\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.lstm1 = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden_size, num_layers=L, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(self.hidden_size*2, 512)\n",
    "        self.fc2 = nn.Linear(512, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shape: (N, L, embedding_dim)\n",
    "        with torch.no_grad():\n",
    "            self.emb = self.embedding(x)\n",
    "        N = self.emb.shape[0]\n",
    "        out = torch.zeros((N, self.hidden_size*2)).to(device=device)\n",
    "        # LSTM outputs: (out, (h, c)) with h of shape (num_layer, N, H_out) and we want h[-1,:,:]\n",
    "        out[:, :self.hidden_size] = F.relu(self.lstm1(self.emb[:, :self.context_size])[1][0][-1])\n",
    "        out[:, self.hidden_size:] = F.relu(self.lstm2(self.emb[:, self.context_size:])[1][0][-1])\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "class MyAttentionNN(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, out_size, embedding, \n",
    "        context_size=10, h=1, p=None):\n",
    "        super().__init__()\n",
    "\n",
    "        (vocab_size, embedding_dim) = embedding.weight.shape\n",
    "        self.context_size = context_size*2\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.load_state_dict(embedding.state_dict())\n",
    "        for param in self.embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.attention = MultiHeadAttention(h, p, embedding_dim, max_len=100)\n",
    "        self.fc1 = nn.Linear(2*context_size*embedding_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shape: (N, L, embedding_dim)\n",
    "        with torch.no_grad():\n",
    "            self.emb = self.embedding(x)\n",
    "        N = self.emb.shape[0]\n",
    "        # out is of shape (N, actual_len, embedding dim)\n",
    "        out = self.attention(self.emb)\n",
    "        out = torch.flatten(out, 1)\n",
    "        # out is of shape (N, actual_len*embedding dim)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "    data_train, data_val, data_test,\n",
    "    context_size, model_class, model_params, model_name,\n",
    "    use_unk_limit=True,\n",
    "    generated_path='../generated/'\n",
    "):\n",
    "    \"\"\"\n",
    "    Warning: this function relies heavily on global variables and default parameters\n",
    "    \"\"\"\n",
    "    device = set_device()\n",
    "    \n",
    "    print(\"=\"*59)\n",
    "    print(\n",
    "        \"Context size  %d  |  use_unk_limit %s\"\n",
    "        %(context_size,  str(use_unk_limit) )\n",
    "    )\n",
    "\n",
    "    # -------------- Datasets -------------\n",
    "    torch.manual_seed(seed)\n",
    "    train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(data_val, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(data_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # ------- Loss function parameters -------\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # ---------- Optimizer parameters --------\n",
    "    list_lr = [0.001]\n",
    "    optimizers = [optim.Adam for _ in range(len(list_lr))]\n",
    "    optim_params = [{\n",
    "            \"lr\" : list_lr[i],\n",
    "        } for i in range(len(list_lr))]\n",
    "\n",
    "    # -------- Model class parameters --------\n",
    "\n",
    "    \n",
    "    # ----------- Model name -----------------\n",
    "    hyperparams = {\n",
    "        \"context\": context_size,\n",
    "        \"emb_dim\": embedding_dim,\n",
    "    }\n",
    "    model_name += \"_\".join(['%s=%s' %(k, v) for (k, v) in hyperparams.items()]) + '.pt'\n",
    "\n",
    "    # ----------- Model selection -----------\n",
    "    best_model, i_best_model = model_selection(\n",
    "        model_class, model_params, optimizers, optim_params,\n",
    "        n_epochs, loss_fn,\n",
    "        train_loader, val_loader,\n",
    "        seed=265, model_name=model_name, device=device\n",
    "    )\n",
    "\n",
    "    # ----------- Model evaluation -----------\n",
    "    test_acc = model_evaluation(best_model, train_loader, val_loader, test_loader, device=device)\n",
    "\n",
    "    # ----------- Embedding analysis -----------\n",
    "        \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device cuda.\n",
      "===========================================================\n",
      "Context size  4  |  use_unk_limit True\n",
      "   Current parameters: \n",
      "lr = 0.001\n",
      "\n",
      "On device cuda.\n",
      "18:02:50.477670  |  Epoch 1  |  Training loss 1.66432\n",
      "18:02:55.513069  |  Epoch 5  |  Training loss 1.24367\n",
      "18:03:01.769582  |  Epoch 10  |  Training loss 1.15639\n",
      "18:03:07.513124  |  Epoch 15  |  Training loss 1.11465\n",
      "18:03:13.738080  |  Epoch 20  |  Training loss 1.09111\n",
      "18:03:19.951691  |  Epoch 25  |  Training loss 1.07481\n",
      "18:03:26.134262  |  Epoch 30  |  Training loss 1.06341\n",
      "18:03:32.006061  |  Epoch 35  |  Training loss 1.05535\n",
      "18:03:38.150290  |  Epoch 40  |  Training loss 1.04812\n",
      "18:03:44.391204  |  Epoch 45  |  Training loss 1.04247\n",
      "18:03:50.613600  |  Epoch 50  |  Training loss 1.03680\n",
      "Training Accuracy:     0.6223\n",
      "Validation Accuracy:   0.5418\n",
      "Training Accuracy:     0.6223\n",
      "Validation Accuracy:   0.5418\n",
      "Test Accuracy:         0.5056\n"
     ]
    }
   ],
   "source": [
    "model_class = MyNet\n",
    "model_params = (n_out, model_cbow.embeddings, context_size)\n",
    "model_name = 'MLP_'\n",
    "\n",
    "model_MLP = pipeline(\n",
    "    data_train, data_val, data_test,\n",
    "    context_size, model_class, model_params, model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device cuda.\n",
      "===========================================================\n",
      "Context size  4  |  use_unk_limit True\n",
      "   Current parameters: \n",
      "lr = 0.001\n",
      "\n",
      "On device cuda.\n",
      "18:03:56.519836  |  Epoch 1  |  Training loss 1.64106\n",
      "18:04:03.360536  |  Epoch 5  |  Training loss 1.13505\n",
      "18:04:12.442212  |  Epoch 10  |  Training loss 1.04699\n",
      "18:04:21.453917  |  Epoch 15  |  Training loss 1.00097\n",
      "18:04:30.113286  |  Epoch 20  |  Training loss 0.96872\n",
      "18:04:39.211178  |  Epoch 25  |  Training loss 0.94214\n",
      "18:04:48.248879  |  Epoch 30  |  Training loss 0.92061\n",
      "18:04:56.839077  |  Epoch 35  |  Training loss 0.90116\n",
      "18:05:05.829705  |  Epoch 40  |  Training loss 0.88295\n",
      "18:05:14.838065  |  Epoch 45  |  Training loss 0.86607\n",
      "18:05:23.442554  |  Epoch 50  |  Training loss 0.85084\n",
      "Training Accuracy:     0.6897\n",
      "Validation Accuracy:   0.6161\n",
      "Training Accuracy:     0.6897\n",
      "Validation Accuracy:   0.6161\n",
      "Test Accuracy:         0.5614\n"
     ]
    }
   ],
   "source": [
    "model_class = MyRNN\n",
    "model_params = (n_out, model_cbow.embeddings, context_size, 1)\n",
    "model_name = 'RNN_'\n",
    "\n",
    "model_RNN = pipeline(\n",
    "    data_train, data_val, data_test,\n",
    "    context_size, model_class, model_params, model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device cuda.\n",
      "===========================================================\n",
      "Context size  4  |  use_unk_limit True\n",
      "   Current parameters: \n",
      "lr = 0.001\n",
      "\n",
      "On device cuda.\n",
      "18:05:27.850773  |  Epoch 1  |  Training loss 1.80227\n",
      "18:05:35.649431  |  Epoch 5  |  Training loss 1.39124\n",
      "18:05:45.508090  |  Epoch 10  |  Training loss 1.31015\n",
      "18:05:55.385518  |  Epoch 15  |  Training loss 1.26078\n",
      "18:06:05.364456  |  Epoch 20  |  Training loss 1.22599\n",
      "18:06:15.008399  |  Epoch 25  |  Training loss 1.20039\n",
      "18:06:24.342419  |  Epoch 30  |  Training loss 1.18202\n",
      "18:06:34.071710  |  Epoch 35  |  Training loss 1.16748\n",
      "18:06:43.453614  |  Epoch 40  |  Training loss 1.15717\n",
      "18:06:53.306697  |  Epoch 45  |  Training loss 1.14934\n",
      "18:07:03.208472  |  Epoch 50  |  Training loss 1.14253\n",
      "Training Accuracy:     0.5779\n",
      "Validation Accuracy:   0.5290\n",
      "Training Accuracy:     0.5779\n",
      "Validation Accuracy:   0.5290\n",
      "Test Accuracy:         0.4929\n"
     ]
    }
   ],
   "source": [
    "model_class = MyAttentionNN\n",
    "h=1\n",
    "p=32\n",
    "max_len=20\n",
    "\n",
    "model_params = (\n",
    "    n_out, model_cbow.embeddings, context_size,\n",
    "    h, p,\n",
    ")\n",
    "model_name = 'AttentionSingle_'\n",
    "\n",
    "model_AttentionSingle = pipeline(\n",
    "    data_train, data_val, data_test,\n",
    "    context_size, model_class, model_params, model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device cuda.\n",
      "===========================================================\n",
      "Context size  4  |  use_unk_limit True\n",
      "   Current parameters: \n",
      "lr = 0.001\n",
      "\n",
      "On device cuda.\n",
      "18:07:08.732527  |  Epoch 1  |  Training loss 1.82552\n",
      "18:07:18.284512  |  Epoch 5  |  Training loss 1.33305\n",
      "18:07:30.765595  |  Epoch 10  |  Training loss 1.24623\n",
      "18:07:42.985415  |  Epoch 15  |  Training loss 1.19817\n",
      "18:07:56.530104  |  Epoch 20  |  Training loss 1.16555\n",
      "18:08:10.430550  |  Epoch 25  |  Training loss 1.14245\n",
      "18:08:25.413769  |  Epoch 30  |  Training loss 1.12143\n",
      "18:08:39.228472  |  Epoch 35  |  Training loss 1.10688\n",
      "18:08:52.509785  |  Epoch 40  |  Training loss 1.09494\n",
      "18:09:05.756401  |  Epoch 45  |  Training loss 1.08419\n",
      "18:09:18.637869  |  Epoch 50  |  Training loss 1.07549\n",
      "Training Accuracy:     0.6041\n",
      "Validation Accuracy:   0.5430\n",
      "Training Accuracy:     0.6041\n",
      "Validation Accuracy:   0.5430\n",
      "Test Accuracy:         0.5132\n"
     ]
    }
   ],
   "source": [
    "model_class = MyAttentionNN\n",
    "h=4\n",
    "p=6\n",
    "max_len=20\n",
    "\n",
    "\n",
    "model_params = (\n",
    "    n_out, model_cbow.embeddings, context_size,\n",
    "    h, p,\n",
    ")\n",
    "model_name = 'AttentionMulti01_'\n",
    "\n",
    "model_AttentionMulti01 = pipeline(\n",
    "    data_train, data_val, data_test,\n",
    "    context_size, model_class, model_params, model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device cuda.\n",
      "===========================================================\n",
      "Context size  4  |  use_unk_limit True\n",
      "   Current parameters: \n",
      "lr = 0.001\n",
      "\n",
      "On device cuda.\n",
      "18:09:24.682991  |  Epoch 1  |  Training loss 1.67113\n",
      "18:09:35.185680  |  Epoch 5  |  Training loss 1.22359\n",
      "18:09:48.852845  |  Epoch 10  |  Training loss 1.14638\n",
      "18:10:02.569807  |  Epoch 15  |  Training loss 1.10283\n",
      "18:10:16.722914  |  Epoch 20  |  Training loss 1.07517\n",
      "18:10:31.063254  |  Epoch 25  |  Training loss 1.05488\n",
      "18:10:45.440966  |  Epoch 30  |  Training loss 1.04098\n",
      "18:10:59.559197  |  Epoch 35  |  Training loss 1.02967\n",
      "18:11:13.092930  |  Epoch 40  |  Training loss 1.02217\n",
      "18:11:26.780427  |  Epoch 45  |  Training loss 1.01308\n",
      "18:11:39.994776  |  Epoch 50  |  Training loss 1.00817\n",
      "Training Accuracy:     0.6315\n",
      "Validation Accuracy:   0.5445\n",
      "Training Accuracy:     0.6315\n",
      "Validation Accuracy:   0.5445\n",
      "Test Accuracy:         0.5016\n"
     ]
    }
   ],
   "source": [
    "model_class = MyAttentionNN\n",
    "h=4\n",
    "p=32\n",
    "max_len=20\n",
    "\n",
    "\n",
    "model_params = (\n",
    "    n_out, model_cbow.embeddings, context_size,\n",
    "    h, p,\n",
    ")\n",
    "model_name = 'AttentionMulti02_'\n",
    "\n",
    "model_AttentionMulti02 = pipeline(\n",
    "    data_train, data_val, data_test,\n",
    "    context_size, model_class, model_params, model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device cuda.\n",
      "===========================================================\n",
      "Context size  4  |  use_unk_limit True\n",
      "   Current parameters: \n",
      "lr = 0.001\n",
      "\n",
      "On device cuda.\n",
      "18:24:05.887995  |  Epoch 1  |  Training loss 1.60508\n",
      "18:24:15.567411  |  Epoch 5  |  Training loss 1.22025\n",
      "18:24:27.666381  |  Epoch 10  |  Training loss 1.14532\n",
      "18:24:39.769178  |  Epoch 15  |  Training loss 1.10464\n",
      "18:24:52.035523  |  Epoch 20  |  Training loss 1.07825\n",
      "18:25:04.700185  |  Epoch 25  |  Training loss 1.06181\n",
      "18:25:18.158391  |  Epoch 30  |  Training loss 1.05027\n",
      "18:25:32.029670  |  Epoch 35  |  Training loss 1.03938\n",
      "18:25:46.230678  |  Epoch 40  |  Training loss 1.03162\n",
      "18:25:59.695014  |  Epoch 45  |  Training loss 1.02462\n",
      "18:26:11.944832  |  Epoch 50  |  Training loss 1.02007\n",
      "Training Accuracy:     0.6264\n",
      "Validation Accuracy:   0.5402\n",
      "Training Accuracy:     0.6264\n",
      "Validation Accuracy:   0.5402\n",
      "Test Accuracy:         0.5090\n"
     ]
    }
   ],
   "source": [
    "model_class = MyAttentionNN\n",
    "h=4\n",
    "p=128\n",
    "max_len=20\n",
    "\n",
    "\n",
    "model_params = (\n",
    "    n_out, model_cbow.embeddings, context_size,\n",
    "    h, p,\n",
    ")\n",
    "model_name = 'AttentionMulti02_'\n",
    "\n",
    "model_AttentionMulti02 = pipeline(\n",
    "    data_train, data_val, data_test,\n",
    "    context_size, model_class, model_params, model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device cuda.\n",
      "===========================================================\n",
      "Context size  4  |  use_unk_limit True\n",
      "   Current parameters: \n",
      "lr = 0.001\n",
      "\n",
      "On device cuda.\n",
      "18:26:18.627220  |  Epoch 1  |  Training loss 1.65583\n",
      "18:26:31.681392  |  Epoch 5  |  Training loss 1.20988\n",
      "18:26:47.868401  |  Epoch 10  |  Training loss 1.12296\n",
      "18:27:03.797991  |  Epoch 15  |  Training loss 1.07308\n",
      "18:27:20.106841  |  Epoch 20  |  Training loss 1.04006\n",
      "18:27:36.271818  |  Epoch 25  |  Training loss 1.01482\n",
      "18:27:52.457905  |  Epoch 30  |  Training loss 0.99828\n",
      "18:28:08.518587  |  Epoch 35  |  Training loss 0.98324\n",
      "18:28:24.739358  |  Epoch 40  |  Training loss 0.97096\n",
      "18:28:40.954383  |  Epoch 45  |  Training loss 0.96157\n",
      "18:28:57.467884  |  Epoch 50  |  Training loss 0.95463\n",
      "Training Accuracy:     0.6530\n",
      "Validation Accuracy:   0.5542\n",
      "Training Accuracy:     0.6530\n",
      "Validation Accuracy:   0.5542\n",
      "Test Accuracy:         0.5191\n"
     ]
    }
   ],
   "source": [
    "model_class = MyAttentionNN\n",
    "h=10\n",
    "p=16\n",
    "max_len=20\n",
    "\n",
    "\n",
    "model_params = (\n",
    "    n_out, model_cbow.embeddings, context_size,\n",
    "    h, p,\n",
    ")\n",
    "model_name = 'AttentionMulti02_'\n",
    "\n",
    "model_AttentionMulti02 = pipeline(\n",
    "    data_train, data_val, data_test,\n",
    "    context_size, model_class, model_params, model_name\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "71c2cb666ff353b4e7b5c350d66179fa0af5c84ce239ad9fa105d94543f3ad59"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
