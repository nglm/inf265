{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import model_selection, model_evaluation, set_device\n",
    "from cbow import create_dataset, CBoW\n",
    "from embedding_utils import similarity_matrix, find_N_closest\n",
    "\n",
    "seed = 265\n",
    "torch.manual_seed(seed)\n",
    "device = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the dataset:    2684706\n",
      "Total number of words in the dataset:    49526\n",
      "Number of distinct words kept:           1879\n"
     ]
    }
   ],
   "source": [
    "# List of words contained in the dataset\n",
    "generated_path = '../generated/'\n",
    "list_words_train = torch.load(generated_path + 'books_train.pt')\n",
    "list_words_val = torch.load(  generated_path + 'books_val.pt')\n",
    "list_words_test = torch.load( generated_path + 'books_test.pt')\n",
    "\n",
    "# vocab contains the vocabulary found in the data, associating an index to each word\n",
    "vocab = torch.load( generated_path + 'vocab.pt')\n",
    "weight = torch.load(generated_path + 'weight.pt')\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Total number of words in the dataset:   \", len(list_words_train))\n",
    "print(\"Total number of words in the dataset:   \", len(list_words_val))\n",
    "print(\"Number of distinct words kept:          \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "    context_size, embedding_dim, occ_max=np.inf, use_weight=True, use_unk_limit=True,\n",
    "    black_list=[\"<unk>\", \",\", \".\", \"!\", \"?\", '\"'],\n",
    "    generated_path='../generated/'\n",
    "):\n",
    "    \"\"\"\n",
    "    Warning: this function relies heavily on global variables and default parameters\n",
    "    \"\"\"\n",
    "    device = set_device()\n",
    "    \n",
    "    print(\"=\"*59)\n",
    "    print(\n",
    "        \"Context size  %d  |  Embedding dim  %d  |  occ_max  %s  |  weights %s\"\n",
    "        %(context_size, embedding_dim, str(occ_max), str(use_weight) )\n",
    "    )\n",
    "    print(\n",
    "        \"use_unk_limit %s \" %(str(use_unk_limit))\n",
    "    )\n",
    "    print(\"Black_list: %s\" %\" | \".join(black_list))\n",
    "\n",
    "    # -------------- Datasets -------------\n",
    "    data_train_ngram = create_dataset(list_words_train, vocab, context_size, black_list=black_list, occ_max=occ_max, use_unk_limit=use_unk_limit)\n",
    "    data_val_ngram = create_dataset(list_words_val,     vocab, context_size, black_list=black_list, occ_max=occ_max, use_unk_limit=use_unk_limit)\n",
    "    data_test_ngram = create_dataset(list_words_test,   vocab, context_size, black_list=black_list, occ_max=occ_max, use_unk_limit=use_unk_limit)\n",
    "\n",
    "    print(len(data_train_ngram))\n",
    "    print(len(data_val_ngram))\n",
    "    print(len(data_test_ngram))\n",
    "\n",
    "    train_loader = DataLoader(data_train_ngram, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(data_val_ngram, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(data_test_ngram, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # ------- Loss function parameters -------\n",
    "    if use_weight:\n",
    "        loss_fn = nn.CrossEntropyLoss(weight=weight.to(device=device))\n",
    "    else:\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # ---------- Optimizer parameters --------\n",
    "    list_lr = [0.001]\n",
    "    optimizers = [optim.Adam for _ in range(len(list_lr))]\n",
    "    optim_params = [{\n",
    "            \"lr\" : list_lr[i],\n",
    "        } for i in range(len(list_lr))]\n",
    "\n",
    "    # -------- Model class parameters --------\n",
    "    model_class = CBoW\n",
    "    model_params = (vocab_size, embedding_dim, context_size)\n",
    "    \n",
    "    # ----------- Model name -----------------\n",
    "    model_name = generated_path +'CBoW_'\n",
    "    hyperparams = {\n",
    "        \"context\": context_size,\n",
    "        \"emb_dim\": embedding_dim,\n",
    "        \"weights\": use_weight,\n",
    "        \"unk_limit\": use_unk_limit,\n",
    "        \"occ_max\": occ_max, \n",
    "    }\n",
    "    model_name += \"_\".join(['%s=%s' %(k, v) for (k, v) in hyperparams.items()]) + '.pt'\n",
    "\n",
    "    # ----------- Model selection -----------\n",
    "    model_cbow, i_best_model = model_selection(\n",
    "        model_class, model_params, optimizers, optim_params,\n",
    "        n_epochs, loss_fn,\n",
    "        train_loader, val_loader,\n",
    "        seed=265, model_name=model_name, device=device\n",
    "    )\n",
    "\n",
    "    # ----------- Model evaluation -----------\n",
    "    test_acc = model_evaluation(model_cbow, train_loader, val_loader, test_loader, device=device)\n",
    "\n",
    "    # ----------- Embedding analysis -----------\n",
    "    sim, embs = similarity_matrix(vocab, model_cbow)\n",
    "    words = [\n",
    "        'the', 'table', \"man\", 'little', 'big', 'always', 'mind', 'black', 'white', 'child', 'children', \n",
    "        'yes', 'out', \"me\", \"have\", \"be\"\n",
    "    ]\n",
    "    for w in words:\n",
    "        print('-'*59)\n",
    "        find_N_closest(sim, w, vocab)\n",
    "        \n",
    "    return model_cbow, embs, sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device cuda.\n",
      "===========================================================\n",
      "Context size  2  |  Embedding dim  12  |  occ_max  inf  |  weights True\n",
      "use_unk_limit True \n",
      "Black_list: <unk>\n",
      "2189034\n",
      "40333\n",
      "94891\n",
      "   Current parameters: \n",
      "lr = 0.001\n",
      "\n",
      "On device cuda.\n",
      "12:57:54.175866  |  Epoch 1  |  Training loss 5.26034\n",
      "12:59:13.015910  |  Epoch 5  |  Training loss 4.18400\n",
      "13:00:48.817708  |  Epoch 10  |  Training loss 4.08659\n",
      "13:02:25.125085  |  Epoch 15  |  Training loss 4.05612\n",
      "13:04:00.358892  |  Epoch 20  |  Training loss 4.03864\n",
      "13:05:36.383461  |  Epoch 25  |  Training loss 4.02694\n",
      "13:07:12.073661  |  Epoch 30  |  Training loss 4.01881\n",
      "Accuracy: 0.23610\n",
      "Accuracy: 0.23423\n",
      "Training Accuracy:     0.2361\n",
      "Validation Accuracy:   0.2342\n",
      "Accuracy: 0.23610\n",
      "Accuracy: 0.23423\n",
      "Accuracy: 0.24609\n",
      "Training Accuracy:     0.2361\n",
      "Validation Accuracy:   0.2342\n",
      "Validation Accuracy:   0.2461\n",
      "On device cuda.\n",
      "-----------------------------------------------------------\n",
      "the\n",
      "0  |   similitude: 1.000000   |   the \n",
      "1  |   similitude: 0.690791   |   a \n",
      "2  |   similitude: 0.658507   |   minutes \n",
      "3  |   similitude: 0.656057   |   two \n",
      "4  |   similitude: 0.654396   |   pure \n",
      "5  |   similitude: 0.650328   |   eight \n",
      "6  |   similitude: 0.625730   |   committed \n",
      "7  |   similitude: 0.625446   |   yes \n",
      "8  |   similitude: 0.606034   |   wooden \n",
      "9  |   similitude: 0.600085   |   heads \n",
      "-----------------------------------------------------------\n",
      "table\n",
      "0  |   similitude: 1.000000   |   table \n",
      "1  |   similitude: 0.848412   |   ground \n",
      "2  |   similitude: 0.826367   |   street \n",
      "3  |   similitude: 0.791904   |   luck \n",
      "4  |   similitude: 0.770237   |   group \n",
      "5  |   similitude: 0.757306   |   sea \n",
      "6  |   similitude: 0.751541   |   entrance \n",
      "7  |   similitude: 0.749224   |   l \n",
      "8  |   similitude: 0.745805   |   cold \n",
      "9  |   similitude: 0.721604   |   frost \n",
      "-----------------------------------------------------------\n",
      "man\n",
      "0  |   similitude: 1.000000   |   man \n",
      "1  |   similitude: 0.873263   |   mouth \n",
      "2  |   similitude: 0.823959   |   train \n",
      "3  |   similitude: 0.815072   |   flesh \n",
      "4  |   similitude: 0.811659   |   misery \n",
      "5  |   similitude: 0.794106   |   lady \n",
      "6  |   similitude: 0.787223   |   nobody \n",
      "7  |   similitude: 0.779376   |   understood \n",
      "8  |   similitude: 0.777810   |   gardener \n",
      "9  |   similitude: 0.776030   |   chain \n",
      "-----------------------------------------------------------\n",
      "little\n",
      "0  |   similitude: 1.000000   |   little \n",
      "1  |   similitude: 0.854686   |   common \n",
      "2  |   similitude: 0.843053   |   bare \n",
      "3  |   similitude: 0.828530   |   whole \n",
      "4  |   similitude: 0.793308   |   skin \n",
      "5  |   similitude: 0.743866   |   married \n",
      "6  |   similitude: 0.738606   |   variety \n",
      "7  |   similitude: 0.729923   |   simple \n",
      "8  |   similitude: 0.727218   |   red \n",
      "9  |   similitude: 0.695428   |   three \n",
      "-----------------------------------------------------------\n",
      "big\n",
      "0  |   similitude: 1.000000   |   big \n",
      "1  |   similitude: 0.846355   |   real \n",
      "2  |   similitude: 0.795259   |   message \n",
      "3  |   similitude: 0.786020   |   noble \n",
      "4  |   similitude: 0.785713   |   bell \n",
      "5  |   similitude: 0.777086   |   gloomy \n",
      "6  |   similitude: 0.748929   |   month \n",
      "7  |   similitude: 0.726666   |   ill \n",
      "8  |   similitude: 0.711839   |   old \n",
      "9  |   similitude: 0.710478   |   girl \n",
      "-----------------------------------------------------------\n",
      "always\n",
      "0  |   similitude: 1.000000   |   always \n",
      "1  |   similitude: 0.772756   |   or \n",
      "2  |   similitude: 0.720178   |   thy \n",
      "3  |   similitude: 0.716549   |   none \n",
      "4  |   similitude: 0.710252   |   several \n",
      "5  |   similitude: 0.706741   |   charming \n",
      "6  |   similitude: 0.681983   |   called \n",
      "7  |   similitude: 0.677437   |   keeping \n",
      "8  |   similitude: 0.673408   |   stopped \n",
      "9  |   similitude: 0.656463   |   ideas \n",
      "-----------------------------------------------------------\n",
      "mind\n",
      "0  |   similitude: 1.000000   |   mind \n",
      "1  |   similitude: 0.868471   |   men \n",
      "2  |   similitude: 0.847312   |   faces \n",
      "3  |   similitude: 0.780154   |   pair \n",
      "4  |   similitude: 0.742695   |   de \n",
      "5  |   similitude: 0.725233   |   part \n",
      "6  |   similitude: 0.722715   |   work \n",
      "7  |   similitude: 0.718572   |   quick \n",
      "8  |   similitude: 0.718237   |   larger \n",
      "9  |   similitude: 0.704516   |   movements \n",
      "-----------------------------------------------------------\n",
      "black\n",
      "0  |   similitude: 1.000000   |   black \n",
      "1  |   similitude: 0.812554   |   small \n",
      "2  |   similitude: 0.802086   |   cold \n",
      "3  |   similitude: 0.793929   |   fifteen \n",
      "4  |   similitude: 0.775858   |   board \n",
      "5  |   similitude: 0.774329   |   thirty \n",
      "6  |   similitude: 0.763125   |   voices \n",
      "7  |   similitude: 0.747960   |   dress \n",
      "8  |   similitude: 0.735180   |   especially \n",
      "9  |   similitude: 0.729116   |   fields \n",
      "-----------------------------------------------------------\n",
      "white\n",
      "0  |   similitude: 1.000000   |   white \n",
      "1  |   similitude: 0.963061   |   officers \n",
      "2  |   similitude: 0.927937   |   highest \n",
      "3  |   similitude: 0.924304   |   dog \n",
      "4  |   similitude: 0.912673   |   regiment \n",
      "5  |   similitude: 0.898894   |   infantry \n",
      "6  |   similitude: 0.889226   |   leaves \n",
      "7  |   similitude: 0.883072   |   hearts \n",
      "8  |   similitude: 0.876458   |   nature \n",
      "9  |   similitude: 0.857721   |   breast \n",
      "-----------------------------------------------------------\n",
      "child\n",
      "0  |   similitude: 1.000000   |   child \n",
      "1  |   similitude: 0.811485   |   girl \n",
      "2  |   similitude: 0.782061   |   cloak \n",
      "3  |   similitude: 0.766625   |   colonel \n",
      "4  |   similitude: 0.745822   |   cavalry \n",
      "5  |   similitude: 0.744629   |   wound \n",
      "6  |   similitude: 0.742832   |   linen \n",
      "7  |   similitude: 0.741885   |   fact \n",
      "8  |   similitude: 0.722300   |   sword \n",
      "9  |   similitude: 0.710395   |   won \n",
      "-----------------------------------------------------------\n",
      "children\n",
      "0  |   similitude: 1.000000   |   children \n",
      "1  |   similitude: 0.850692   |   distant \n",
      "2  |   similitude: 0.835910   |   five \n",
      "3  |   similitude: 0.835466   |   fine \n",
      "4  |   similitude: 0.817985   |   growing \n",
      "5  |   similitude: 0.816023   |   absence \n",
      "6  |   similitude: 0.814838   |   others \n",
      "7  |   similitude: 0.802600   |   thence \n",
      "8  |   similitude: 0.794998   |   single \n",
      "9  |   similitude: 0.786316   |   considerable \n",
      "-----------------------------------------------------------\n",
      "yes\n",
      "0  |   similitude: 1.000000   |   yes \n",
      "1  |   similitude: 0.821067   |   whilst \n",
      "2  |   similitude: 0.819608   |   but \n",
      "3  |   similitude: 0.737480   |   unknown \n",
      "4  |   similitude: 0.733052   |   mon \n",
      "5  |   similitude: 0.730822   |   weeks \n",
      "6  |   similitude: 0.729986   |   fond \n",
      "7  |   similitude: 0.697782   |   demanded \n",
      "8  |   similitude: 0.683415   |   important \n",
      "9  |   similitude: 0.666643   |   smoke \n",
      "-----------------------------------------------------------\n",
      "out\n",
      "0  |   similitude: 1.000000   |   out \n",
      "1  |   similitude: 0.864462   |   up \n",
      "2  |   similitude: 0.804965   |   forward \n",
      "3  |   similitude: 0.803515   |   apart \n",
      "4  |   similitude: 0.801955   |   side \n",
      "5  |   similitude: 0.739122   |   to-night \n",
      "6  |   similitude: 0.734806   |   throat \n",
      "7  |   similitude: 0.725356   |   foot \n",
      "8  |   similitude: 0.722517   |   itself \n",
      "9  |   similitude: 0.715616   |   notice \n",
      "-----------------------------------------------------------\n",
      "me\n",
      "0  |   similitude: 1.000000   |   me \n",
      "1  |   similitude: 0.826648   |   society \n",
      "2  |   similitude: 0.797016   |   inches \n",
      "3  |   similitude: 0.772512   |   friendly \n",
      "4  |   similitude: 0.762514   |   nonsense \n",
      "5  |   similitude: 0.737774   |   more \n",
      "6  |   similitude: 0.730630   |   splendid \n",
      "7  |   similitude: 0.718242   |   familiar \n",
      "8  |   similitude: 0.712810   |   wooden \n",
      "9  |   similitude: 0.710633   |   race \n",
      "-----------------------------------------------------------\n",
      "have\n",
      "0  |   similitude: 1.000000   |   have \n",
      "1  |   similitude: 0.826174   |   can \n",
      "2  |   similitude: 0.819532   |   join \n",
      "3  |   similitude: 0.819181   |   bring \n",
      "4  |   similitude: 0.815195   |   really \n",
      "5  |   similitude: 0.804368   |   rest \n",
      "6  |   similitude: 0.801738   |   help \n",
      "7  |   similitude: 0.801165   |   pay \n",
      "8  |   similitude: 0.795824   |   know \n",
      "9  |   similitude: 0.793394   |   sewer \n",
      "-----------------------------------------------------------\n",
      "be\n",
      "0  |   similitude: 1.000000   |   be \n",
      "1  |   similitude: 0.898630   |   carry \n",
      "2  |   similitude: 0.879990   |   kill \n",
      "3  |   similitude: 0.875579   |   choose \n",
      "4  |   similitude: 0.875521   |   send \n",
      "5  |   similitude: 0.873253   |   get \n",
      "6  |   similitude: 0.864132   |   serve \n",
      "7  |   similitude: 0.856365   |   mean \n",
      "8  |   similitude: 0.854669   |   want \n",
      "9  |   similitude: 0.834491   |   avoid \n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "batch_size = 2048\n",
    "\n",
    "# These hyperparameters were decided after analysing the output of bigger experiments\n",
    "# ------------------------------------------------------------------------\n",
    "context_size = 2\n",
    "embedding_dim = 12\n",
    "occ_max = np.inf\n",
    "use_weight = True\n",
    "use_unk_limit=True\n",
    "black_list = [\"<unk>\"]\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "model_cbow, embs, sim = pipeline(\n",
    "    context_size, embedding_dim, \n",
    "    occ_max=occ_max, use_weight=use_weight, use_unk_limit=use_unk_limit,\n",
    "    black_list=black_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_np = embs.cpu().numpy()\n",
    "embs_df = pd.DataFrame(embs_np)\n",
    "embs_df.to_csv(generated_path + 'embeddings.tsv', sep=\"\\t\", header=False, index=False)\n",
    "words_np = np.array(vocab.lookup_tokens(range(vocab_size)))\n",
    "words_df = pd.DataFrame(words_np)\n",
    "words_df.to_csv(generated_path + 'vocabulary.tsv', sep=\"\\t\", header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "71c2cb666ff353b4e7b5c350d66179fa0af5c84ce239ad9fa105d94543f3ad59"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
