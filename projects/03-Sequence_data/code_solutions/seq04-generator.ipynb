{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import numpy as np\n",
    "\n",
    "from cbow import CBoW, create_dataset\n",
    "from utils import model_selection, model_evaluation, set_device\n",
    "\n",
    "seed = 265\n",
    "torch.manual_seed(seed)\n",
    "device = set_device()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the dataset:    2684706\n",
      "Total number of words in the dataset:    49526\n",
      "Number of distinct words kept:           1879\n",
      "CBoW(\n",
      "  (embeddings): Embedding(1879, 16)\n",
      "  (fc1): Linear(in_features=64, out_features=1879, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# List of words contained in the dataset\n",
    "generated_path = '../generated/'\n",
    "list_words_train = torch.load(generated_path + 'books_train.pt')\n",
    "list_words_val = torch.load(  generated_path + 'books_val.pt')\n",
    "list_words_test = torch.load( generated_path + 'books_test.pt')\n",
    "\n",
    "# vocab contains the vocabulary found in the data, associating an index to each word\n",
    "vocab = torch.load( generated_path + 'vocab.pt')\n",
    "weight = torch.load(generated_path + 'weight.pt')\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Total number of words in the dataset:   \", len(list_words_train))\n",
    "print(\"Total number of words in the dataset:   \", len(list_words_val))\n",
    "print(\"Number of distinct words kept:          \", vocab_size)\n",
    "\n",
    "# tokenizer will split a long text into a list of english words\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "model_cbow = torch.load(generated_path + 'CBoW.pt')\n",
    "context_size = 4\n",
    "embedding_dim = model_cbow.embedding_dim\n",
    "print(model_cbow)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_candidates(batch, model, k, device=None):\n",
    "    \"\"\"\n",
    "    Find top k candidates and log probabilities for each sequences in a batch\n",
    "\n",
    "    `batch` of shape (N, seq_len)\n",
    "    `prob` and `cand` of shape (N, k)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = set_device()\n",
    "    with torch.no_grad():\n",
    "        # Predict word probabilities `out` using model\n",
    "        # Shape (N, n_out)\n",
    "        batch = batch.to(device=device)\n",
    "        outputs = model(batch)\n",
    "        # Get log probabilities\n",
    "        log_probs = F.log_softmax(outputs, dim=-1)\n",
    "        # Find top k candidates and probabilities for each sequence individually\n",
    "        # prob and cand of shape (N, k)\n",
    "        (prob, cand) = torch.topk(log_probs, k, dim=1, largest=True, sorted=True)\n",
    "    return prob, cand\n",
    "\n",
    "def find_top_ij(probs):\n",
    "    \"\"\"\n",
    "    Find (i,j) indices of global top k candidates.\n",
    "\n",
    "    Assume that probs[i, :] are sorted for each i (see top_k_candidates)\n",
    "    `top_ij` integer tensor of shape (k, 2)\n",
    "    \"\"\"\n",
    "    device = probs.device\n",
    "    (N, k) = probs.shape\n",
    "    # For each sequence, index of candidate currently considered\n",
    "    # Shape (N)\n",
    "    curr_ind = torch.zeros(N, dtype=int).to(device=device)\n",
    "    # For each sequence candidate currently considered\n",
    "    # Shape (N)\n",
    "    curr_cand = probs[:, 0]\n",
    "    top_ij = torch.zeros((k, 2), dtype=int).to(device=device)\n",
    "    for k_curr in range(k):\n",
    "        # Find best among considered candidates\n",
    "        i_best = torch.argmax(curr_cand)\n",
    "        j_best = curr_ind[i_best]\n",
    "        top_ij[k_curr, :] = torch.tensor([i_best, j_best])\n",
    "\n",
    "        # Update currently considered candidates\n",
    "        curr_ind[i_best] += 1\n",
    "        curr_cand[i_best] = probs[i_best, j_best+1]\n",
    "    return top_ij\n",
    "\n",
    "def global_top_k_candidates(probs, cands, prev_seqs, prev_probs):\n",
    "    \"\"\"\n",
    "    Find global top k candidates.\n",
    "\n",
    "    Assume that probs[i, :] are sorted for each i\n",
    "    `probs` and `cands` of shape (N, k) (with N=1 or k)\n",
    "    `seqs` of shape (N, seq_len)        (with N=1 or k)\n",
    "    `kept_probs` of shape (k)\n",
    "    kept_seqs of shape (k, seq_len+1)\n",
    "    \"\"\"\n",
    "    device = probs.device\n",
    "    (N, k) = probs.shape\n",
    "    (N, seq_len) = prev_seqs.shape\n",
    "\n",
    "    kept_seqs = torch.zeros((k, seq_len+1), dtype=int).to(device=device)\n",
    "    kept_probs = torch.zeros(k).to(device=device)\n",
    "\n",
    "    # indices of global top_k candidates\n",
    "    top_ij = find_top_ij(probs)\n",
    "    for k_curr in range(k):\n",
    "        i, j = top_ij[k_curr]\n",
    "\n",
    "        # Update kept sequences\n",
    "        kept_seqs[k_curr, :seq_len] = prev_seqs[i]\n",
    "        kept_seqs[k_curr, -1] = cands[i, j]\n",
    "\n",
    "        # Update log probabilities\n",
    "        kept_probs[k_curr] = prev_probs[i] + probs[i, j]\n",
    "    return kept_probs, kept_seqs\n",
    "\n",
    "def beam_search(model, seq, n_preds=5, k=3):\n",
    "    \"\"\"\n",
    "    Return the `n_preds` next word after `seq` according to the beam search algo\n",
    "\n",
    "    `seq` of shape(1, seq_len)\n",
    "    `completion` of shape (1, seq_len+k)\n",
    "    `prob_completion` being a float\n",
    "    \"\"\"\n",
    "    device = seq.device\n",
    "    # Initialisation with N=1\n",
    "    start_seq_len = seq.shape[-1]\n",
    "    # Find top k candidates and log prob for the initial sequence\n",
    "    # prob and cand of shape (1, k)\n",
    "    prob, cand = top_k_candidates(seq, model, k, device=device)\n",
    "\n",
    "    # kept_seqs of shape (k, seq_len) (with seq_len being incremented)\n",
    "    kept_seqs = torch.zeros((k, start_seq_len+1), dtype=int)\n",
    "    kept_seqs[:, :start_seq_len] = start_seq_len\n",
    "    kept_seqs[:, -1] = cand\n",
    "    \n",
    "    # kept_probs of shape (k) (won't change)\n",
    "    kept_probs = prob.squeeze()\n",
    "\n",
    "    for i in range(n_preds):\n",
    "        # Find top k candidates and log prob for each sequences in batch\n",
    "        # probs and cands of shape (N, k) with N=k\n",
    "        probs, cands = top_k_candidates(kept_seqs, model, k, device=device)\n",
    "\n",
    "        # Keep only the global k top candidates and log probs\n",
    "        kept_probs, kept_seqs = global_top_k_candidates(\n",
    "            probs, cands, kept_seqs, kept_probs\n",
    "        )\n",
    "\n",
    "    # Keep only the best completion among the top k\n",
    "    completion = kept_seqs[0, start_seq_len:]\n",
    "    prob_completion = kept_probs[0].item()\n",
    "    return completion, prob_completion\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_generator(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding, L=1, hidden_size=None):\n",
    "        super().__init__()\n",
    "\n",
    "        (vocab_size, embedding_dim) = embedding.weight.shape\n",
    "        if hidden_size is None:\n",
    "            self.hidden_size = embedding_dim*2\n",
    "        else:\n",
    "            self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.load_state_dict(embedding.state_dict())\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "        self.lstm1 = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden_size, num_layers=L, batch_first=True)\n",
    "        self.fc1 = nn.Linear(self.hidden_size, vocab_size)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shape: (N, L, embedding_dim)\n",
    "        self.emb = self.embedding(x)\n",
    "        # LSTM outputs: (out, (h, c)) with h of shape (num_layer, N, H_out) and we want h[-1,:,:]\n",
    "        out = F.relu(self.lstm1(self.emb)[1][0][-1])\n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1888297\n",
      "35124\n",
      "80751\n"
     ]
    }
   ],
   "source": [
    "context_size = 6\n",
    "n_beam = 3\n",
    "data_train_gen = create_dataset(list_words_train, vocab, context_size, bidirectional=False, occ_max=np.inf)\n",
    "data_val_gen = create_dataset(list_words_val, vocab, context_size, bidirectional=False, occ_max=np.inf)\n",
    "data_test_gen = create_dataset(list_words_test, vocab, context_size, bidirectional=False, occ_max=np.inf)\n",
    "\n",
    "print(len(data_train_gen))\n",
    "print(len(data_val_gen))\n",
    "print(len(data_test_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Current parameters: \n",
      "lr = 0.001\n",
      "\n",
      "On device cuda.\n",
      "17:04:15.073012  |  Epoch 1  |  Training loss 5.41481\n",
      "17:05:44.560565  |  Epoch 5  |  Training loss 4.78494\n",
      "Training Accuracy:     0.1585\n",
      "Validation Accuracy:   0.1550\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 1024\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(data_train_gen, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(data_val_gen, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(data_test_gen, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "list_lr = [0.001]\n",
    "optimizers = [optim.Adam for _ in range(len(list_lr))]\n",
    "optim_params = [{\n",
    "        \"lr\" : list_lr[i],\n",
    "    } for i in range(len(list_lr))]\n",
    "\n",
    "\n",
    "model_class = RNN_generator\n",
    "model_params = (model_cbow.embeddings, 1)\n",
    "model_name = 'generator_'\n",
    "\n",
    "model_generator, i_best_model = model_selection(\n",
    "    model_class, model_params, optimizers, optim_params,\n",
    "    n_epochs, loss_fn,\n",
    "    train_loader, val_loader,\n",
    "    seed=265, model_name=model_name, device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:     0.1585\n",
      "Validation Accuracy:   0.1550\n",
      "Test Accuracy:         0.1789\n"
     ]
    }
   ],
   "source": [
    "test_acc = model_evaluation(model_generator, train_loader, val_loader, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-35.525 | Did you know that I am a very as he could not see him in his eyes to the \n",
      "\n",
      "-33.735 | Sometimes, I wish I were that he said to me that she had been of the \n",
      "\n",
      "-36.171 | Do you think that that the king was not in a man who had been \n",
      "\n",
      "-33.664 | I am not angry, I just think it is strange that in his head in his eyes in his head in the \n",
      "\n",
      "-36.414 | What would happen if he had not been to be a great and in the \n",
      "\n",
      "-32.965 | I am not sure, but perhaps I in his head in his head in his head in the \n",
      "\n",
      "-34.930 | I am so happy that i am to go in his eyes to his head of the \n",
      "\n",
      "-33.136 | The most important thing in life is to say to him in his head in his head of the \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict(model, vocab, sentences, n_preds, k):\n",
    "    \"\"\"\n",
    "    Complete a given sequence using the beam search algorithm\n",
    "\n",
    "    sentences are list strings, each element representing a sequence\n",
    "    \"\"\"\n",
    "    for s in sentences:\n",
    "        # From a long string to a list of words\n",
    "        start_seq = tokenizer(s)\n",
    "        # After that start_seq is a int tensor of shape (1, seq_len)\n",
    "        start_seq = torch.tensor([vocab[w] for w in start_seq]).unsqueeze(0)\n",
    "        start_seq = start_seq.to(device=device)\n",
    "\n",
    "        # Find most likely completion according to beam search\n",
    "        completion, prob_completion = beam_search( model, start_seq, n_preds, k)\n",
    "\n",
    "        # From tensor of integer to long string\n",
    "        completion =\" \".join(vocab.lookup_tokens(list(completion.squeeze())))\n",
    "        print(\"{:.3f} | {} {} \\n\".format(prob_completion, s, completion))\n",
    "\n",
    "sentences = [\n",
    "    \"Did you know that I am a very\", \"Sometimes, I wish I were\", \"Do you think that\", \n",
    "    \"I am not angry, I just think it is strange that\", \"What would happen if\", \"I am not sure, but perhaps I\",\n",
    "    \"I am so happy that i\", \"The most important thing in life is to\",\n",
    "]\n",
    "\n",
    "n_preds = 10\n",
    "k = 20\n",
    "predict(model_generator, vocab, sentences, n_preds, k)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "71c2cb666ff353b4e7b5c350d66179fa0af5c84ce239ad9fa105d94543f3ad59"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
