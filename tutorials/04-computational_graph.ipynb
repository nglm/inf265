{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 Computational graph\n",
    "\n",
    "This tutorial is not related to the \"[Deep Learning with PyTorch](https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf) book.\n",
    "\n",
    "## Contents\n",
    "\n",
    "\n",
    "1. MiniNet  \n",
    "2. PyTorch's computational graph\n",
    "    1. Weight and gradient values \n",
    "    2. Updating weights\n",
    "    3. Updating gradients\n",
    "3. Good to know\n",
    "\n",
    "  \n",
    "## Some of Andrew's videos related to this topic\n",
    "\n",
    "- [Computation Graph (C1W2L07)](https://www.youtube.com/watch?v=hCP1vGoCdYU&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=13)\n",
    "- [Derivatives With Computation Graphs (C1W2L08)](https://www.youtube.com/watch?v=nJyUyKN-XBQ&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=14)\n",
    "- [Deep L-Layer Neural Network (C1W4L01)](https://www.youtube.com/watch?v=2gw5tE2ziqA&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=36) (to clarify notations)\n",
    "- [Forward Propagation in a Deep Network (C1W4L02)](https://www.youtube.com/watch?v=a8i2eJin0lY&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=39) (clarify notations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd7e344b7d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MiniNet\n",
    "\n",
    "Here is the definition of a very simple MLP that we will use this exercise. It corresponds to the following network (following Andrew's notations from [Deep L-Layer Neural Network (C1W4L01)](https://www.youtube.com/watch?v=2gw5tE2ziqA&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=36) and [Forward Propagation in a Deep Network (C1W4L02)](https://www.youtube.com/watch?v=a8i2eJin0lY&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=39)):\n",
    "\n",
    "![MiniNet architecture](MiniNet.png)\n",
    "\n",
    "We will use the following implementation of MiniNet, it might then be useful to read the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        \n",
    "        # number of layers in our network (following Andrew's notations)\n",
    "        self.L = 2\n",
    "        \n",
    "        # Where we will store our neuron values\n",
    "        # - z: before activation function \n",
    "        # - a: after activation function (a=f(z))\n",
    "        self.z = {i : None for i in range(1, self.L+1)}\n",
    "        self.a = {i : None for i in range(self.L+1)}\n",
    "\n",
    "        # Fully connected layers\n",
    "        # We have to use nn.ModuleDict and to use strings as keys here to \n",
    "        # respect pytorch requirements (otherwise, the model does not learn)\n",
    "        self.fc = nn.ModuleDict({str(i): None for i in range(1, self.L+1)})\n",
    "        self.fc['1'] = nn.Linear(in_features=2, out_features=3)\n",
    "        self.fc['2'] = nn.Linear(in_features=3, out_features=2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # The first dimension of the input must be the batch size\n",
    "        out = torch.flatten(x, 1)\n",
    "\n",
    "        # Input layer\n",
    "        self.a[0] = out\n",
    "        \n",
    "        # First layer (hidden layer)\n",
    "        self.z[1] = self.fc['1'](out)\n",
    "        self.a[1] = torch.tanh(self.z[1])\n",
    "        \n",
    "        # Second layer (output layer)\n",
    "        self.z[2] = self.fc['2'](self.a[1])\n",
    "        self.a[2] = torch.tanh(self.z[2])\n",
    "\n",
    "        return self.a[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch's computational graph\n",
    "\n",
    "### 2.1 Weight and gradient values \n",
    "\n",
    "- In general, we can access trainable parameter values using ``model.layer_name.weight.data``\n",
    "- In general, we can access trainable parameter gradients using ``model.layer_name.weight.grad``\n",
    "- With our MiniNet, we access neuron values using ``model.a`` and ``model.z``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " =================== Print parameters =================== \n",
      "\n",
      "Name :  fc.1.weight \n",
      "Value:  tensor([[-0.2883,  0.0234],\n",
      "        [-0.3512,  0.2667],\n",
      "        [-0.6025,  0.5183]])\n",
      "\n",
      "Name :  fc.1.bias \n",
      "Value:  tensor([-0.5140, -0.5622, -0.4468])\n",
      "\n",
      "Name :  fc.2.weight \n",
      "Value:  tensor([[ 0.2615, -0.2133,  0.2161],\n",
      "        [-0.4900, -0.3503, -0.2120]])\n",
      "\n",
      "Name :  fc.2.bias \n",
      "Value:  tensor([-0.1135, -0.4404])\n",
      "\n",
      " ======== Access parameter values and gradients ========== \n",
      "\n",
      " -- Layer 'model.fc['1']':\n",
      "\n",
      " Linear(in_features=2, out_features=3, bias=True)\n",
      "\n",
      " -- Trainable parameter 'model.fc['1'].weight':\n",
      "\n",
      " Parameter containing:\n",
      "tensor([[-0.2883,  0.0234],\n",
      "        [-0.3512,  0.2667],\n",
      "        [-0.6025,  0.5183]], requires_grad=True)\n",
      "\n",
      " -- Parameter values 'model.fc['1'].weight.data':\n",
      "\n",
      " tensor([[-0.2883,  0.0234],\n",
      "        [-0.3512,  0.2667],\n",
      "        [-0.6025,  0.5183]])\n",
      "\n",
      " -- Gradient values 'model.fc['1'].weight.grad':\n",
      "\n",
      " None\n",
      "\n",
      " ========= Neuron values at initialization  ============== \n",
      "\n",
      " -------------- Input ---------------- \n",
      "a0:             None\n",
      "\n",
      " -------------- First Layer ---------------- \n",
      "z1:             None\n",
      "a1 = tanh(z1):  None\n",
      "\n",
      " --------------  2nd Layer  ---------------- \n",
      "z2:             None\n",
      "a2 = tanh(z2):  None\n",
      "\n",
      " ========= Neuron values after first input  ============== \n",
      "\n",
      " -------------- Input ---------------- \n",
      "a0:             tensor([[1., 1.]])\n",
      "\n",
      " -------------- First Layer ---------------- \n",
      "z1:             tensor([[0.9788, 0.4081, 0.5491]], grad_fn=<AddmmBackward0>)\n",
      "a1 = tanh(z1):  tensor([[0.7525, 0.3868, 0.4998]], grad_fn=<TanhBackward0>)\n",
      "\n",
      " --------------  2nd Layer  ---------------- \n",
      "z2:             tensor([[0.4484, 0.1656]], grad_fn=<AddmmBackward0>)\n",
      "a2 = tanh(z2):  tensor([[0.4206, 0.1641]], grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = MiniNet()\n",
    "\n",
    "def print_parameters(model):\n",
    "    \"\"\"\n",
    "    Print trainable parameters of our MiniNet\n",
    "    \"\"\"\n",
    "    for name, p in model.named_parameters():\n",
    "        print(\"\\nName : \", name, \"\\nValue: \", p.data)\n",
    "\n",
    "def print_neuron_values(model):\n",
    "    \"\"\"\n",
    "    Print neuron values (a and z) of our MiniNet \n",
    "    \"\"\"\n",
    "    print(\"\\n -------------- Input ---------------- \")\n",
    "    print(\"a0:            \", model.a[0] )\n",
    "    print(\"\\n -------------- First Layer ---------------- \")\n",
    "    print(\"z1:            \", model.z[1] )\n",
    "    print(\"a1 = tanh(z1): \", model.a[1] )\n",
    "    print(\"\\n --------------  2nd Layer  ---------------- \")\n",
    "    print(\"z2:            \", model.z[2] )\n",
    "    print(\"a2 = tanh(z2): \", model.a[2] )\n",
    "\n",
    "print(\" =================== Print parameters =================== \")\n",
    "print_parameters(model) # We can see that all parameters are randomly initialized\n",
    "print(\"\\n ======== Access parameter values and gradients ========== \")\n",
    "# We can access our layers using their name\n",
    "print(\"\\n -- Layer 'model.fc['1']':\\n\\n\", model.fc['1'])\n",
    "# As well as its corresponding trainable parameter\n",
    "print(\"\\n -- Trainable parameter 'model.fc['1'].weight':\\n\\n\", model.fc['1'].weight)\n",
    "# With its corresponding trainable parameter values\n",
    "print(\"\\n -- Parameter values 'model.fc['1'].weight.data':\\n\\n\",model.fc['1'].weight.data)\n",
    "# And its corresponding trainable parameter gradient\n",
    "# Note that the gradient is None for now because we haven't called .backward() yet\n",
    "print(\"\\n -- Gradient values 'model.fc['1'].weight.grad':\\n\\n\",model.fc['1'].weight.grad)\n",
    "\n",
    "\n",
    "print(\"\\n ========= Neuron values at initialization  ============== \")\n",
    "# We have not given any input to our model yet, so all neuron values should be None\n",
    "model = MiniNet()\n",
    "print_neuron_values(model)\n",
    "\n",
    "print(\"\\n ========= Neuron values after first input  ============== \")\n",
    "# Now we give some input...\n",
    "input = torch.tensor([[1, 1]], dtype=torch.float)\n",
    "output = model(input)\n",
    "# ... and everything has been computed in the forward pass \n",
    "print_neuron_values(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Updating weights\n",
    "\n",
    "If you take a closer look at the above output of \"Trainable parameter 'model.fc['1'].weight'\" you can see that it mentions \"``requires_grad=True``\". Then, if you take a closer look at the output of \"Neuron values after first input\" you see that it mentions \"``grad_fn=<TanhBackward>``\"  \"``grad_fn=<AddmmBackward>``\"\n",
    "\n",
    "What is this all about? Well, it has to do with the *computational graph* which is how Pytorch manages all the operations made during the forward pass (``outputs = model(inputs)`` i.e ``forward`` method) so that it can compute all the gradients in the backward pass (``loss.backward()``) and finally update parameters accordingly when calling ``optimizer.step()``. \n",
    "\n",
    "Now to illustrate how necessary it is to have some understanding of this computational graph, let's try to initialize our weights in a custom way and check how easily we can mess up everything.\n",
    "\n",
    "This is okay:\n",
    "\n",
    "- ``model.layer.param.data = new_values``\n",
    "- ``model.layer.param.data[:] = new_values``\n",
    "\n",
    "This is **NOT** okay:\n",
    "\n",
    "- ``model.layer.param = new_values``     Raises an error (unless ``new_values`` are of [nn.Parameter](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter) and not [torch.Tensor](https://pytorch.org/docs/stable/tensors.html?highlight=tensor#torch.Tensor)): ``TypeError: cannot assign 'torch.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)``\n",
    "- ``model.layer.param[:] = new_values``  Will remove the parameter from the list of leaves and put ``CopySlices`` as gradient function\n",
    "\n",
    "Basically, we want each of our trainable parameters (weights) to require grad ([requires_grad](https://pytorch.org/docs/stable/autograd.html?highlight=requires#torch.Tensor.requires_grad)) and to be a leaf ([is_leaf](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.is_leaf)). Variables that have nothing to do with the computational graph (i.e. that are not a part of the network) should be detached of the computational graph. (see [detach](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach)). \n",
    "\n",
    "The concept of leaf might be counter intuitive in PyTorch. The fact that your weight is in the middle of your network does not mean that it should not be a leaf. It should always be a leaf. In Pytorch, weights are leaves because in the forward pass their values do not depend on the values of the input. Their values only change when calling ``optimizer.step()`` or when you initialize them manually. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ====================== Initialization ====================== \n",
      "\n",
      "All parameters seem correctly attached to the computational graph! :) \n",
      "\n",
      " ==================== Updated parameters ==================== \n",
      "Parameter containing:\n",
      "tensor([[42.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([42.,  1.], requires_grad=True)\n",
      "\n",
      "All parameters seem correctly attached to the computational graph! :) \n",
      "\n",
      "Name :  fc.1.weight \n",
      "Value:  tensor([[42.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.]])\n",
      "\n",
      "Name :  fc.1.bias \n",
      "Value:  tensor([-0.1320, -0.3793, -0.0643])\n",
      "\n",
      "Name :  fc.2.weight \n",
      "Value:  tensor([[ 0.5470, -0.0455,  0.0183],\n",
      "        [-0.0900,  0.0908,  0.5144]])\n",
      "\n",
      "Name :  fc.2.bias \n",
      "Value:  tensor([42.,  1.])\n",
      "\n",
      " ========== Updated parameters WITHOUT '.data' =============\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a view of a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_258612/2613202218.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mprint_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# To check that parameters have been updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n ========== Updated parameters WITHOUT '.data' =============\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mprint_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# To check that parameters have been updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mcheck_computational_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# Now fc['1'].weight is not a leaf anymore! (and see \"grad_fn=<CopySlices>\"\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a view of a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "def check_computational_graph(model):\n",
    "    \"\"\"\n",
    "    Make sure all trainable parameters require grad and are leaves\n",
    "    \"\"\"\n",
    "    res = True\n",
    "    # Go through all layers\n",
    "    for i_layer in range(1, model.L+1):\n",
    "        # Each layer has a weight and bias parameter\n",
    "        for param_name in ['weight', 'bias']:\n",
    "            \n",
    "            # 'getattr(object, string variable)' is like `object.myattribute` when variable = \"myattribute\"\n",
    "            param =  getattr(model.fc[str(i_layer)], param_name)\n",
    "            msg = \" !!!! WARNING !!!!\\nmodel.fc[\" + str(i_layer) + \"].\" + param_name\n",
    "            if not param.requires_grad:\n",
    "                print(msg + \" does not require grad!\")\n",
    "                print(param)\n",
    "                res = False\n",
    "            if not param.is_leaf:\n",
    "                print(msg + \" is not a leaf!\")\n",
    "                print(param)\n",
    "                res = False\n",
    "    if res:\n",
    "        print(\"\\nAll parameters seem correctly attached to the computational graph! :) \")\n",
    "\n",
    "\n",
    "print(\"\\n ====================== Initialization ====================== \")\n",
    "\n",
    "model = MiniNet()\n",
    "check_computational_graph(model)      # So far so good, since we have not done anything yet\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n ==================== Updated parameters ==================== \")\n",
    "\n",
    "# We can update weight values using '.data'\n",
    "model.fc['1'].weight.data = torch.ones(3,2)\n",
    "model.fc['1'].weight.data[0,0] = 42\n",
    "\n",
    "model.fc['2'].bias.data[:] = torch.ones(2)\n",
    "model.fc['2'].bias.data[0] = 42\n",
    "\n",
    "print(model.fc['1'].weight)\n",
    "print(model.fc['2'].bias)\n",
    "\n",
    "check_computational_graph(model)      # To check that there are still correctly attached to the graph\n",
    "\n",
    "print_parameters(model)               # To check that parameters have been updated\n",
    "print(\"\\n ========== Updated parameters WITHOUT '.data' =============\" )\n",
    "model.fc['1'].weight[:,:] = torch.zeros_like(model.fc['1'].weight)\n",
    "print_parameters(model)               # To check that parameters have been updated\n",
    "check_computational_graph(model)      # Now fc['1'].weight is not a leaf anymore! (and see \"grad_fn=<CopySlices>\"\")\n",
    "\n",
    "# This would raise an error if uncommented\n",
    "#model.fc['1'].weight=torch.arange(1,7, dtype=torch.float).view(3,2)/10 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Updating gradients\n",
    "\n",
    "By default, Pytorch keeps track of all the operations you make that involve a tensor that requires grad (i.e. with ``requires_grad=True``) in order to correctly update trainable parameters at the next training step. \n",
    "\n",
    "This is \"contagious\". For example if you define ``a`` using your model output and then ``b`` using ``a`` then both ``a`` and ``b`` will require grad because your ouput does (because your weights do...).\n",
    "\n",
    "So whenever you manipulate your model or weights or outputs or losses outside the training loop you should always put your operations inside a ``with torch.no_grad():`` context (see [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html?highlight=no_grad#torch.no_grad)) so that pytorch does not pollute your computational graph with unwanted operations. Note that this is what we did in the ``compute_accuracy`` function in the previous tutorials\n",
    "\n",
    "In addition, pytorch documentation highly advise again using in-place operations (see [in-place operations with autograd](https://pytorch.org/docs/stable/notes/autograd.html#in-place-operations-with-autograd)). So even if you like to write ``a += b``, try to stick to ``a = a + b``. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniNet()\n",
    "\n",
    "print(\"\\n ======== Gradient values 'model.fc['1'].weight.grad' ======== \")\n",
    "\n",
    "# Initially there is no gradient\n",
    "print(\"\\n ------------- At initialization -------------- \")\n",
    "print(model.fc['1'].weight.grad)\n",
    "\n",
    "# Now we will do the forward and backward pass\n",
    "print(\"\\n --- After first input and backpropagation ---- \")\n",
    "\n",
    "# First, forward pass\n",
    "x = torch.tensor([[1., 1.]])\n",
    "y = model(x)\n",
    "\n",
    "# ... then compute some loss\n",
    "y_exp = torch.tensor([[0., 1.]])\n",
    "loss = torch.sum( (y - y_exp)**2 )\n",
    "\n",
    "# ... and backprogagate the gradient\n",
    "loss.backward()\n",
    "print(model.fc['1'].weight.grad)\n",
    "\n",
    "# Remember that after each training iteration we zero out the gradients\n",
    "print(\"\\n ------ After zeroing out the gradients ------- \")\n",
    "model.zero_grad()\n",
    "print(model.fc['1'].weight.grad)\n",
    "\n",
    "print(\"\\n ============ Which tensors require grad =============== \")\n",
    "\n",
    "print(\"Does 'x' require grad?                  \", x.requires_grad)\n",
    "print(\"Does 'model.fc['1'].weight' require grad?   \", model.fc['1'].weight.requires_grad)\n",
    "print(\"Does 'y' require grad?                  \", y.requires_grad)\n",
    "print(\"Does 'loss' require grad?               \", loss.requires_grad)\n",
    "\n",
    "a = torch.zeros_like(model.fc['1'].weight.grad)\n",
    "a = model.fc['1'].weight.data\n",
    "print(\"'a = model.fc['1'].weight.data' requires grad?   \", a.requires_grad)\n",
    "a = model.fc['1'].weight.grad\n",
    "print(\"'a = model.fc['1'].weight.grad' requires grad?   \", a.requires_grad)\n",
    "a = model.fc['1'].weight\n",
    "print(\"'a = model.fc['1'].weight' requires grad?        \", a.requires_grad)\n",
    "\n",
    "\n",
    "print(\"\\n ======== Operations irrelevant to the training ======== \")\n",
    "print(\"\\n ------ Without torch.no_grad ------- \")\n",
    "a = torch.cos(torch.tensor([[1., 1.]]))\n",
    "print(\"Does 'a' require grad?    \", a.requires_grad)\n",
    "a = 10 + y\n",
    "# Now a requires grad (and pytorch will automatically compute its gradient)\n",
    "print(\"Does 'a' require grad?    \", a.requires_grad)\n",
    "\n",
    "print(\"\\n ------ Using torch.no_grad ------- \")\n",
    "a = torch.cos(torch.tensor([[1., 1.]]))\n",
    "print(\"Does 'a' require grad?    \", a.requires_grad)\n",
    "with torch.no_grad():\n",
    "    a = 10 + y\n",
    "# a doesn't require grad this time \n",
    "print(\"Does 'a' require grad?    \", a.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Good to know\n",
    "\n",
    "- We access trainable parameter values using ``model.layer_name.weight.data``.\n",
    "- We access trainable parameter gradients using ``model.layer_name.weight.grad``.\n",
    "- ``model.layer_name.weight`` is not a regular [torch.Tensor](https://pytorch.org/docs/stable/tensors.html?highlight=tensor#torch.Tensor) but a [nn.Parameter](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter). They also have  ``requires_grad = True``.\n",
    "- By default, pytorch keeps track of all operations that involve at least one tensor that has ``requires_grad = True``.\n",
    "- [requires_grad](https://pytorch.org/docs/stable/autograd.html?highlight=requires#torch.Tensor.requires_grad) is contagious: if ``a`` requires grad, then ``b`` defined as ``b = a + 1`` requires grad as well.\n",
    "- Use ``with torch.no_grad():`` context whenever you manipulate your model or weights or outputs or losses outside the training loop.\n",
    "- Don't use in-place operations (e.g. don't use ``a += b``, but ``a = a + b`` instead)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "d7334498cbea74be2f983349dd0c062cc89e10cb2d32c736100e0abee6e40bc2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
