{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Define a custom deep Neural Network in Pytorch\n",
    "\n",
    "These tutorials are inspired by the book \"[Deep Learning with PyTorch](https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf)\" by Stevens et al and can be seen as a summary of the part I of the book regarding PyTorch itself. Normally, following the tutorials should be enough and reading the book is not required.\n",
    "\n",
    "## Contents \n",
    "\n",
    "1. .Loading data, training and measuring accuracy (see previous tutorial)  \n",
    "2. Define a simple custom neural network  \n",
    "    1. Naive (but totally ok) method  \n",
    "    3. Using the functional API  \n",
    "    4. Train our custom network (as any other model)  \n",
    "    5. Measuring accuracy (as any other model)  \n",
    "3. Going deeper: defining blocks of layers  \n",
    "    1. Using nn.Sequential  \n",
    "    2. Using a subclass of nn.Module  \n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f51a1a33770>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "from datetime import datetime\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data, training and measuring accuracy (see previous tutorial)\n",
    "\n",
    "#### Loading CIFAR-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Size of the train dataset:         45000\n",
      "Size of the validation dataset:    5000\n",
      "Size of the test dataset:          10000\n",
      "Size of the training dataset:  9017\n",
      "Size of the validation dataset:  983\n",
      "Size of the test dataset:  2000\n"
     ]
    }
   ],
   "source": [
    "def load_cifar(train_val_split=0.9, data_path='../data/', preprocessor=None):\n",
    "    \n",
    "    # Define preprocessor if not already given\n",
    "    if preprocessor is None:\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                                (0.2470, 0.2435, 0.2616))\n",
    "        ])\n",
    "    \n",
    "    # load datasets\n",
    "    data_train_val = datasets.CIFAR10(\n",
    "        data_path,       \n",
    "        train=True,      \n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "\n",
    "    data_test = datasets.CIFAR10(\n",
    "        data_path, \n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "\n",
    "    # train/validation split\n",
    "    n_train = int(len(data_train_val)*train_val_split)\n",
    "    n_val =  len(data_train_val) - n_train\n",
    "\n",
    "    data_train, data_val = random_split(\n",
    "        data_train_val, \n",
    "        [n_train, n_val],\n",
    "        generator=torch.Generator().manual_seed(123)\n",
    "    )\n",
    "\n",
    "    print(\"Size of the train dataset:        \", len(data_train))\n",
    "    print(\"Size of the validation dataset:   \", len(data_val))\n",
    "    print(\"Size of the test dataset:         \", len(data_test))\n",
    "    \n",
    "    return (data_train, data_val, data_test)\n",
    "\n",
    "cifar10_train, cifar10_val, cifar10_test = load_cifar()\n",
    "\n",
    "# Now define a lighter version of CIFAR10: cifar\n",
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "\n",
    "# For each dataset, keep only airplanes and birds\n",
    "cifar2_train = [(img, label_map[label]) for img, label in cifar10_train if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0, 2]]\n",
    "cifar2_test = [(img, label_map[label]) for img, label in cifar10_test if label in [0, 2]]\n",
    "\n",
    "print('Size of the training dataset: ', len(cifar2_train))\n",
    "print('Size of the validation dataset: ', len(cifar2_val))\n",
    "print('Size of the test dataset: ', len(cifar2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop and compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    \n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "\n",
    "            imgs = imgs.to(device=device) \n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "    return losses_train\n",
    "\n",
    "def compute_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "\n",
    "    acc =  correct / total\n",
    "    print(\"Accuracy: {:.2f}\".format(acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a simple custom neural network\n",
    "\n",
    "### 2.1 Naive (but totally ok) method\n",
    "\n",
    "*(Inspired by 8.3.1 Our network as subclass of an nn.Module)*\n",
    "\n",
    "We saw earlier how to define a neural network using [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential). This solution is simple and convenient but might suffer from a lack of flexibility. In order to take advantage of Pytorch's flexibility we need to define our own [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). \n",
    "\n",
    "Since most of the basic building blocks for neural networks are [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) in Pytorch, we will proceed in a similar way if we want to define a custom layer, block of layers, neural network, activation function, loss function etc. etc. It will always start by subclassing the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class. \n",
    "\n",
    "Let's start with a custom neural network!\n",
    "\n",
    "In order to subclass nn.Module, at a minimum we need to define a forward function that takes the inputs to the module and returns the output. This is where we define our moduleâ€™s computation. With PyTorch, if we use standard torch operations, autograd will take care of the backward pass automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # to inherit the '__init__' method from the 'nn.Module' class\n",
    "        # Add whatever you want here (e.g layers and activation functions)\n",
    "        # The order and names don't matter here but it is easier to understand\n",
    "        # if you go for Layer1, fun1, layer2, fun2, etc\n",
    "        # Some conventions:\n",
    "        # - conv stands for convolution\n",
    "        # - pool for pooling\n",
    "        # - fc for fully connected\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "        # 32*32*3: determined by our dataset: 32x32 RGB images\n",
    "        self.fc1 = nn.Linear(32*32*3, 256)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.act2 = nn.ReLU()\n",
    "        # 2: determined by our number of classes (birds and planes)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "        \n",
    "    # Remember, we saw earlier that `forward` defines the \n",
    "    # computation performed at every call (the forward pass) and that it\n",
    "    # should be overridden by all subclasses.\n",
    "    def forward(self, x):\n",
    "        # Now the order matters! \n",
    "        out = self.flat(x)\n",
    "        out = self.act1(self.fc1(out))\n",
    "        out = self.act2(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters:  803266\n",
      "Number of parameter per layer:  [786432, 256, 16384, 64, 128, 2]\n",
      "Output: \n",
      " tensor([[-0.1395, -0.3130]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Now we can instantiate a model with the architecture defined in the cell above\n",
    "model = MyNet()\n",
    "\n",
    "# Our model can be inspected exactly as we inspected our model in the previous tutorial (which was then defined using nn.Sequential) \n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "print(\"Total number of parameters: \", sum(numel_list))\n",
    "print(\"Number of parameter per layer: \", numel_list)\n",
    "\n",
    "img, _ = cifar2_train[0]\n",
    "# Again we can feed a input and get the output exactly the same way as before\n",
    "output_tensor = model(img.unsqueeze(0))\n",
    "print(\"Output: \\n\", output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Using the functional API\n",
    "*(Inspired by 8.3.3 The functional API)*\n",
    "\n",
    "We could write a more concise -- but equivalent -- definition of our custom network. Many things are automatically managed when using already defined [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) objects. For instance, we don't need to implement the convolution operation, we don't need to specify which parameters should be trained nor how to train (update) them. Now, some of the operations are simpler than others: \n",
    "- The [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) layer and the [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#conv2d) layer automatically instanciate trainable parameters (see [nn.parameter.Parameter](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html?highlight=parameter#torch.nn.parameter.Parameter)), link them to the network, tell the network how to do the operations, how to derive them, etc.  \n",
    "- The [nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html?highlight=maxpool#torch.nn.MaxPool2d) layer has no associated trainable parameters and the same holds for activation functions. \n",
    "\n",
    "Modules (e.g. layers or activation functions) that do not generate trainable parameters can be more concisely used in Pytorch using [nn.functional](https://pytorch.org/docs/stable/nn.functional.html#torch-nn-functional) (often imported as ``F``) \n",
    "\n",
    "For example, the functional counterpart of [nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d) is [nn.functional.max_pool2d](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.max_pool2d) (often imported as ``F.max_pool2d``). And the functional counterpart of [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html?highlight=relu#torch.nn.ReLU) is [relu](https://pytorch.org/docs/stable/nn.functional.html?highlight=relu#torch.nn.functional.relu) (often imported as ``F.relu``). Since ``tanh`` is a generic math function and not only used as an activation function, the counterpart of [nn.Tanh](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh) is directly implemented at [torch.tanh](https://pytorch.org/docs/stable/generated/torch.tanh.html?highlight=tanh#torch.tanh)\n",
    "\n",
    "We need to keep using [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) for nn.Linear and nn.Conv2d so that our custom networks manage their trainable parameters automatically. However, we can safely switch to the functional counterparts of pooling and activation, since they have no trainable parameters. \n",
    "\n",
    "This is a lot more concise than and fully equivalent to our previous definition of MyNet.\n",
    "\n",
    "Whether to use the [functional]((https://pytorch.org/docs/stable/nn.functional.html#torch-nn-functional)) or the [modular](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) API regarding operations without trainable parameters is a decision based on style and taste. When part of a network is so simple that we want to use nn.Sequential , we're in the modular realm. When we are writing our own forwards, it may be more natural to use the functional interface for things that do not need state in the form of parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same model as MyNet but using the functional API\n",
    "class MyNetBis(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # No need to declare activation functions nor maxpool layers anymore\n",
    "        self.fc1 = nn.Linear(32*32*3, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Activation functions now come from the functional API \n",
    "        out = torch.flatten(x, 1)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        # Note that we don't need a softmax function in the output layer if we\n",
    "        # use nn.CrossEntropyLoss as the loss function\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "# Another model using maxpool layers (no trainable parameters in such layers)\n",
    "class MyNet02(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Images are 32x32x3 but we start with a maxpool layer that divide H and W by 2\n",
    "        self.fc1 = nn.Linear(32//2 * 32//2 * 3, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Maxpool layers and activation functions using the functional API \n",
    "        out = torch.flatten(F.max_pool2d(x, 2), 1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1919, -0.1124]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Again we can use our new custom model as any other module! \n",
    "img, _ = cifar2_train[0]\n",
    "model = MyNetBis()\n",
    "output_tensor = model(img.unsqueeze(0))\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train our custom network (as any other model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n",
      "08:27:34.134459  |  Epoch 1  |  Training loss 0.552\n",
      "08:27:41.098048  |  Epoch 10  |  Training loss 0.352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5523930487903297,\n",
       " 0.4804809275248372,\n",
       " 0.4568690401865235,\n",
       " 0.437317356150201,\n",
       " 0.4209292771968436,\n",
       " 0.40565535151366644,\n",
       " 0.3901551380647835,\n",
       " 0.376962959132296,\n",
       " 0.36580844688500075,\n",
       " 0.35189006060150496]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=True)\n",
    "model = MyNetBis().to(device=device) \n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# WARNING. This is supposed to much much faster than previously but it \n",
    "# might still take a while if your gpu is not available\n",
    "# AGAIN STOP YOUR KERNEL IF IT'S TOO SLOW \n",
    "train(\n",
    "    n_epochs = 10,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Measuring accuracy (as any other model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:\n",
      "Accuracy: 0.86\n",
      "Validation accuracy:\n",
      "Accuracy: 0.82\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8209562563580874"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"Training accuracy:\")\n",
    "compute_accuracy(model, train_loader)\n",
    "print(\"Validation accuracy:\")\n",
    "compute_accuracy(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Going deeper: defining blocks of layers \n",
    "\n",
    "Deep neural networks often contain \"blocks\" of layers. Blocks are simply groups of layers and make it easier to define deeper neural networks. For example, [ResNet](https://pytorch.org/vision/stable/models.html#id10) uses [Bottleneck](https://github.com/pytorch/vision/blob/65676b4ba1a9fd4417293cb16f690d06a4b2fb4b/torchvision/models/resnet.py#L57) or [BasicBlock](https://github.com/pytorch/vision/blob/65676b4ba1a9fd4417293cb16f690d06a4b2fb4b/torchvision/models/resnet.py#L57) groups of layers.\n",
    "\n",
    "Since a group of layer and a neural network is exactly the same thing in Pytorch (i.e. a nn.Module), we create block of layers exactly as we would create an entire model. Again, we can use nn.Sequential or defining a custom class that inherits nn.Module. \n",
    "\n",
    "### 3.1 Using nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDeepNN(nn.Module):\n",
    "    def __init__(self, n_blocks=10, n_in_out=128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(32*32*3, n_in_out)\n",
    "        \n",
    "        # Here we define a block of layer using nn.Sequential\n",
    "        self.fcblock = nn.Sequential(\n",
    "            *[nn.ReLU( nn.Linear(n_in_out, n_in_out) ) for _ in range(n_blocks)]\n",
    "        )\n",
    "        self.fc2 = nn.Linear(n_in_out, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.flatten(x, 1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fcblock(out)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of parameters:  558722\n",
      "Number of layers:  24\n",
      "Number of parameter per layer:  [393216, 128, 16384, 128, 16384, 128, 16384, 128, 16384, 128, 16384, 128, 16384, 128, 16384, 128, 16384, 128, 16384, 128, 16384, 128, 256, 2]\n",
      "\n",
      " MyDeepNN(\n",
      "  (fc1): Linear(in_features=3072, out_features=128, bias=True)\n",
      "  (fcblock): Sequential(\n",
      "    (0): ReLU(\n",
      "      inplace=True\n",
      "      (inplace): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (1): ReLU(\n",
      "      inplace=True\n",
      "      (inplace): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (2): ReLU(\n",
      "      inplace=True\n",
      "      (inplace): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (3): ReLU(\n",
      "      inplace=True\n",
      "      (inplace): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (4): ReLU(\n",
      "      inplace=True\n",
      "      (inplace): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (5): ReLU(\n",
      "      inplace=True\n",
      "      (inplace): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (6): ReLU(\n",
      "      inplace=True\n",
      "      (inplace): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (7): ReLU(\n",
      "      inplace=True\n",
      "      (inplace): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (8): ReLU(\n",
      "      inplace=True\n",
      "      (inplace): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (9): ReLU(\n",
      "      inplace=True\n",
      "      (inplace): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "img, _ = cifar2_train[0]\n",
    "model = MyDeepNN(n_blocks=10, n_in_out=128)\n",
    "output_tensor = model(img.unsqueeze(0))\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "print(\"\\nTotal number of parameters: \", sum(numel_list))\n",
    "print(\"Number of layers: \", len(numel_list))\n",
    "print(\"Number of parameter per layer: \", numel_list)\n",
    "\n",
    "print(\"\\n\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Using a subclass of nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_in_out = 128, \n",
    "        n1 = 1024,\n",
    "        n2 = 256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # If you want to stack your blocks, the input size and the \n",
    "        # output size must be consistent, so here n_in_out\n",
    "        self.fc1 = nn.Linear(n_in_out, n1)\n",
    "        self.fc2 = nn.Linear(n1, n2)\n",
    "        self.fc3 = nn.Linear(n2, n_in_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.relu(self.fc3(out))\n",
    "        return out\n",
    "\n",
    "class MyDeepNN_WithMyBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_blocks=10, \n",
    "        n_in_out=128,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(32*32*3, n_in_out)\n",
    "        # Here we define a block of layer using our custom MyBlock module.\n",
    "        self.myblocks = nn.Sequential(\n",
    "            *[\n",
    "                MyBlock(n_in_out=n_in_out, n1=1024,n2=128*(i+1)) \n",
    "                for i in range(n_blocks)\n",
    "            ]\n",
    "        )\n",
    "        self.fc2 = nn.Linear(n_in_out, 2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.flatten(x, 1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.myblocks(out)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of parameters:  3268482\n",
      "Number of layers:  34\n",
      "Number of parameter per layer:  [393216, 128, 131072, 1024, 131072, 128, 16384, 128, 131072, 1024, 262144, 256, 32768, 128, 131072, 1024, 393216, 384, 49152, 128, 131072, 1024, 524288, 512, 65536, 128, 131072, 1024, 655360, 640, 81920, 128, 256, 2]\n",
      "\n",
      " MyDeepNN_WithMyBlock(\n",
      "  (fc1): Linear(in_features=3072, out_features=128, bias=True)\n",
      "  (myblocks): Sequential(\n",
      "    (0): MyBlock(\n",
      "      (fc1): Linear(in_features=128, out_features=1024, bias=True)\n",
      "      (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "      (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (1): MyBlock(\n",
      "      (fc1): Linear(in_features=128, out_features=1024, bias=True)\n",
      "      (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (2): MyBlock(\n",
      "      (fc1): Linear(in_features=128, out_features=1024, bias=True)\n",
      "      (fc2): Linear(in_features=1024, out_features=384, bias=True)\n",
      "      (fc3): Linear(in_features=384, out_features=128, bias=True)\n",
      "    )\n",
      "    (3): MyBlock(\n",
      "      (fc1): Linear(in_features=128, out_features=1024, bias=True)\n",
      "      (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (fc3): Linear(in_features=512, out_features=128, bias=True)\n",
      "    )\n",
      "    (4): MyBlock(\n",
      "      (fc1): Linear(in_features=128, out_features=1024, bias=True)\n",
      "      (fc2): Linear(in_features=1024, out_features=640, bias=True)\n",
      "      (fc3): Linear(in_features=640, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "img, _ = cifar2_train[0]\n",
    "model = MyDeepNN_WithMyBlock(n_blocks=5)\n",
    "output_tensor = model(img.unsqueeze(0))\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "print(\"\\nTotal number of parameters: \", sum(numel_list))\n",
    "print(\"Number of layers: \", len(numel_list))\n",
    "print(\"Number of parameter per layer: \", numel_list)\n",
    "\n",
    "print(\"\\n\", model)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "inf265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "d7334498cbea74be2f983349dd0c062cc89e10cb2d32c736100e0abee6e40bc2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
